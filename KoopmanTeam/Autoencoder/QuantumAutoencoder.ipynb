{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Autoencoder\n",
    "\n",
    "This notebook contains the functions to train the functions $\\phi$ and $\\phi^{-1}$ such that $\\phi^{-1}\\circ\\phi|\\alpha\\rangle = |\\alpha\\rangle$, where $|\\alpha\\rangle$ is a pure state on the Bloch sphere.\n",
    "\n",
    "\n",
    "### TODO\n",
    "- ~~Preform (grid?) search of different network architectures to determine the simplest model which yields the best results~~\n",
    "    - Decide on learning rate scheduler?\n",
    "- ~~Implement validation dataset / re-write training generator (Must be done before search is actually preformed on supercomputer)~~\n",
    "    - Use very small chunks of the 4 dimensional sphere in each of the 16 'quadrants' (~200 points in each?)\n",
    "    - Disallow the model from training on these small subsets of the unit 4-sphere (even the points in the regions which were not selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt         \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from utils import *\n",
    "sys.path.remove('..')\n",
    "\n",
    "\n",
    "#Some GPU configuration\n",
    "#Always uses the 1st GPU avalible (if avalible) unless 1st line is uncommented, in which case no GPU is used\n",
    "\n",
    "#tf.config.set_visible_devices([], 'GPU') #uncomment to set tensorflow to use CPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) != 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "elif len(physical_devices) == 0:\n",
    "    print(\"Warning: No GPU detected.  Running tensorflow on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARAMETER SETUP\n",
    "\n",
    "- STATE_DIMENSION - The dimension of the original space.  Treating one complex dimension as two real dimensions\n",
    "- ANTIKOOPMAN_DIMENSION - The dimension of the reduced space.  Of form (dimension,) to keep tensorflow happy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_DIMENSION = 4    #Treating two complex dimensions as 4 real dimensions for now\n",
    "                          #Vector will be [real1, imag1, real2, imag2]\n",
    "ANTIKOOPMAN_DIMENSION = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA GENERATION\n",
    "\n",
    "Data will be valid states for our quantum system.  For the case of pure states on the Bloch sphere, these are 2 complex dimensional (4 real dimensional) vectors with an L2 norm of 1.\n",
    "\n",
    "Forming state $|\\alpha\\rangle =\\begin{bmatrix}x_1+iy_1\\\\ x_2+iy_2\\end{bmatrix}$ as the row vector $[x_1, y_1, x_2, y_2]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pure_bloch(batch_size=16):\n",
    "    '''Generate random pure states on the bloch sphere.\n",
    "    These are two complex dimensional vectors with an L2 norm of 1.\n",
    "    Note that the state dimension of the Bloch sphere is always 4.\n",
    "    '''\n",
    "    bloch_state_dimension = 4\n",
    "    while True:\n",
    "        states = np.empty([batch_size, bloch_state_dimension])\n",
    "        for i in range(batch_size):\n",
    "            x1,y1,x2,y2 = np.random.random(4)\n",
    "            norm = np.sqrt(x1*x1 + y1*y1 + x2*x2 + y2*y2)\n",
    "            states[i] = 1/norm * np.array([x1,y1, x2,y2])\n",
    "        yield (states, states) #autoencoder, so data and label are the same thing\n",
    "        \n",
    "#fix one component to zero, do random select, repeat through each component?\n",
    "#fix some epsilon instead of zero?\n",
    "#would be easy to exclude from training set at least (check that no component is zero or maybe within some epsilon of zero, else re-draw)\n",
    "def generate_pure_bloch_val(batch_size=4096):\n",
    "    bloch_state_dimension = 4\n",
    "    epsilon_max = 1e-5\n",
    "    while True:\n",
    "        states = np.empty([batch_size, bloch_state_dimension])\n",
    "        for i in range(batch_size//16):\n",
    "            fixed1, fixed2, fixed3 = np.random.uniform(low=-1, high=1, size=3)\n",
    "            epsilon = epsilon_max * np.random.uniform(low = -1, high = 1, size = 1)\n",
    "            norm = np.sqrt(fixed1*fixed1 + fixed2*fixed2 + fixed3*fixed3 + epsilon*epsilon)\n",
    "            states[16*i] = 1/norm * np.array([epsilon, fixed1, fixed2, fixed3])\n",
    "            states[16*i+1] = 1/norm * np.array([fixed1, epsilon, fixed2, fixed3])\n",
    "            states[16*i+2] = 1/norm * np.array([fixed1, fixed2, epsilon, fixed3])\n",
    "            states[16*i+3] = 1/norm * np.array([fixed1, fixed2, fixed3, epsilon])\n",
    "            states[16*i+4] = -1/norm * np.array([epsilon, fixed1, fixed2, fixed3])\n",
    "            states[16*i+5] = -1/norm * np.array([fixed1, epsilon, fixed2, fixed3])\n",
    "            states[16*i+6] = -1/norm * np.array([fixed1, fixed2, epsilon, fixed3])\n",
    "            states[16*i+7] = -1/norm * np.array([fixed1, fixed2, fixed3, epsilon])\n",
    "            \n",
    "            fixed1, fixed2, fixed3 = np.random.uniform(low=0.5-epsilon_max, high=0.5+epsilon_max, size=3)\n",
    "            epsilon = np.sqrt(1-fixed1*fixed1 - fixed2*fixed2 - fixed3*fixed3)\n",
    "            states[16*i+8] = np.array([epsilon, fixed1, fixed2, fixed3])\n",
    "            states[16*i+9] = np.array([fixed1, epsilon, fixed2, fixed3])\n",
    "            states[16*i+10] = np.array([fixed1, fixed2, epsilon, fixed3])\n",
    "            states[16*i+11] = np.array([fixed1, fixed2, fixed3, epsilon])\n",
    "            states[16*i+12] = -1 * np.array([epsilon, fixed1, fixed2, fixed3])\n",
    "            states[16*i+13] = -1 * np.array([fixed1, epsilon, fixed2, fixed3])\n",
    "            states[16*i+14] = -1 * np.array([fixed1, fixed2, epsilon, fixed3])\n",
    "            states[16*i+15] = -1 * np.array([fixed1, fixed2, fixed3, epsilon])\n",
    "          \n",
    "            \n",
    "        yield(states, states)\n",
    "        \n",
    "        \n",
    "def generate_pure_bloch_test(batch_size=4096):\n",
    "    bloch_state_dimension = 4\n",
    "    epsilon_max = 1e-5\n",
    "    while True:\n",
    "        states = np.empty([batch_size, bloch_state_dimension])\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            x1, y1, x2, y2 = np.random.uniform(low=-1, high=1, size=4)\n",
    "            norm = np.sqrt(x1*x1 + y1*y1 + x2*x2 + y2*y2)\n",
    "            state = 1/norm * np.array([x1, y1, x2, y2])\n",
    "            #Remove any elements from our validation set\n",
    "            state[np.abs(state)<=epsilon_max] += 3*epsilon_max\n",
    "            state[np.abs(state-0.5)<=epsilon_max] += 3*epsilon_max\n",
    "            states[i] = state\n",
    "            \n",
    "        yield(states, states)\n",
    "        \n",
    "def generate_pure_bloch_file(size = 100000):\n",
    "    bloch_state_dimension = 4\n",
    "    states = np.empty([size, bloch_state_dimension])\n",
    "    for i in range(size):\n",
    "        x1, y1, x2, y2 = np.random.uniform(low=-1,high=1, size=4)\n",
    "        norm = np.sqrt(x1*x1+y1*y1+x2*x2+y2*y2)\n",
    "        state = 1/norm * np.array([x1,y1,x2,y2])\n",
    "        states[i] = state\n",
    "    pd.DataFrame(states).to_csv('./pure_bloch_states.csv')\n",
    "    \n",
    "def read_pure_bloch_file(file, shuffle_buffer = 100000):\n",
    "    with open(file, 'r') as f:\n",
    "        states = [x.strip().split(',')[1:] for x in f.readlines()[1:]]\n",
    "        states = [[float(y) for y in x] for x in states]\n",
    "        \n",
    "    ds = tf.data.Dataset.from_tensor_slices((states,states))\n",
    "    \n",
    "    return ds.shuffle(shuffle_buffer, reshuffle_each_iteration=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Model\n",
    "\n",
    "Things to test:\n",
    "- Various network depths\n",
    "- Various numbers of neurons in each layer\n",
    "- Activation functions? (Note: ReLUs do not give good results; they keep dying off)\n",
    "- Initilizers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input layers for the encoder and decoder, respectivley\n",
    "initial_state = tf.keras.Input(shape = STATE_DIMENSION)\n",
    "antikoop_state = tf.keras.Input(shape = ANTIKOOPMAN_DIMENSION)\n",
    "\n",
    "##########################################ENCODER####################################################################\n",
    "encoding_layer_1 = tf.keras.layers.Dense(64, activation=\"selu\", name='encoding_layer_1')(initial_state)\n",
    "encoding_layer_2 = tf.keras.layers.Dense(128, activation=\"selu\", name='encoding_layer_2')(encoding_layer_1)\n",
    "encoding_layer_3 = tf.keras.layers.Dense(256, activation=\"selu\", name='encoding_layer_3')(encoding_layer_2)\n",
    "#encoding_layer_4 = tf.keras.layers.Dense(64, activation=\"selu\", name='encoding_layer_4')(encoding_layer_3)\n",
    "encoding_layer_5 = tf.keras.layers.Dense(64, activation=\"selu\", name='encoding_layer_5')(encoding_layer_3)\n",
    "encoded_state = tf.keras.layers.Dense(ANTIKOOPMAN_DIMENSION, activation=\"selu\", name='bottleneck')(encoding_layer_5)\n",
    "#####################################################################################################################\n",
    "\n",
    "#########################################DECODER#####################################################################\n",
    "decoding_layer_1 = tf.keras.layers.Dense(64, activation = \"selu\", name='decoding_layer_1')(antikoop_state)\n",
    "decoding_layer_2 = tf.keras.layers.Dense(256, activation = \"selu\", name='decoding_layer_2')(decoding_layer_1)\n",
    "decoding_layer_3 = tf.keras.layers.Dense(128, activation = \"selu\", name='decoding_layer_3')(decoding_layer_2)\n",
    "#decoding_layer_4 = tf.keras.layers.Dense(64, activation = \"selu\", name='decoding_layer_4')(decoding_layer_3)\n",
    "decoding_layer_5 = tf.keras.layers.Dense(64, activation = \"selu\", name='decoding_layer_5')(decoding_layer_3)\n",
    "decoded_state = tf.keras.layers.Dense(STATE_DIMENSION, activation = \"selu\", name='decoded_layer')(decoding_layer_5)\n",
    "#####################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "#Model declarations\n",
    "Phi = tf.keras.Model(inputs=initial_state, outputs = encoded_state, name='Phi')\n",
    "Phi_inv = tf.keras.Model(inputs = antikoop_state, outputs = decoded_state, name='Phi_inv')\n",
    "\n",
    "Autoencoder = tf.keras.models.Sequential([Phi, Phi_inv], name='Autoencoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and various utility functions\n",
    "\n",
    "Loss used for the model is $$|\\ \\| |\\alpha\\rangle\\|_2 - \\|\\tilde{|\\alpha\\rangle}\\|_2\\ | + |\\phi - \\tilde{\\phi}|$$ where $|\\alpha\\rangle$ is our input state, $|\\tilde{\\alpha}\\rangle$ is our autoencoded state, $\\phi$ is the relative phase of our input, and $\\tilde{\\phi}$ is the relative phase of our output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_loss(y_true, y_pred):\n",
    "    '''The L2 norm of the input vector\n",
    "    and the autoencoded vector'''\n",
    "    return tf.norm(y_true-y_pred, ord = 2, axis=-1)\n",
    "\n",
    "\n",
    "def get_relative_phase(vector):\n",
    "    '''Returns the relative phase between\n",
    "    the two complex components of a two\n",
    "    complex dimensional vector\n",
    "    Assumes the vector is passed in as a \n",
    "    four dimensional real row vector of form\n",
    "    [real1, imag1, real2, imag2]\n",
    "    '''\n",
    "    \n",
    "\n",
    "    #Tensorflow likes to return a list of a single\n",
    "    #element sometimes, which breaks this function\n",
    "    #This does not happen during training, only when\n",
    "    #manually run on a single vector\n",
    "    if vector.shape == (4,):\n",
    "        return tf.atan2(vector[1], vector[0])%(2*np.pi) - tf.atan2(vector[3], vector[2])%(2*np.pi)\n",
    "\n",
    "    return tf.atan2(vector[:,1],vector[:,0])%(2*np.pi) - tf.atan2(vector[:,3],vector[:,2])%(2*np.pi)\n",
    "    \n",
    "\n",
    "\n",
    "def norm_phase_difference_loss(y_true, y_pred):\n",
    "    '''\n",
    "    Autoencoding loss accounting for magnitude of\n",
    "    input/output vector and the relative phase\n",
    "    of the two complex components of the\n",
    "    input/output vectors (we don't care if the \n",
    "    autoencoder rotates both components, so long\n",
    "    as it rotates them both equally)\n",
    "    '''\n",
    "    y_true_L2 = tf.norm(y_true, ord=2)\n",
    "    y_pred_L2 = tf.norm(y_pred, ord=2)\n",
    "    \n",
    "    return tf.abs(y_true_L2 - y_pred_L2) + tf.abs(get_relative_phase(y_true) - get_relative_phase(y_pred))\n",
    "\n",
    "\n",
    "def rotate_complex_vectors(vector, theta):\n",
    "    zero = tf.zeros_like(theta, dtype=tf.float32)\n",
    "    rotation_matrix = tf.stack([(tf.cos(theta), -tf.sin(theta), zero, zero), (tf.sin(theta), tf.cos(theta), zero, zero), (zero,zero, tf.cos(theta), -tf.sin(theta)), (zero, zero, tf.sin(theta), tf.cos(theta))])\n",
    "    #rotation_matrix = tf.reshape(rotation_matrix, (4,4))\n",
    "    return tf.linalg.matmul(rotation_matrix, vector)\n",
    "\n",
    "\n",
    "def shifted_L2_loss(y_true, y_pred):\n",
    "    '''Autoencoding loss that rotates both the \n",
    "    ground truth and prediction so that the 2nd complex component\n",
    "    is purely real (the relative phase becomes the absolute phase of the \n",
    "    first complex component), then takes the L2 norm of the\n",
    "    difference between the two rotated vectors\n",
    "    '''\n",
    "    y_true_abs_phase = tf.atan2(y_true[:,-1],y_true[:,-2])\n",
    "    y_pred_abs_phase = tf.atan2(y_pred[:,-1],y_pred[:,-2])\n",
    "\n",
    "    \n",
    "    return tf.norm(rotate_complex_vectors(y_true, -y_true_abs_phase) - rotate_complex_vectors(y_pred, -y_pred_abs_phase), ord=2)\n",
    "\n",
    "\n",
    "\n",
    "def predict_single_state(state, encoder = Phi, decoder = Phi_inv):\n",
    "    '''Outputs the prediction of a single \n",
    "    state.  Primarily for sanity checks.\n",
    "    '''\n",
    "    encoded = encoder(np.array([state,]))\n",
    "    decoded = decoder(encoder(np.array([state,])))\n",
    "    input_norm = np.linalg.norm(state, ord=2)\n",
    "    output_norm = np.linalg.norm(decoded.numpy(), ord=2)\n",
    "    input_rel_phase = get_relative_phase(state).numpy()\n",
    "    output_rel_phase = get_relative_phase(decoded.numpy()).numpy()\n",
    "    print('Initial State:{}\\nEncoded State:{}\\nDecoded State:{}\\nInput Norm:{}\\nOutput Norm:{}\\nInput Relative Phase:{}\\nOutput Relative Phase:{}\\nNorm Difference:{}\\nPhase Difference:{}\\nLoss:{}'.format(\n",
    "            state, encoded.numpy(), decoded.numpy(), input_norm, output_norm,\n",
    "            input_rel_phase, output_rel_phase, np.abs(input_norm-output_norm), \n",
    "            np.abs(input_rel_phase-output_rel_phase), \n",
    "            np.abs(input_norm-output_norm)+np.abs(input_rel_phase-output_rel_phase)))\n",
    "          \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling/Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = read_pure_bloch_file('./pure_bloch_states100k.csv')\n",
    "test_data = training_data.skip(int(0.8*100000)).batch(int(0.2*100000))\n",
    "training_data = training_data.take(int(0.8*100000)).batch(int(0.8*100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = .00001), loss=L2_loss, metrics = ['mse', 'mae'], run_eagerly=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = Autoencoder.fit(training_data, validation_data=test_data,epochs=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State:[-0.62863475 -0.02524175  0.72536144 -0.27934205]\n",
      "Encoded State:[[ 0.3335149  -0.02881466 -0.8669906 ]]\n",
      "Decoded State:[[-0.62157875  0.01418796  0.71986514 -0.28032872]]\n",
      "Input Norm:1.0\n",
      "Output Norm:0.9916408061981201\n",
      "Input Relative Phase:-2.733858562533067\n",
      "Output Relative Phase:[-2.7930632]\n",
      "Norm Difference:0.008359193801879883\n",
      "Phase Difference:[0.05920458]\n",
      "Loss:[0.06756377]\n"
     ]
    }
   ],
   "source": [
    "x1,y1,x2,y2 = np.random.uniform(low=-1, high=1, size=4)\n",
    "teststate = 1/np.sqrt(x1*x1+x2*x2+y1*y1+y2*y2)*np.array([x1,y1,x2,y2])\n",
    "predict_single_state(teststate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Autoencoder_Trials/datafiles/trial18.data'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_history(history, [Autoencoder, Phi, Phi_inv], datadir='./Autoencoder_Trials/datafiles/', batch_size='80000', loss='L2_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_history(history, trial=18, datadir='./Autoencoder_Trials/datafiles/', params_update=True, params={'Learning Rate':.00001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Autoencoder.save('./Autoencoder_Trials/models/trial18e5000.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Autoencoder = tf.keras.models.load_model('./Autoencoder_Trials/models/trial17e4000.h5', compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4L trial 34"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
