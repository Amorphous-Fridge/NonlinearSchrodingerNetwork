{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Autoencoder\n",
    "\n",
    "This notebook contains the functions to train the functions $\\phi$ and $\\phi^{-1}$ such that $\\phi^{-1}\\circ\\phi|\\alpha\\rangle = |\\alpha\\rangle$, where $|\\alpha\\rangle$ is a pure state on the Bloch sphere.\n",
    "\n",
    "\n",
    "### TODO\n",
    "- Preform (grid?) search of different network architectures to determine the simplest model which yields the best results\n",
    "    - Decide on learning rate scheduler?\n",
    "- Implement validation dataset / re-write training generator (Must be done before search is actually preformed on supercomputer)\n",
    "    - Use very small chunks of the 4 dimensional sphere in each of the 16 'quadrants' (~200 points in each?)\n",
    "    - Disallow the model from training on these small subsets of the unit 4-sphere (even the points in the regions which were not selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import redirect_stdout  #Used for writing model architecture to datafiles\n",
    "import matplotlib.pyplot as plt         \n",
    "from datetime import date               #Used for datafiles\n",
    "import tensorflow as tf\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "#Some GPU configuration\n",
    "#Always uses the 1st GPU avalible (if avalible) unless 1st line is uncommented, in which case no GPU is used\n",
    "\n",
    "#tf.config.set_visible_devices([], 'GPU') #uncomment to set tensorflow to use CPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) != 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARAMETER SETUP\n",
    "\n",
    "- STATE_DIMENSION - The dimension of the original space.  Treating one complex dimension as two real dimensions\n",
    "- ANTIKOOPMAN_DIMENSION - The dimension of the reduced space.  Of form (dimension,) to keep tensorflow happy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_DIMENSION = 4    #Treating two complex dimensions as 4 real dimensions for now\n",
    "                          #Vector will be [real1, imag1, real2, imag2]\n",
    "ANTIKOOPMAN_DIMENSION = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA GENERATION\n",
    "\n",
    "Data will be valid states for our quantum system.  For the case of pure states on the Bloch sphere, these are 2 complex dimensional (4 real dimensional) vectors with an L2 norm of 1.\n",
    "\n",
    "Forming state $|\\alpha\\rangle =\\begin{bmatrix}x_1+iy_1\\\\ x_2+iy_2\\end{bmatrix}$ as the row vector $[x_1, y_1, x_2, y_2]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pure_bloch(batch_size=16):\n",
    "    '''Generate random pure states on the bloch sphere.\n",
    "    These are two complex dimensional vectors with an L2 norm of 1.\n",
    "    Note that the state dimension of the Bloch sphere is always 4.\n",
    "    '''\n",
    "    bloch_state_dimension = 4\n",
    "    while True:\n",
    "        states = np.empty([batch_size, bloch_state_dimension])\n",
    "        for i in range(batch_size):\n",
    "            x1,y1,x2,y2 = np.random.random(4)\n",
    "            norm = np.sqrt(x1*x1 + y1*y1 + x2*x2 + y2*y2)\n",
    "            states[i] = 1/norm * np.array([x1,y1, x2,y2])\n",
    "        yield (states, states) #autoencoder, so data and label are the same thing\n",
    "        \n",
    "#fix one component to zero, do random select, repeat through each component?\n",
    "#fix some epsilon instead of zero?\n",
    "#would be easy to exclude from training set at least (check that no component is zero or maybe within some epsilon of zero, else re-draw)\n",
    "def generate_pure_bloch_val(batch_size=4096):\n",
    "    bloch_state_dimension = 4\n",
    "    epsilon_max = 1e-5\n",
    "    while True:\n",
    "        states = np.empty([batch_size, bloch_state_dimension])\n",
    "        for i in range(batch_size//16):\n",
    "            fixed1, fixed2, fixed3 = np.random.uniform(low=-1, high=1, size=3)\n",
    "            epsilon = epsilon_max * np.random.uniform(low = -1, high = 1, size = 1)\n",
    "            norm = np.sqrt(fixed1*fixed1 + fixed2*fixed2 + fixed3*fixed3 + epsilon*epsilon)\n",
    "            states[16*i] = 1/norm * np.array([epsilon, fixed1, fixed2, fixed3])\n",
    "            states[16*i+1] = 1/norm * np.array([fixed1, epsilon, fixed2, fixed3])\n",
    "            states[16*i+2] = 1/norm * np.array([fixed1, fixed2, epsilon, fixed3])\n",
    "            states[16*i+3] = 1/norm * np.array([fixed1, fixed2, fixed3, epsilon])\n",
    "            states[16*i+4] = -1/norm * np.array([epsilon, fixed1, fixed2, fixed3])\n",
    "            states[16*i+5] = -1/norm * np.array([fixed1, epsilon, fixed2, fixed3])\n",
    "            states[16*i+6] = -1/norm * np.array([fixed1, fixed2, epsilon, fixed3])\n",
    "            states[16*i+7] = -1/norm * np.array([fixed1, fixed2, fixed3, epsilon])\n",
    "            \n",
    "            fixed1, fixed2, fixed3 = np.random.uniform(low=0.5-epsilon_max, high=0.5+epsilon_max, size=3)\n",
    "            epsilon = np.sqrt(1-fixed1*fixed1 - fixed2*fixed2 - fixed3*fixed3)\n",
    "            states[16*i+8] = np.array([epsilon, fixed1, fixed2, fixed3])\n",
    "            states[16*i+9] = np.array([fixed1, epsilon, fixed2, fixed3])\n",
    "            states[16*i+10] = np.array([fixed1, fixed2, epsilon, fixed3])\n",
    "            states[16*i+11] = np.array([fixed1, fixed2, fixed3, epsilon])\n",
    "            states[16*i+12] = -1 * np.array([epsilon, fixed1, fixed2, fixed3])\n",
    "            states[16*i+13] = -1 * np.array([fixed1, epsilon, fixed2, fixed3])\n",
    "            states[16*i+14] = -1 * np.array([fixed1, fixed2, epsilon, fixed3])\n",
    "            states[16*i+15] = -1 * np.array([fixed1, fixed2, fixed3, epsilon])\n",
    "          \n",
    "            \n",
    "        yield(states, states)\n",
    "        \n",
    "        \n",
    "def generate_pure_bloch_test(batch_size=4096):\n",
    "    bloch_state_dimension = 4\n",
    "    epsilon_max = 1e-5\n",
    "    while True:\n",
    "        states = np.empty([batch_size, bloch_state_dimension])\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            x1, y1, x2, y2 = np.random.uniform(low=-1, high=1, size=4)\n",
    "            norm = np.sqrt(x1*x1 + y1*y1 + x2*x2 + y2*y2)\n",
    "            state = 1/norm * np.array([x1, y1, x2, y2])\n",
    "            #Remove any elements from our validation set\n",
    "            state[np.abs(state)<=epsilon_max] += 3*epsilon_max\n",
    "            state[np.abs(state-0.5)<=epsilon_max] += 3*epsilon_max\n",
    "            states[i] = state\n",
    "            \n",
    "        yield(states, states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Model\n",
    "\n",
    "Things to test:\n",
    "- Various network depths\n",
    "- Various numbers of neurons in each layer\n",
    "- Activation functions? (Note: ReLUs do not give good results; they keep dying off)\n",
    "- Initilizers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input layers for the encoder and decoder, respectivley\n",
    "initial_state = tf.keras.Input(shape = STATE_DIMENSION)\n",
    "antikoop_state = tf.keras.Input(shape = ANTIKOOPMAN_DIMENSION)\n",
    "\n",
    "##########################################ENCODER####################################################################\n",
    "encoding_layer_1 = tf.keras.layers.Dense(16, activation=\"selu\", name='encoding_layer_1')(initial_state)\n",
    "encoding_layer_2 = tf.keras.layers.Dense(64, activation=\"selu\", name='encoding_layer_2')(encoding_layer_1)\n",
    "encoding_layer_3 = tf.keras.layers.Dense(128, activation=\"selu\", name='encoding_layer_3')(encoding_layer_2)\n",
    "encoding_layer_4 = tf.keras.layers.Dense(64, activation=\"selu\", name='encoding_layer_4')(encoding_layer_3)\n",
    "encoding_layer_5 = tf.keras.layers.Dense(16, activation=\"selu\", name='encoding_layer_5')(encoding_layer_4)\n",
    "encoded_state = tf.keras.layers.Dense(ANTIKOOPMAN_DIMENSION, activation=\"selu\", name='bottleneck')(encoding_layer_5)\n",
    "#####################################################################################################################\n",
    "\n",
    "#########################################DECODER#####################################################################\n",
    "decoding_layer_1 = tf.keras.layers.Dense(16, activation = \"selu\", name='decoding_layer_1')(antikoop_state)\n",
    "decoding_layer_2 = tf.keras.layers.Dense(64, activation = \"selu\", name='decoding_layer_2')(decoding_layer_1)\n",
    "decoding_layer_3 = tf.keras.layers.Dense(128, activation = \"selu\", name='decoding_layer_3')(decoding_layer_2)\n",
    "decoding_layer_4 = tf.keras.layers.Dense(64, activation = \"selu\", name='decoding_layer_4')(decoding_layer_3)\n",
    "decoding_layer_5 = tf.keras.layers.Dense(16, activation = \"selu\", name='decoding_layer_5')(decoding_layer_4)\n",
    "decoded_state = tf.keras.layers.Dense(STATE_DIMENSION, activation = \"selu\", name='decoded_layer')(decoding_layer_5)\n",
    "#####################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "#Model declarations\n",
    "Phi = tf.keras.Model(inputs=initial_state, outputs = encoded_state, name='Phi')\n",
    "Phi_inv = tf.keras.Model(inputs = antikoop_state, outputs = decoded_state, name='Phi_inv')\n",
    "\n",
    "Autoencoder = tf.keras.models.Sequential([Phi, Phi_inv], name='Autoencoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and various utility functions\n",
    "\n",
    "Loss used for the model is $$|\\ \\| |\\alpha\\rangle\\|_2 - \\|\\tilde{|\\alpha\\rangle}\\|_2\\ | + |\\phi - \\tilde{\\phi}|$$ where $|\\alpha\\rangle$ is our input state, $|\\tilde{\\alpha}\\rangle$ is our autoencoded state, $\\phi$ is the relative phase of our input, and $\\tilde{\\phi}$ is the relative phase of our output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_loss(y_true, y_pred):\n",
    "    '''The L2 norm of the input vector\n",
    "    and the autoencoded vector'''\n",
    "    return tf.norm(y_true-y_pred, ord = 2)\n",
    "\n",
    "\n",
    "def get_relative_phase(vector):\n",
    "    '''Returns the relative phase between\n",
    "    the two complex components of a two\n",
    "    complex dimensional vector\n",
    "    Assumes the vector is passed in as a \n",
    "    four dimensional real row vector of form\n",
    "    [real1, imag1, real2, imag2]\n",
    "    '''\n",
    "    \n",
    "\n",
    "    #Tensorflow likes to return a list of a single\n",
    "    #element sometimes, which breaks this function\n",
    "    #This does not happen during training, only when\n",
    "    #manually run on a single vector\n",
    "    if vector.shape == (4,):\n",
    "        return tf.atan2(vector[1], vector[0])%(2*np.pi) - tf.atan2(vector[3], vector[2])%(2*np.pi)\n",
    "\n",
    "    return tf.atan2(vector[:,1],vector[:,0])%(2*np.pi) - tf.atan2(vector[:,3],vector[:,2])%(2*np.pi)\n",
    "    \n",
    "\n",
    "\n",
    "def norm_phase_difference_loss(y_true, y_pred):\n",
    "    '''\n",
    "    Autoencoding loss accounting for magnitude of\n",
    "    input/output vector and the relative phase\n",
    "    of the two complex components of the\n",
    "    input/output vectors (we don't care if the \n",
    "    autoencoder rotates both components, so long\n",
    "    as it rotates them both equally)\n",
    "    '''\n",
    "    y_true_L2 = tf.norm(y_true, ord=2)\n",
    "    y_pred_L2 = tf.norm(y_pred, ord=2)\n",
    "    \n",
    "    return tf.abs(y_true_L2 - y_pred_L2) + tf.abs(get_relative_phase(y_true) - get_relative_phase(y_pred))\n",
    "\n",
    "\n",
    "def rotate_complex_vectors(vector, theta):\n",
    "    zero = tf.zeros_like(theta, dtype=tf.float32)\n",
    "    rotation_matrix = tf.stack([(tf.cos(theta), -tf.sin(theta), zero, zero), (tf.sin(theta), tf.cos(theta), zero, zero), (zero,zero, tf.cos(theta), -tf.sin(theta)), (zero, zero, tf.sin(theta), tf.cos(theta))])\n",
    "    #rotation_matrix = tf.reshape(rotation_matrix, (4,4))\n",
    "    return tf.linalg.matmul(rotation_matrix, vector)\n",
    "\n",
    "\n",
    "def shifted_L2_loss(y_true, y_pred):\n",
    "    '''Autoencoding loss that rotates both the \n",
    "    ground truth and prediction so that the 2nd complex component\n",
    "    is purely real (the relative phase becomes the absolute phase of the \n",
    "    first complex component), then takes the L2 norm of the\n",
    "    difference between the two rotated vectors\n",
    "    '''\n",
    "    y_true_abs_phase = tf.atan2(y_true[:,-1],y_true[:,-2])\n",
    "    y_pred_abs_phase = tf.atan2(y_pred[:,-1],y_pred[:,-2])\n",
    "\n",
    "    \n",
    "    return tf.norm(rotate_complex_vectors(y_true, -y_true_abs_phase) - rotate_complex_vectors(y_pred, -y_pred_abs_phase), ord=2)\n",
    "\n",
    "\n",
    "\n",
    "def predict_single_state(state, encoder = Phi, decoder = Phi_inv):\n",
    "    '''Outputs the prediction of a single \n",
    "    state.  Primarily for sanity checks.\n",
    "    '''\n",
    "    encoded = encoder(np.array([state,]))\n",
    "    decoded = decoder(encoder(np.array([state,])))\n",
    "    input_norm = np.linalg.norm(state, ord=2)\n",
    "    output_norm = np.linalg.norm(decoded.numpy(), ord=2)\n",
    "    input_rel_phase = get_relative_phase(state).numpy()\n",
    "    output_rel_phase = get_relative_phase(decoded.numpy()).numpy()\n",
    "    print('Initial State:{}\\nEncoded State:{}\\nDecoded State:{}\\nInput Norm:{}\\nOutput Norm:{}\\nInput Relative Phase:{}\\nOutput Relative Phase:{}\\nNorm Difference:{}\\nPhase Difference:{}\\nLoss:{}'.format(\n",
    "            state, encoded.numpy(), decoded.numpy(), input_norm, output_norm,\n",
    "            input_rel_phase, output_rel_phase, np.abs(input_norm-output_norm), \n",
    "            np.abs(input_rel_phase-output_rel_phase), \n",
    "            np.abs(input_norm-output_norm)+np.abs(input_rel_phase-output_rel_phase)))\n",
    "          \n",
    "    return None\n",
    "\n",
    "###################DATA WRITING FUNCTIONS#####################\n",
    "\n",
    "##############################################################\n",
    "def write_history(history, model, loss = 'autoencoding_loss', \n",
    "                  optimizer='Adam', lr='.001', \n",
    "                  batch_size='1024', datadir='./datafiles/'):\n",
    "    '''Writes training history to a datafile\n",
    "    This will create a new trial datafile.  If the model has \n",
    "    had additional training, append_history should be used instead.\n",
    "    \n",
    "    PARAMS:\n",
    "    -------\n",
    "    history - The history callback returned by the model.fit method in keras\n",
    "    model - The model (or list of models) which we want to write the architecture of to the datafile\n",
    "    string loss - The loss function the model was trained on\n",
    "    string optimizer - The optimizer the model was compiled with\n",
    "    string lr - The learning rate the model was initially compiled with\n",
    "    string spe - The number of steps per epoch\n",
    "    string batch_size - The number of samples trained on per step\n",
    "    string datadir - Directory where the datafiles are stored\n",
    "    '''\n",
    "    \n",
    "    rundatadir = datadir\n",
    "    filename = 'trial'+str(len(os.listdir(rundatadir)))\n",
    "\n",
    "    with open(rundatadir+filename+'.data', 'w') as f:\n",
    "        f.write(str(date.today())+'\\n')\n",
    "        for key in history.history.keys():\n",
    "            f.write(key+',')\n",
    "            for epoch in range(history.params['epochs']):\n",
    "                f.write(str(history.history[key][epoch])+',')\n",
    "            f.write('\\n')\n",
    "        f.write(\"Loss,{}\\nOptimizer,{}\\nLearning Rate,{}\\nSteps Per Epoch,{}\\nBatch Size,{}\\nEpochs,{}\\n\".format(loss,optimizer,lr,history.params['steps'],batch_size, history.params['epochs']))\n",
    "        f.write('\\n')\n",
    "        with redirect_stdout(f):\n",
    "            if type(model) == list:\n",
    "                for i in model:              \n",
    "                    i.summary()\n",
    "            else:\n",
    "                model.summary()\n",
    "    return rundatadir+filename+'.data'\n",
    "\n",
    "#####################################################################\n",
    "#####################################################################\n",
    "\n",
    "def append_history(history, trial=None, datadir='./datafiles/', params_update = True, \n",
    "                   params = {'Loss':None, 'Optimizer':None, 'Learning Rate':None, 'Batch Size':None}):\n",
    "    '''Appends new training data to trial datafile.\n",
    "    This will only work with datafiles written using write_history (or files of identical form)\n",
    "    \n",
    "    PARAMS:\n",
    "    -------\n",
    "    history - The history callback containing the new run's data\n",
    "    int trial - The trial number to append the data to.  If we want to append data to \n",
    "            trial43.data, this would be 43.  If none specified, updates the most recent trial\n",
    "    str datadir - The directory containing the datafile\n",
    "    bool params_update - Boolean indicating if we should update parameters (loss used, optimizer, etc.)\n",
    "                    in addition to adding the new loss data\n",
    "    dict params - Dictionary containing updated parameter values\n",
    "                  If parameter is not included, the previous value \n",
    "                  written for that parameter will be repeated\n",
    "    '''\n",
    "    \n",
    "    if trial==None:\n",
    "        trial = max([int(x.strip('.data').strip('trial')) for x in os.listdir(datadir)])\n",
    "    \n",
    "    filename = 'trial'+str(trial)+'.data'\n",
    "    \n",
    "    \n",
    "    newlines = []\n",
    "    keys = history.history.keys()\n",
    "    \n",
    "    \n",
    "    for k in ['Loss', 'Optimizer', 'Learning Rate', 'Batch Size']:\n",
    "        if k not in params.keys():\n",
    "            params[k] = None\n",
    "    params['Epochs'] = history.params['epochs']\n",
    "    params['Steps Per Epoch'] = history.params['steps']\n",
    "   \n",
    "      \n",
    "    #Read old data and add in new data as it is read\n",
    "    with open(datadir+filename, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            \n",
    "            tag = line.split(',')[0]\n",
    "            \n",
    "            if tag in keys:\n",
    "                newdata = [str(x) for x in history.history[tag]]\n",
    "                newlines.append(line.split(',')[:-1] + newdata ) #[:-1] to drop the newline\n",
    "            elif (tag in params.keys() and params_update == True):\n",
    "                if params[tag] is None:\n",
    "                    newlines.append(line.strip().split(',')[:] + [line.strip().split(',')[-1]])\n",
    "                else:\n",
    "                    newdata = [str(params[tag])]\n",
    "                    newlines.append(line.strip().split(',')[:] + newdata)\n",
    "            else:\n",
    "                newlines.append(line)\n",
    "    \n",
    "    #Write the old data with the appended data\n",
    "    with open(datadir+filename, 'w') as f:\n",
    "        for el in newlines:\n",
    "            if type(el) == list:\n",
    "                f.write(','.join(el))\n",
    "                f.write('\\n')\n",
    "            else:\n",
    "                f.write(el)\n",
    "    return\n",
    "    \n",
    "    \n",
    "#####################################################################\n",
    "#####################################################################\n",
    "    \n",
    "def loss_plot(trial, datadir='./datafiles/', savefig = True,\n",
    "              figdir = './figures/', logplot=False,\n",
    "              metric = None, mark_runs = False, \n",
    "              skip_epochs=0, mark_lowest = True):\n",
    "    '''Creates a plot of the loss/metric for the given trial number\n",
    "    '''\n",
    "    \n",
    "    if metric == None:\n",
    "        metric = 'loss'\n",
    "\n",
    "    losses = []\n",
    "    runs = []\n",
    "    \n",
    "    #Read in the data\n",
    "    with open(datadir+'trial'+str(trial)+'.data', 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if line.split(',')[0] == metric:\n",
    "                losses = [float(x) for x in line.strip().split(',')[1:]]\n",
    "                if not mark_runs:\n",
    "                    break\n",
    "            elif (line.split(',')[0] == 'Epochs' and mark_runs == True):\n",
    "                runs = ['0.']+line.strip().split(',')[1:]\n",
    "                runs = [float(runs[i-1])+float(runs[i-2]) for i in range(2, len(runs))]\n",
    "                break\n",
    "            \n",
    "    \n",
    "    fig, ax = plt.subplots(1,1, figsize = (8,8))\n",
    "\n",
    "\n",
    "    ax.plot(range(len(losses[skip_epochs:])), losses[skip_epochs:])\n",
    "    if mark_runs:\n",
    "        for i in runs:\n",
    "            ax.plot([i,i], ax.get_ylim(), c='black', ls=':')\n",
    "    if mark_lowest:\n",
    "        lowest = min(losses)\n",
    "        ax.plot(losses.index(lowest), lowest, 'go')\n",
    " #       ax.text(ax.get_xlim()[1]-0.05*ax.get_xlim()[0], ax.get_ylim()[1]-0.05*ax.get_ylim()[0], 'Lowest loss: {}'.format(lowest))\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    if logplot:\n",
    "        ax.set_yscale('log')\n",
    "    ax.set_title('Trial '+str(trial)+' Loss')\n",
    "\n",
    "    if savefig:\n",
    "        fig.savefig(figdir+'trial{}.png'.format(trial))\n",
    "    \n",
    "    return None\n",
    "\n",
    "#####################################################################\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling/Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "Autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = .001), loss=shifted_L2_loss, metrics = ['mse', 'mae'], run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = Autoencoder.fit(generate_pure_bloch_test(4096), validation_data=generate_pure_bloch_val(128), validation_steps=10, steps_per_epoch=50,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State:[ 0.60206733 -0.21711227 -0.3792101   0.66826409]\n",
      "Encoded State:[[ 0.5637911  -0.15494716]]\n",
      "Decoded State:[[0.0266394  0.1134955  0.07015761 0.7456037 ]]\n",
      "Input Norm:1.0\n",
      "Output Norm:0.7579167485237122\n",
      "Input Relative Phase:3.8501464974261426\n",
      "Output Relative Phase:[-0.13672554]\n",
      "Norm Difference:0.24208325147628784\n",
      "Phase Difference:[3.9868722]\n",
      "Loss:[4.2289553]\n"
     ]
    }
   ],
   "source": [
    "x1,y1,x2,y2 = np.random.uniform(low=-1, high=1, size=4)\n",
    "teststate = 1/np.sqrt(x1*x1+x2*x2+y1*y1+y2*y2)*np.array([x1,y1,x2,y2])\n",
    "predict_single_state(teststate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Autoencoder_Trials/datafiles/trial14.data'"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_history(history, [Autoencoder, Phi, Phi_inv], datadir='./Autoencoder_Trials/datafiles/', batch_size='4096', loss='norm_phase_difference_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_history(history, trial=14, datadir='./Autoencoder_Trials/datafiles/', params_update=True, params={'Learning Rate':.0001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "Autoencoder.save('./Autoencoder_Trials/models/trial8e200.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
