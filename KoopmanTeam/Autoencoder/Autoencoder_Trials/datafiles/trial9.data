2021-06-20
loss,11.98884391784668,5.311070442199707,3.1480460166931152,4.020235061645508,6.261417388916016,4.047519683837891,2.8491432666778564,2.0317909717559814,1.2625139951705933,1.1552566289901733,1.7998329401016235,1.8416765928268433,2.1745858192443848,1.2324193716049194,2.098398447036743,2.29632306098938,1.8650197982788086,2.151348829269409,1.2135677337646484,1.7478604316711426,1.1149497032165527,1.0451250076293945,1.492079496383667,1.1462894678115845,0.8175216913223267,0.9256049394607544,0.9133101105690002,1.1511671543121338,1.1725797653198242,1.6417046785354614,1.1120502948760986,0.9244461059570312,0.8501383066177368,1.3789314031600952,2.2120392322540283,1.166499137878418,0.9847815632820129,0.8107618689537048,0.7094363570213318,0.761507511138916,0.755476176738739,0.8341548442840576,0.6222412586212158,0.7599822878837585,0.7111740708351135,1.0650091171264648,0.7809759378433228,0.8821191191673279,0.7133537530899048,0.9066693186759949,0.9568105936050415,1.2573301792144775,1.2260832786560059,0.982134997844696,0.8157142400741577,0.8087558150291443,0.7242060303688049,0.7519914507865906,0.7151998281478882,0.6954597234725952,0.8643637895584106,0.8129981160163879,0.9948356747627258,0.8552004098892212,0.8061942458152771,1.1225608587265015,0.7356640696525574,0.6740542054176331,0.8480146527290344,1.2986383438110352,0.7001174688339233,1.09877347946167,0.9268772602081299,1.1151294708251953,0.8937740921974182,0.8923429250717163,0.725790798664093,0.6235367655754089,0.9745234847068787,0.8882434964179993,0.7162113785743713,0.7688829898834229,0.8441992998123169,0.657378077507019,0.6665080189704895,0.6679035425186157,0.9442757368087769,0.7836203575134277,0.7160667181015015,0.7690233588218689,0.7768547534942627,0.7527439594268799,0.7024233937263489,0.6351458430290222,0.6569288372993469,0.6533543467521667,0.6315858364105225,0.7426364421844482,0.7052469849586487,0.756466805934906,
mse,0.663638174533844,0.6531654596328735,0.5554291009902954,0.5701006054878235,0.6834362149238586,0.677703320980072,0.6469053030014038,0.6356599926948547,0.6510837078094482,0.6542139649391174,0.6355124711990356,0.6397044658660889,0.6651502251625061,0.6568058729171753,0.6514745950698853,0.6212884187698364,0.599667489528656,0.5961822867393494,0.5883492827415466,0.5841017365455627,0.5871789455413818,0.5867365002632141,0.5691179633140564,0.583854615688324,0.5758844614028931,0.5737819075584412,0.5740955471992493,0.5749542713165283,0.5717278122901917,0.5859693884849548,0.5890841484069824,0.5816973447799683,0.5781628489494324,0.5937204957008362,0.5425240397453308,0.5481266379356384,0.5692479014396667,0.5804378390312195,0.5639415979385376,0.5773453712463379,0.5750635862350464,0.5701025128364563,0.5777430534362793,0.5714287161827087,0.5586684346199036,0.5611802935600281,0.5805394649505615,0.5810708403587341,0.5852976441383362,0.5799759030342102,0.5826264023780823,0.5841346979141235,0.5883364677429199,0.5872364044189453,0.5935218930244446,0.5900515913963318,0.5920676589012146,0.5806210041046143,0.5733859539031982,0.570543646812439,0.5744159817695618,0.5723074078559875,0.5788283348083496,0.581906259059906,0.5769592523574829,0.5967075824737549,0.5873428583145142,0.5809977054595947,0.5806978940963745,0.5888855457305908,0.5855879783630371,0.5713456869125366,0.5347047448158264,0.5393379926681519,0.5618097186088562,0.5610911846160889,0.5658085346221924,0.5598562359809875,0.5670359134674072,0.5682350397109985,0.5606951713562012,0.5617304444313049,0.5603121519088745,0.5696796774864197,0.5710564255714417,0.5679346323013306,0.5728711485862732,0.5819407105445862,0.5594248175621033,0.5697531700134277,0.5792264342308044,0.5846827030181885,0.5770620107650757,0.5700316429138184,0.5751403570175171,0.5806396007537842,0.5741286873817444,0.5695368051528931,0.5844359397888184,0.5721909999847412,
mae,0.6805366277694702,0.678210437297821,0.596662700176239,0.5963995456695557,0.6783943176269531,0.6782931685447693,0.651619017124176,0.6413144469261169,0.6499336361885071,0.642483115196228,0.6280675530433655,0.6324100494384766,0.6528949737548828,0.6374961137771606,0.6422888040542603,0.6122648119926453,0.5999086499214172,0.5917522311210632,0.5725162029266357,0.5684617757797241,0.5693457126617432,0.5646772980690002,0.5554704070091248,0.5631207823753357,0.5586974024772644,0.5547260642051697,0.5548717379570007,0.5585761070251465,0.5583662986755371,0.566936731338501,0.5687277317047119,0.5616680979728699,0.5599536299705505,0.5759482383728027,0.5464193224906921,0.5366454720497131,0.5510132312774658,0.5613200068473816,0.5459001660346985,0.5589284300804138,0.5556918382644653,0.5498254299163818,0.5577145218849182,0.5528674721717834,0.5414891839027405,0.5418463349342346,0.5605400800704956,0.5600379705429077,0.5644639730453491,0.5606539249420166,0.5651711225509644,0.5648316740989685,0.5710026621818542,0.566693902015686,0.570984423160553,0.5678918957710266,0.5719952583312988,0.5594921708106995,0.5556273460388184,0.5514866709709167,0.5576309561729431,0.5548098087310791,0.559852659702301,0.5622122883796692,0.5598804950714111,0.5760634541511536,0.5664089322090149,0.5630560517311096,0.5622801184654236,0.5697438716888428,0.5663133859634399,0.556928277015686,0.5334169268608093,0.5292662978172302,0.5441621541976929,0.5448734760284424,0.5459790825843811,0.5417248606681824,0.548145055770874,0.5510042905807495,0.5448140501976013,0.5432996153831482,0.5416762828826904,0.5508126616477966,0.5523970127105713,0.5483056902885437,0.5580343008041382,0.5621439218521118,0.5417041182518005,0.5514885187149048,0.5597507357597351,0.5648584961891174,0.5592455267906189,0.5523661971092224,0.5563853979110718,0.5608349442481995,0.5591679215431213,0.5525861382484436,0.5667162537574768,0.5544338822364807,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 2)                 18818     
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 18820     
=================================================================
Total params: 37,638
Trainable params: 37,638
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_7 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 16)                80        
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 64)                1088      
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 128)               8320      
_________________________________________________________________
encoding_layer_4 (Dense)     (None, 64)                8256      
_________________________________________________________________
encoding_layer_5 (Dense)     (None, 16)                1040      
_________________________________________________________________
bottleneck (Dense)           (None, 2)                 34        
=================================================================
Total params: 18,818
Trainable params: 18,818
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_8 (InputLayer)         [(None, 2)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 16)                48        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 64)                1088      
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 128)               8320      
_________________________________________________________________
decoding_layer_4 (Dense)     (None, 64)                8256      
_________________________________________________________________
decoding_layer_5 (Dense)     (None, 16)                1040      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 68        
=================================================================
Total params: 18,820
Trainable params: 18,820
Non-trainable params: 0
_________________________________________________________________
