2021-06-26
loss,3.873124361038208,3.242713689804077,3.141017436981201,3.0288851261138916,2.2636032104492188,1.8403174877166748,0.4709043502807617,0.4451153874397278,0.4450565278530121,0.4466307759284973,0.44471946358680725,0.4447466731071472,0.4449126124382019,0.4872615337371826,0.44861727952957153,0.44624873995780945,0.44288644194602966,0.47270384430885315,0.44565632939338684,0.4449387490749359,0.4454382658004761,0.44500285387039185,0.4452367424964905,0.44589951634407043,0.537941038608551,0.4522821307182312,0.4446636438369751,0.4454045593738556,0.4454290270805359,0.4459999203681946,0.4463827908039093,0.44580408930778503,0.44587165117263794,0.4452442228794098,0.44600415229797363,0.44543561339378357,0.4462122321128845,0.44632697105407715,0.44561389088630676,0.44479984045028687,0.44569918513298035,0.4466588497161865,0.44548100233078003,0.44623863697052,0.44502532482147217,0.4461621940135956,0.44657963514328003,0.44638460874557495,0.4462187886238098,0.44615787267684937,0.4451627731323242,0.44560784101486206,0.44595661759376526,0.4462079107761383,0.44657421112060547,0.4455212354660034,0.44609153270721436,0.4464572072029114,0.4454817473888397,0.446483850479126,0.44540730118751526,0.4453991949558258,0.44533511996269226,0.4471553862094879,0.44553616642951965,0.44548892974853516,0.44548577070236206,0.44717925786972046,0.4475083649158478,0.4460567831993103,0.445948988199234,0.4448527991771698,0.44534575939178467,0.44565096497535706,0.4466854929924011,0.4471941888332367,0.4490010440349579,0.44621315598487854,0.44654422998428345,0.44638144969940186,0.4465141296386719,0.44652682542800903,0.4459802508354187,0.44628146290779114,0.44678616523742676,0.4457530975341797,0.445627897977829,0.4462266266345978,0.4458813965320587,0.4453871250152588,1.2989869117736816,4.424229621887207,4.424344062805176,4.42405891418457,4.424355983734131,4.424148082733154,4.424433708190918,4.424200534820557,4.424221038818359,4.424143314361572,
mse,3.7756388187408447,2.6471304893493652,2.4758214950561523,2.3195695877075195,1.2947518825531006,1.0198943614959717,0.062142979353666306,0.055195145308971405,0.05519620329141617,0.05559437721967697,0.055132221430540085,0.05512861907482147,0.05517520010471344,0.06775765120983124,0.05617076903581619,0.05548804625868797,0.05481140688061714,0.06303685903549194,0.05533083528280258,0.055197738111019135,0.05525616183876991,0.05518603324890137,0.0552399605512619,0.05538852512836456,0.1417810469865799,0.05708697810769081,0.0551457516849041,0.05529573932290077,0.05529240518808365,0.0554923377931118,0.05551399290561676,0.05537622421979904,0.055406227707862854,0.05526081472635269,0.055404629558324814,0.05528661236166954,0.0555262565612793,0.055498119443655014,0.055340155959129333,0.055158983916044235,0.055380478501319885,0.0555625781416893,0.05533589795231819,0.05552658438682556,0.05518176034092903,0.05549592897295952,0.05558471754193306,0.05554116144776344,0.055512621998786926,0.05546119809150696,0.055244430899620056,0.05536692216992378,0.055432166904211044,0.05550278350710869,0.05557827651500702,0.0553320087492466,0.055522371083498,0.05551976338028908,0.055302564054727554,0.0555274598300457,0.05529037117958069,0.055274009704589844,0.05529514700174332,0.05570072680711746,0.05533088743686676,0.05534515529870987,0.05533240735530853,0.055728793144226074,0.05581590160727501,0.055468689650297165,0.05541166290640831,0.05512796342372894,0.055263012647628784,0.05532744526863098,0.055542413145303726,0.055748071521520615,0.05629055202007294,0.055475953966379166,0.05554445832967758,0.055507127195596695,0.05556190386414528,0.05558812618255615,0.05546444281935692,0.05549846962094307,0.05562819913029671,0.055356670171022415,0.05531631037592888,0.0554853118956089,0.055391475558280945,0.05528299883008003,1.0182304382324219,4.894349575042725,4.894601821899414,4.893965721130371,4.8946213722229,4.894162178039551,4.894801616668701,4.894282341003418,4.894323825836182,4.894157886505127,
mae,1.7413673400878906,1.3222445249557495,1.1992847919464111,1.1525373458862305,0.7317580580711365,0.6910569667816162,0.20702238380908966,0.19741405546665192,0.19741909205913544,0.19805260002613068,0.19719772040843964,0.1971994787454605,0.19734545052051544,0.21343213319778442,0.1986679881811142,0.1978338211774826,0.19627927243709564,0.20702362060546875,0.19760696589946747,0.19730542600154877,0.1975209265947342,0.19735729694366455,0.1974499672651291,0.19776812195777893,0.23535695672035217,0.1999349594116211,0.19724275171756744,0.19751158356666565,0.19753165543079376,0.19776560366153717,0.19794680178165436,0.19769717752933502,0.1977335512638092,0.19749179482460022,0.19774696230888367,0.19748583436012268,0.19787058234214783,0.1978932023048401,0.19749805331230164,0.1971885859966278,0.19764859974384308,0.19802436232566833,0.19751955568790436,0.19786222279071808,0.19733557105064392,0.19780179858207703,0.1979248970746994,0.1978737711906433,0.19772763550281525,0.19775724411010742,0.19736677408218384,0.19753441214561462,0.19766521453857422,0.1978430151939392,0.1979290395975113,0.19747987389564514,0.19768038392066956,0.19790241122245789,0.19751764833927155,0.19788195192813873,0.1974291205406189,0.19750718772411346,0.19749273359775543,0.19816097617149353,0.1975385546684265,0.19761669635772705,0.19754558801651,0.1981218457221985,0.1982976347208023,0.19775620102882385,0.1976674497127533,0.19720812141895294,0.19746693968772888,0.1976235955953598,0.1979745328426361,0.19812637567520142,0.1987917572259903,0.19783371686935425,0.19803406298160553,0.19786900281906128,0.19790099561214447,0.19801855087280273,0.19775177538394928,0.19776754081249237,0.19810038805007935,0.19768737256526947,0.19753660261631012,0.19785209000110626,0.1977243423461914,0.19747304916381836,0.5946753025054932,2.1998934745788574,2.199965000152588,2.199784517288208,2.1999711990356445,2.1998398303985596,2.200021982192993,2.1998746395111084,2.1998863220214844,2.1998393535614014,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 485315    
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 485316    
=================================================================
Total params: 970,631
Trainable params: 970,631
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 64)                320       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 128)               8320      
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 512)               66048     
_________________________________________________________________
encoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
encoding_layer_5 (Dense)     (None, 256)               131328    
_________________________________________________________________
encoding_layer_6 (Dense)     (None, 64)                16448     
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 195       
=================================================================
Total params: 485,315
Trainable params: 485,315
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 64)                256       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 256)               16640     
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 512)               131584    
_________________________________________________________________
decoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
decoding_layer_5 (Dense)     (None, 128)               65664     
_________________________________________________________________
decoding_layer_6 (Dense)     (None, 64)                8256      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 260       
=================================================================
Total params: 485,316
Trainable params: 485,316
Non-trainable params: 0
_________________________________________________________________
