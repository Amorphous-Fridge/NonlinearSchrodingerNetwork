2021-06-26
loss,4.390298843383789,4.417141914367676,4.424556732177734,4.424278736114502,4.424025058746338,4.424217224121094,4.424304485321045,4.424047470092773,4.424017906188965,4.424108505249023,4.424220085144043,4.424238681793213,4.424073696136475,4.4242024421691895,4.4235758781433105,4.681997299194336,4.415903091430664,4.4242730140686035,4.424191951751709,4.424128532409668,4.424130916595459,4.424204349517822,4.424176216125488,4.424059867858887,4.424205780029297,4.424200534820557,4.424376010894775,4.424457550048828,4.42423152923584,4.424380302429199,4.42404842376709,4.424069881439209,4.424290180206299,4.423915386199951,4.423952579498291,4.424298286437988,4.42415714263916,4.424039840698242,4.424359321594238,4.4242048263549805,4.424075126647949,4.424211502075195,4.424217224121094,4.424106121063232,4.424169540405273,4.4242329597473145,4.423960208892822,4.424219608306885,4.424282073974609,4.424212455749512,4.4240641593933105,4.424318790435791,4.4242143630981445,4.424099922180176,4.424125671386719,4.424306869506836,4.424170017242432,4.424331188201904,4.424209117889404,4.424276351928711,4.424233913421631,4.42399787902832,4.424191951751709,4.42408561706543,4.424440383911133,4.424266815185547,4.42429780960083,4.424407482147217,4.424145698547363,4.424025058746338,4.424291610717773,4.42426872253418,4.424258708953857,4.424417495727539,4.424280643463135,4.423981666564941,4.424274444580078,4.424327850341797,4.4242448806762695,4.424258708953857,4.4242119789123535,4.4242987632751465,4.424288749694824,4.42408561706543,4.424103736877441,4.424219131469727,4.424093246459961,4.424286842346191,4.424212455749512,4.424213409423828,4.424436092376709,4.4242658615112305,4.424544334411621,4.424018383026123,4.424630165100098,4.4244160652160645,4.424150466918945,4.424278736114502,4.4239020347595215,4.424293041229248,
mse,4.862151145935059,4.8791422843933105,4.895066738128662,4.8944525718688965,4.8938984870910645,4.894314765930176,4.894507884979248,4.89394998550415,4.893881320953369,4.894072532653809,4.8943257331848145,4.8943681716918945,4.894000053405762,4.894283771514893,4.8929057121276855,5.9526872634887695,4.885255813598633,4.894433975219727,4.894262790679932,4.894118785858154,4.894125461578369,4.894291400909424,4.894227981567383,4.893971920013428,4.894294261932373,4.894275665283203,4.89466667175293,4.894848823547363,4.894348621368408,4.894676208496094,4.893941879272461,4.893992900848389,4.894479274749756,4.893655300140381,4.893738269805908,4.894497871398926,4.894189357757568,4.893932342529297,4.894627094268799,4.894290924072266,4.894008159637451,4.894308567047119,4.894320487976074,4.894077777862549,4.894211292266846,4.894349098205566,4.893756866455078,4.894321918487549,4.894458770751953,4.894308567047119,4.893982410430908,4.894543647766113,4.894310474395752,4.894056797027588,4.894119739532471,4.8945136070251465,4.894217014312744,4.894571304321289,4.894303321838379,4.894448757171631,4.894351482391357,4.893834590911865,4.894260883331299,4.894029140472412,4.894808769226074,4.894425868988037,4.894495010375977,4.89473295211792,4.894167900085449,4.893897533416748,4.8944830894470215,4.894429683685303,4.894406318664551,4.894752025604248,4.894458770751953,4.893798351287842,4.894445419311523,4.8945698738098145,4.894378662109375,4.894410610198975,4.894315719604492,4.894502639770508,4.894473552703857,4.894027233123779,4.89407205581665,4.894317150115967,4.894042015075684,4.894466400146484,4.894301891326904,4.894314289093018,4.894801139831543,4.894423007965088,4.895037651062012,4.893879413604736,4.89522647857666,4.8947529792785645,4.894168853759766,4.8944501876831055,4.893628120422363,4.89448356628418,
mae,2.160658359527588,2.1961867809295654,2.2000982761383057,2.199923515319824,2.199766159057617,2.1998848915100098,2.199939489364624,2.1997809410095215,2.1997618675231934,2.1998159885406494,2.199888229370117,2.1999013423919678,2.1997971534729004,2.1998801231384277,2.1995487213134766,2.2571277618408203,2.1924455165863037,2.199918270111084,2.199869155883789,2.1998279094696045,2.1998302936553955,2.1998770236968994,2.199859142303467,2.1997861862182617,2.199877977371216,2.1998724937438965,2.1999833583831787,2.200035333633423,2.1998937129974365,2.199986457824707,2.199777364730835,2.1997928619384766,2.1999306678771973,2.1996963024139404,2.1997196674346924,2.1999359130859375,2.199848175048828,2.199775218963623,2.199972629547119,2.1998770236968994,2.199796676635742,2.1998820304870605,2.199885606765747,2.1998167037963867,2.1998538970947266,2.1998932361602783,2.1997249126434326,2.199885368347168,2.1999242305755615,2.1998817920684814,2.199789524078369,2.199948787689209,2.1998825073242188,2.19981050491333,2.1998281478881836,2.199939727783203,2.1998560428619385,2.1999566555023193,2.199880599975586,2.1999216079711914,2.1998941898345947,2.199747085571289,2.1998682022094727,2.1998023986816406,2.200024127960205,2.1999154090881348,2.199934482574463,2.2000021934509277,2.199841022491455,2.19976544380188,2.1999313831329346,2.199916362762451,2.1999099254608154,2.200007677078247,2.1999247074127197,2.1997365951538086,2.199920892715454,2.199955940246582,2.199901819229126,2.1999106407165527,2.1998841762542725,2.199936866760254,2.1999285221099854,2.1998019218444824,2.199814558029175,2.1998848915100098,2.199805498123169,2.1999266147613525,2.1998796463012695,2.199883222579956,2.200021743774414,2.19991397857666,2.2000889778137207,2.1997601985931396,2.2001428604125977,2.200007915496826,2.1998424530029297,2.1999223232269287,2.19968843460083,2.1999316215515137,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 407203    
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 407204    
=================================================================
Total params: 814,407
Trainable params: 814,407
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 32)                160       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 128)               4224      
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 512)               66048     
_________________________________________________________________
encoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
encoding_layer_5 (Dense)     (None, 128)               65664     
_________________________________________________________________
encoding_layer_6 (Dense)     (None, 64)                8256      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 195       
=================================================================
Total params: 407,203
Trainable params: 407,203
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 64)                256       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 128)               8320      
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 512)               66048     
_________________________________________________________________
decoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
decoding_layer_5 (Dense)     (None, 128)               65664     
_________________________________________________________________
decoding_layer_6 (Dense)     (None, 32)                4128      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 132       
=================================================================
Total params: 407,204
Trainable params: 407,204
Non-trainable params: 0
_________________________________________________________________
