2021-06-26
loss,0.7441330552101135,0.2623387277126312,0.23741395771503448,0.19867782294750214,0.16141510009765625,0.12550783157348633,0.1055944636464119,0.0943296030163765,0.101689912378788,0.08784916251897812,0.08374001830816269,0.07731829583644867,0.05531276762485504,0.055141519755125046,0.05705239623785019,0.06492849439382553,0.06190978363156319,0.06117308512330055,0.05029815062880516,0.048570357263088226,0.0487312413752079,0.05059562250971794,0.04976893588900566,0.04697062447667122,0.04490615427494049,0.0428839772939682,0.041830841451883316,0.03705092892050743,0.03429931402206421,0.0432029664516449,0.044301871210336685,0.03519326075911522,0.034734632819890976,0.03599848598241806,0.030924931168556213,0.035239435732364655,0.03512981906533241,0.0332440510392189,0.0364859476685524,0.032633498311042786,0.034210454672575,0.027625493705272675,0.028034549206495285,0.034158576279878616,0.032865460962057114,0.029440071433782578,0.02919510193169117,0.03383645787835121,0.029902126640081406,0.03160484507679939,0.028881419450044632,0.025187652558088303,0.032097410410642624,0.030506635084748268,0.028012389317154884,0.0313752144575119,0.03192935138940811,0.028410809114575386,0.027174506336450577,0.027532167732715607,0.028410278260707855,0.02599950321018696,0.0282945204526186,0.027477603405714035,0.025090292096138,0.029180558398365974,0.026024315506219864,0.02535138837993145,0.027017125859856606,0.024308698251843452,0.027437496930360794,0.025534287095069885,0.02587960660457611,0.02629902772605419,0.02604699693620205,0.025001371279358864,0.02408541925251484,0.022881047800183296,0.023093802854418755,0.02591952309012413,0.02556280605494976,0.02192569710314274,0.02178851328790188,0.023968467488884926,0.026038112118840218,0.02334749698638916,0.02006126381456852,0.024470679461956024,0.02403375878930092,0.021529875695705414,0.021127885207533836,0.023241205140948296,0.02416657842695713,0.02294626273214817,0.019935276359319687,0.025173187255859375,0.022330639883875847,0.02385525219142437,0.019951414316892624,0.018228286877274513,
mse,0.28224241733551025,0.022250564768910408,0.019306348636746407,0.013807176612317562,0.008180246688425541,0.004719812422990799,0.003398139728233218,0.0027142029721289873,0.0030575839336961508,0.002320802304893732,0.002051620278507471,0.0017961444100365043,0.0009461104637011886,0.0008855739142745733,0.0009780771797522902,0.001229379791766405,0.001106042880564928,0.0010916697792708874,0.0007322036544792354,0.0006864382885396481,0.0006858967826701701,0.0007226894958876073,0.000720786105375737,0.0006336763617582619,0.0005803399253636599,0.0005349951097741723,0.000508191529661417,0.0003921787429135293,0.00035288440994918346,0.0005264250212348998,0.0005423962720669806,0.0003623387892730534,0.0003519748570397496,0.0003626679244916886,0.0002840556262526661,0.00035569732426665723,0.00034867829526774585,0.0003214678436052054,0.0003707682772073895,0.00030912450165487826,0.0003310508618596941,0.00023078151571098715,0.0002365513500990346,0.00033129443181678653,0.00030310815782286227,0.00024716611369512975,0.00024539834703318775,0.0003260891535319388,0.00025339587591588497,0.0002845209382940084,0.0002387640270171687,0.0001824240607675165,0.0002915908698923886,0.0002714005531743169,0.00022678897948935628,0.0002793016901705414,0.0002802111266646534,0.00022420701861847192,0.0002130616339854896,0.0002129671338479966,0.0002337147161597386,0.00019596022320911288,0.00022658293892163783,0.00021140760509297252,0.00018464826280251145,0.00024371661129407585,0.00019305816385895014,0.00018182588974013925,0.00021090875088702887,0.00017142918659374118,0.0002158674760721624,0.00018547268700785935,0.0001891892170533538,0.00019990680448245257,0.00020048301666975021,0.00017871859017759562,0.00016605226846877486,0.0001534673065179959,0.00015406655438710004,0.00019118750060442835,0.0001863022189354524,0.00014044986164662987,0.00014135814853943884,0.00016607882571406662,0.00019337135017849505,0.000158804512466304,0.00011918355448869988,0.00017627434863243252,0.0001647283206693828,0.00013359788863454014,0.0001320307346759364,0.00015859138511586934,0.0001703198067843914,0.0001482302468502894,0.00011690380051732063,0.00018181964696850628,0.00014595552056562155,0.00015969731612131,0.00011639251169981435,9.819938713917509e-05,
mae,0.32420027256011963,0.11490580439567566,0.10529287159442902,0.08749471604824066,0.06843210756778717,0.053122714161872864,0.04494770988821983,0.039682019501924515,0.04389454796910286,0.03720153495669365,0.03591161593794823,0.03285320848226547,0.02328496053814888,0.02333921752870083,0.02406076155602932,0.027909141033887863,0.026205820962786674,0.026171183213591576,0.020813854411244392,0.020960796624422073,0.02088647335767746,0.021309977397322655,0.021381905302405357,0.01972971111536026,0.019413163885474205,0.01844654232263565,0.017646128311753273,0.015492907725274563,0.014771765097975731,0.01863301359117031,0.01951877400279045,0.015150751918554306,0.014963784255087376,0.01510575134307146,0.013085546903312206,0.014785485342144966,0.014966846443712711,0.013951681554317474,0.015359667129814625,0.013799580745398998,0.014045113697648048,0.011572510935366154,0.011888542212545872,0.014801641926169395,0.013934689573943615,0.012492564506828785,0.012487655505537987,0.01438015978783369,0.01241725217550993,0.013286370784044266,0.01220161933451891,0.01049770601093769,0.013758906163275242,0.013032030314207077,0.011793561279773712,0.013414446264505386,0.013813463039696217,0.012360277585685253,0.011788550764322281,0.011662223376333714,0.012167191132903099,0.010990816168487072,0.012044521979987621,0.011707617901265621,0.010555357672274113,0.01249933335930109,0.011145190335810184,0.010863786563277245,0.01153196208178997,0.010465352796018124,0.011630523949861526,0.010776863433420658,0.010792212560772896,0.011022396385669708,0.011094143614172935,0.010558091104030609,0.010138828307390213,0.009807719849050045,0.009737971238791943,0.011065845377743244,0.010838805697858334,0.009245865046977997,0.009186018258333206,0.01021627802401781,0.011107894591987133,0.009974149987101555,0.008453065529465675,0.010469692759215832,0.01016999501734972,0.009117686189711094,0.008995461277663708,0.009801991283893585,0.010340315289795399,0.009764958173036575,0.008384481072425842,0.01072827260941267,0.009398744441568851,0.009944348596036434,0.008482501842081547,0.007774258963763714,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 117003    
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 117004    
=================================================================
Total params: 234,007
Trainable params: 234,007
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 8)                 40        
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 64)                576       
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 256)               16640     
_________________________________________________________________
encoding_layer_4 (Dense)     (None, 256)               65792     
_________________________________________________________________
encoding_layer_5 (Dense)     (None, 128)               32896     
_________________________________________________________________
encoding_layer_6 (Dense)     (None, 8)                 1032      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 27        
=================================================================
Total params: 117,003
Trainable params: 117,003
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 8)                 32        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 128)               1152      
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 256)               33024     
_________________________________________________________________
decoding_layer_4 (Dense)     (None, 256)               65792     
_________________________________________________________________
decoding_layer_5 (Dense)     (None, 64)                16448     
_________________________________________________________________
decoding_layer_6 (Dense)     (None, 8)                 520       
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 36        
=================================================================
Total params: 117,004
Trainable params: 117,004
Non-trainable params: 0
_________________________________________________________________
