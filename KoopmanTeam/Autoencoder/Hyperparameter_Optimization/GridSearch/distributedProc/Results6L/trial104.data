2021-06-26
loss,3.866459608078003,3.835963487625122,3.830950975418091,3.829463005065918,4.036337375640869,4.309884548187256,3.889484167098999,3.8370351791381836,3.839503526687622,3.830435276031494,3.8295111656188965,3.8295676708221436,3.8293662071228027,3.829678535461426,3.8294906616210938,3.828902244567871,3.8290910720825195,3.829164505004883,3.8299529552459717,3.8297131061553955,3.8286008834838867,4.002507209777832,3.840057373046875,3.829793930053711,3.8294241428375244,3.8292341232299805,3.8286218643188477,3.8287863731384277,3.8287954330444336,3.8289148807525635,3.8292269706726074,3.829557180404663,3.8291127681732178,3.828975200653076,3.829225540161133,4.172398567199707,4.424238204956055,8.59221076965332,4.424455642700195,4.424209117889404,4.424237251281738,4.42414665222168,4.424165725708008,4.4242844581604,4.4243574142456055,4.424160480499268,4.424294948577881,4.424250602722168,4.424161911010742,4.424089431762695,4.4242353439331055,4.42407751083374,4.424349308013916,4.4244585037231445,4.424044609069824,4.424095630645752,4.424104690551758,4.4241461753845215,4.424025535583496,4.4241228103637695,4.424142360687256,4.424136161804199,4.424108505249023,4.424132823944092,4.424133777618408,4.4242353439331055,4.423832416534424,4.424230098724365,4.424264430999756,4.4241156578063965,4.424263954162598,4.4240007400512695,4.424188613891602,4.424170017242432,4.42399787902832,4.424294948577881,4.424188613891602,4.424205780029297,4.424249649047852,4.424261093139648,4.424305438995361,4.424262523651123,4.424264907836914,4.424108028411865,4.424232482910156,4.4240875244140625,4.424137592315674,4.42431116104126,4.424167633056641,4.424327373504639,4.4242024421691895,4.424116134643555,4.424088478088379,4.4243927001953125,4.424127578735352,4.424410820007324,4.424168586730957,4.424438953399658,4.424378871917725,4.424408435821533,
mse,3.7937567234039307,3.6848998069763184,3.674346923828125,3.6714837551116943,4.46325159072876,4.6842780113220215,3.7909903526306152,3.6858596801757812,3.690694570541382,3.6732468605041504,3.6715433597564697,3.6716620922088623,3.6712875366210938,3.6718621253967285,3.671523332595825,3.670422315597534,3.6707491874694824,3.6709303855895996,3.672395706176758,3.671983242034912,3.6698641777038574,4.022930145263672,3.6917312145233154,3.672142267227173,3.671407699584961,3.671057939529419,3.6698691844940186,3.670206308364868,3.670206069946289,3.6704113483428955,3.6710546016693115,3.6716718673706055,3.670809030532837,3.6705641746520996,3.6710617542266846,4.739455223083496,4.894364833831787,154.75563049316406,4.894837856292725,4.894304275512695,4.8943610191345215,4.894158840179443,4.894207000732422,4.894461154937744,4.894623279571533,4.89419412612915,4.89448881149292,4.894393444061279,4.894196033477783,4.894041061401367,4.894357204437256,4.894016742706299,4.894609451293945,4.894850254058838,4.893939018249512,4.894050121307373,4.894063472747803,4.894161701202393,4.893896579742432,4.894111156463623,4.894150733947754,4.894141674041748,4.894085884094238,4.894134998321533,4.894134044647217,4.894355773925781,4.893474578857422,4.894348621368408,4.894418239593506,4.894093036651611,4.894423484802246,4.893841743469238,4.894254207611084,4.894214153289795,4.893830299377441,4.894489765167236,4.894253730773926,4.894290924072266,4.894387245178223,4.894411087036133,4.894514560699463,4.894419193267822,4.8944244384765625,4.894068717956543,4.894353866577148,4.894032955169678,4.894143104553223,4.894520282745361,4.894209384918213,4.89456033706665,4.894289970397949,4.894092559814453,4.894039154052734,4.894699573516846,4.894118785858154,4.894744396209717,4.894205093383789,4.894805431365967,4.89467716217041,4.894742012023926,
mae,1.781822681427002,1.6989071369171143,1.6727615594863892,1.6552609205245972,1.7619324922561646,2.1065239906311035,1.7788827419281006,1.7005516290664673,1.7018768787384033,1.66790771484375,1.6637250185012817,1.6619900465011597,1.6607298851013184,1.6599332094192505,1.6590924263000488,1.6580499410629272,1.6571208238601685,1.6562995910644531,1.6558574438095093,1.655188798904419,1.6542282104492188,1.8628859519958496,1.7040457725524902,1.6626664400100708,1.6561840772628784,1.654255986213684,1.6532188653945923,1.6529529094696045,1.652762770652771,1.6526098251342773,1.652631402015686,1.652660846710205,1.6523666381835938,1.652269721031189,1.6525890827178955,1.9407141208648682,2.199899196624756,3.5982472896575928,2.2000324726104736,2.199880838394165,2.199897050857544,2.1998393535614014,2.19985294342041,2.199925422668457,2.1999714374542236,2.1998496055603027,2.1999335289001465,2.1999058723449707,2.19985032081604,2.199805736541748,2.1998958587646484,2.199799060821533,2.199967384338379,2.20003604888916,2.1997766494750977,2.1998085975646973,2.199812412261963,2.1998398303985596,2.1997644901275635,2.1998257637023926,2.1998372077941895,2.1998343467712402,2.1998188495635986,2.199831962585449,2.1998326778411865,2.1998958587646484,2.1996445655822754,2.1998934745788574,2.199913263320923,2.1998205184936523,2.1999146938323975,2.199749231338501,2.19986629486084,2.199855327606201,2.1997458934783936,2.1999335289001465,2.19986629486084,2.199876308441162,2.199904441833496,2.199911117553711,2.1999406814575195,2.199913263320923,2.1999144554138184,2.1998136043548584,2.199894666671753,2.1998040676116943,2.1998345851898193,2.1999423503875732,2.1998541355133057,2.199953079223633,2.199876546859741,2.1998207569122314,2.1998050212860107,2.199993371963501,2.1998276710510254,2.2000060081481934,2.199852228164673,2.200023889541626,2.199986457824707,2.200004816055298,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 538323    
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 538324    
=================================================================
Total params: 1,076,647
Trainable params: 1,076,647
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 16)                80        
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 256)               4352      
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 512)               131584    
_________________________________________________________________
encoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
encoding_layer_5 (Dense)     (None, 256)               131328    
_________________________________________________________________
encoding_layer_6 (Dense)     (None, 32)                8224      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 99        
=================================================================
Total params: 538,323
Trainable params: 538,323
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 32)                128       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 256)               8448      
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 512)               131584    
_________________________________________________________________
decoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
decoding_layer_5 (Dense)     (None, 256)               131328    
_________________________________________________________________
decoding_layer_6 (Dense)     (None, 16)                4112      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 68        
=================================================================
Total params: 538,324
Trainable params: 538,324
Non-trainable params: 0
_________________________________________________________________
