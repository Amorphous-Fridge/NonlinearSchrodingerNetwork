2021-06-26
loss,4.367425441741943,4.424371719360352,4.424066543579102,4.424372673034668,4.424323558807373,4.424236297607422,4.424123764038086,4.423971652984619,4.4241042137146,4.424309730529785,4.424165725708008,4.424281597137451,5.671286582946777,4.423962593078613,4.424127101898193,4.424256801605225,4.424067497253418,4.4242143630981445,4.424215316772461,4.424448490142822,4.42398738861084,4.424266815185547,4.424213409423828,4.424363613128662,4.4239301681518555,4.424343109130859,4.424115180969238,4.424191474914551,4.424437522888184,4.424081325531006,4.424157619476318,4.424110412597656,4.4242024421691895,4.42429256439209,4.4241557121276855,4.424206256866455,4.424211502075195,4.424067497253418,4.424246788024902,4.4240193367004395,4.42453145980835,4.4241414070129395,4.424075126647949,4.424306392669678,4.424353122711182,4.424034118652344,4.424271583557129,4.424530029296875,4.424323081970215,4.4240851402282715,4.424324989318848,4.4240312576293945,4.424219608306885,4.423980236053467,4.424192905426025,4.4243574142456055,4.4241042137146,4.424192905426025,4.424196243286133,4.424213409423828,4.424455642700195,4.424346446990967,4.424417495727539,4.424210071563721,4.424321174621582,4.424118518829346,4.424118518829346,4.4243974685668945,4.424196720123291,4.424243450164795,4.42427921295166,4.424224376678467,4.424163341522217,4.424230575561523,4.424494743347168,4.424157619476318,4.424008846282959,4.424326419830322,4.424228668212891,4.4240498542785645,4.424333572387695,4.424063682556152,4.42405366897583,4.42432975769043,4.4240031242370605,4.424081802368164,4.424169540405273,4.424346446990967,4.424298286437988,4.424250602722168,4.424215793609619,4.424032211303711,4.424472808837891,4.424123764038086,4.424111843109131,4.42429780960083,4.424291133880615,4.424386501312256,4.424381256103516,4.424127578735352,
mse,4.7977800369262695,4.894658088684082,4.893983840942383,4.894659996032715,4.894545555114746,4.8943586349487305,4.894113063812256,4.893780708312988,4.894070625305176,4.894522666931152,4.89420223236084,4.894454479217529,16.316652297973633,4.893753528594971,4.894122123718262,4.894404411315918,4.893986701965332,4.8943095207214355,4.894315719604492,4.894824981689453,4.893815517425537,4.89442253112793,4.894309043884277,4.894643783569336,4.893681526184082,4.894592761993408,4.894094467163086,4.894258975982666,4.89480447769165,4.894018173217773,4.894189834594727,4.894078254699707,4.894285678863525,4.894481658935547,4.894179344177246,4.894288539886475,4.894305229187012,4.893986701965332,4.894382476806641,4.8938798904418945,4.8950114250183105,4.89415168762207,4.894012451171875,4.894515037536621,4.894618034362793,4.893918991088867,4.894433975219727,4.895003318786621,4.894549369812012,4.894024848937988,4.894554615020752,4.893916130065918,4.8943257331848145,4.89379358291626,4.8942646980285645,4.894628524780273,4.894067287445068,4.894265651702881,4.89427375793457,4.894308567047119,4.894843578338623,4.894601345062256,4.894752502441406,4.894298553466797,4.894540309906006,4.894100666046143,4.894098281860352,4.894711494445801,4.894268989562988,4.8943772315979,4.894453048706055,4.894331455230713,4.8942060470581055,4.894346237182617,4.894937038421631,4.894182205200195,4.893859386444092,4.894557952880859,4.894338607788086,4.893949508666992,4.894575595855713,4.8939738273620605,4.893960952758789,4.894562244415283,4.893856048583984,4.894022464752197,4.894209384918213,4.8946003913879395,4.894498825073242,4.894393444061279,4.8943190574646,4.893913269042969,4.8948750495910645,4.894115447998047,4.894083499908447,4.894495964050293,4.894478797912598,4.894689559936523,4.894679069519043,4.8941264152526855,
mae,2.166757822036743,2.199981451034546,2.1997897624969482,2.199981927871704,2.1999497413635254,2.1998960971832275,2.199826240539551,2.1997323036193848,2.1998140811920166,2.1999425888061523,2.1998512744903564,2.1999237537384033,2.6160154342651367,2.199723720550537,2.199828863143921,2.199909210205078,2.1997902393341064,2.1998822689056396,2.1998839378356934,2.200028896331787,2.1997416019439697,2.19991397857666,2.1998817920684814,2.199977159500122,2.1997039318084717,2.199962854385376,2.1998207569122314,2.1998677253723145,2.2000229358673096,2.1997992992401123,2.1998488903045654,2.1998164653778076,2.199875593185425,2.1999306678771973,2.1998450756073,2.199876308441162,2.199881076812744,2.1997904777526855,2.1999030113220215,2.1997601985931396,2.2000820636749268,2.1998369693756104,2.1997978687286377,2.1999409198760986,2.19996976852417,2.1997711658477783,2.199917793273926,2.200079917907715,2.199950695037842,2.199801445007324,2.1999518871307373,2.199770212173462,2.1998863220214844,2.199735164642334,2.199869394302368,2.1999728679656982,2.1998131275177,2.199869394302368,2.1998720169067383,2.1998817920684814,2.2000343799591064,2.199965476989746,2.2000081539154053,2.1998794078826904,2.199948310852051,2.1998231410980225,2.199822187423706,2.1999967098236084,2.1998705863952637,2.199901580810547,2.199923038482666,2.199888229370117,2.199852705001831,2.199892520904541,2.2000608444213867,2.199846029281616,2.199754238128662,2.199953079223633,2.19989013671875,2.199779987335205,2.199958086013794,2.19978666305542,2.1997830867767334,2.199954032897949,2.1997530460357666,2.199800491333008,2.1998541355133057,2.199965000152588,2.1999363899230957,2.19990611076355,2.199885129928589,2.1997697353363037,2.200042486190796,2.199826717376709,2.199817419052124,2.1999351978302,2.1999306678771973,2.1999902725219727,2.1999878883361816,2.1998300552368164,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 575875    
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 575876    
=================================================================
Total params: 1,151,751
Trainable params: 1,151,751
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 128)               640       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 256)               33024     
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 512)               131584    
_________________________________________________________________
encoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
encoding_layer_5 (Dense)     (None, 256)               131328    
_________________________________________________________________
encoding_layer_6 (Dense)     (None, 64)                16448     
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 195       
=================================================================
Total params: 575,875
Trainable params: 575,875
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 64)                256       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 256)               16640     
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 512)               131584    
_________________________________________________________________
decoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
decoding_layer_5 (Dense)     (None, 256)               131328    
_________________________________________________________________
decoding_layer_6 (Dense)     (None, 128)               32896     
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 516       
=================================================================
Total params: 575,876
Trainable params: 575,876
Non-trainable params: 0
_________________________________________________________________
