2021-06-26
loss,2.7128255367279053,2.2248878479003906,2.2223618030548096,2.2208333015441895,2.222179651260376,2.2681446075439453,2.258857011795044,2.220806837081909,2.2211053371429443,2.219886541366577,2.218264579772949,2.21770977973938,2.216810941696167,2.215139389038086,2.215500831604004,2.208463668823242,2.205547571182251,2.2121076583862305,2.203765630722046,2.204469680786133,2.2039949893951416,2.2046010494232178,2.205152988433838,2.2041356563568115,2.2071921825408936,2.206730604171753,2.2036139965057373,2.2033939361572266,2.2033798694610596,2.203402280807495,2.203549861907959,2.203505039215088,2.204578161239624,2.204460382461548,2.2045061588287354,2.2027342319488525,2.2049248218536377,2.2634830474853516,4.10106086730957,3.8372464179992676,3.8361823558807373,5.321749210357666,4.424173355102539,4.4241766929626465,4.424125671386719,4.42454719543457,4.424441337585449,4.423754692077637,4.4241156578063965,4.424160003662109,4.424045562744141,4.424048900604248,4.424459457397461,4.424290657043457,4.424278259277344,4.424321174621582,4.424314498901367,4.424307346343994,4.424113750457764,4.424402236938477,4.4243340492248535,4.424120903015137,4.42411994934082,4.424056529998779,4.424110412597656,4.424016952514648,4.424025535583496,4.424005508422852,4.423951625823975,4.424333095550537,4.424372673034668,4.424346446990967,4.424404144287109,4.424261569976807,4.424117565155029,4.424239635467529,4.4243316650390625,4.424232006072998,4.424358367919922,4.423990249633789,4.42405891418457,4.424142360687256,4.424251556396484,4.424409866333008,4.424161434173584,4.42446231842041,4.424084186553955,4.424185276031494,4.424262523651123,4.424447536468506,4.424417972564697,4.424191474914551,4.4242658615112305,4.423977375030518,4.424184799194336,4.424107551574707,4.424190521240234,4.424157619476318,4.4239373207092285,4.424016952514648,
mse,2.0062055587768555,1.2509300708770752,1.248216152191162,1.2464931011199951,1.2480124235153198,1.3022667169570923,1.2891980409622192,1.2464202642440796,1.2468323707580566,1.2454652786254883,1.2438162565231323,1.2430537939071655,1.2421437501907349,1.2403351068496704,1.2408950328826904,1.2332507371902466,1.2301146984100342,1.2372827529907227,1.2281579971313477,1.2289141416549683,1.2284111976623535,1.2291065454483032,1.229697823524475,1.2284613847732544,1.23187255859375,1.2313741445541382,1.2278591394424438,1.2276585102081299,1.2276866436004639,1.2276926040649414,1.227813720703125,1.2276866436004639,1.2289035320281982,1.2287533283233643,1.2288329601287842,1.2268967628479004,1.2292410135269165,1.3126641511917114,4.2436933517456055,3.686262607574463,3.6842048168182373,17.775114059448242,4.894222736358643,4.894228935241699,4.894112586975098,4.895046234130859,4.894810676574707,4.893301486968994,4.89409065246582,4.894191741943359,4.893935680389404,4.893954277038574,4.8948469161987305,4.894479751586914,4.89445161819458,4.894547939300537,4.894528865814209,4.89451789855957,4.894092082977295,4.894730091094971,4.894576549530029,4.894109725952148,4.894106388092041,4.893967151641846,4.894081115722656,4.8938751220703125,4.893899917602539,4.893848419189453,4.893728733062744,4.894570827484131,4.894662380218506,4.894598960876465,4.89472770690918,4.8944196701049805,4.894101619720459,4.8943705558776855,4.894565582275391,4.894350051879883,4.894627571105957,4.8938140869140625,4.8939714431762695,4.894148349761963,4.894389629364014,4.894744873046875,4.894186496734619,4.894853115081787,4.894025802612305,4.894248962402344,4.894418716430664,4.894824981689453,4.89476203918457,4.894259929656982,4.894425392150879,4.8937883377075195,4.894241809844971,4.894073963165283,4.894260883331299,4.894186973571777,4.893701076507568,4.89387845993042,
mae,1.0721499919891357,0.6647358536720276,0.6564885973930359,0.6553058624267578,0.6553736925125122,0.7104972004890442,0.7200159430503845,0.6538448929786682,0.6517988443374634,0.6506526470184326,0.6486318707466125,0.6458041667938232,0.6437554955482483,0.6389531493186951,0.6393513679504395,0.6123945713043213,0.6043592691421509,0.6234757304191589,0.5978657007217407,0.5962408185005188,0.5932716131210327,0.5939211845397949,0.5962007641792297,0.5956628918647766,0.6024629473686218,0.6015075445175171,0.5914393067359924,0.5913906693458557,0.5926367044448853,0.5922672748565674,0.5929601192474365,0.59015953540802,0.594574511051178,0.5953370332717896,0.5945003032684326,0.5877519845962524,0.5986237525939941,0.6556711792945862,1.945939064025879,1.7022918462753296,1.6992449760437012,2.429673910140991,2.199857711791992,2.1998586654663086,2.199826240539551,2.200091600418091,2.2000246047973633,2.1995956897735596,2.1998205184936523,2.1998484134674072,2.199775457382202,2.1997809410095215,2.2000341415405273,2.1999311447143555,2.199923038482666,2.1999499797821045,2.1999449729919434,2.199941396713257,2.199820041656494,2.2000017166137695,2.199958086013794,2.1998252868652344,2.199824333190918,2.199784755706787,2.199817180633545,2.199759006500244,2.199766159057617,2.199751377105713,2.1997172832489014,2.1999566555023193,2.1999826431274414,2.1999640464782715,2.200000524520874,2.199913263320923,2.199822425842285,2.199899673461914,2.199955463409424,2.199894428253174,2.1999731063842773,2.1997413635253906,2.1997861862182617,2.199836254119873,2.1999051570892334,2.2000060081481934,2.199847459793091,2.2000370025634766,2.199801445007324,2.1998648643493652,2.199913263320923,2.200028419494629,2.2000107765197754,2.1998682022094727,2.1999151706695557,2.1997339725494385,2.1998627185821533,2.199814796447754,2.1998682022094727,2.199847936630249,2.199708938598633,2.1997597217559814,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 407235    
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 407236    
=================================================================
Total params: 814,471
Trainable params: 814,471
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 64)                320       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 128)               8320      
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 512)               66048     
_________________________________________________________________
encoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
encoding_layer_5 (Dense)     (None, 128)               65664     
_________________________________________________________________
encoding_layer_6 (Dense)     (None, 32)                4128      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 99        
=================================================================
Total params: 407,235
Trainable params: 407,235
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 32)                128       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 128)               4224      
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 512)               66048     
_________________________________________________________________
decoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
decoding_layer_5 (Dense)     (None, 128)               65664     
_________________________________________________________________
decoding_layer_6 (Dense)     (None, 64)                8256      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 260       
=================================================================
Total params: 407,236
Trainable params: 407,236
Non-trainable params: 0
_________________________________________________________________
