2021-06-26
loss,0.7760669589042664,0.2519104480743408,0.20085375010967255,0.15684416890144348,0.1601339727640152,0.11112508177757263,0.09965207427740097,0.07726078480482101,0.0676535964012146,0.06418787688016891,0.06601586937904358,0.057495612651109695,0.0729125514626503,0.07225929945707321,0.09009241312742233,0.05085701867938042,0.05919969454407692,0.06203080713748932,0.056616004556417465,0.08400926738977432,0.07680873572826385,0.06357048451900482,0.04976649582386017,0.04331982508301735,0.07163767516613007,0.04786791279911995,0.042795754969120026,0.04396865889430046,0.054554738104343414,0.057176243513822556,0.050599850714206696,0.0479370579123497,0.053294211626052856,0.05109544098377228,0.04598616063594818,0.040655165910720825,0.03594250604510307,0.028218131512403488,0.028599679470062256,0.03606422245502472,0.030797569081187248,0.034528184682130814,0.025830356404185295,0.026025880128145218,0.03445211797952652,0.030390772968530655,0.04370506852865219,0.04620637744665146,0.041187435388565063,0.04340048134326935,0.03586452826857567,0.03264720365405083,0.03332041576504707,0.03789788857102394,0.03794082626700401,0.03279923275113106,0.03895092383027077,0.037843476980924606,0.03180721402168274,0.03121821954846382,0.036043256521224976,0.02893706038594246,0.02758733555674553,0.034372683614492416,0.03151256963610649,0.03676178306341171,0.031193843111395836,0.02495749667286873,0.024044698104262352,0.026208853349089622,0.02734198421239853,0.02782566100358963,0.03054504655301571,0.034557193517684937,0.03166588395833969,0.03295858949422836,0.03324801102280617,0.02843787707388401,0.024060077965259552,0.02109426073729992,0.018601227551698685,0.02236415632069111,0.02402682974934578,0.031734246760606766,0.032467275857925415,0.030218951404094696,0.024818213656544685,0.02223123237490654,0.023622753098607063,0.02968587912619114,0.025743495672941208,0.0308018047362566,0.029091401025652885,0.023799605667591095,0.029021305963397026,0.023773355409502983,0.020685814321041107,0.02103605680167675,0.02287587895989418,0.02115493267774582,
mse,0.25353536009788513,0.020974598824977875,0.013256105594336987,0.0073730964213609695,0.007487226277589798,0.0034877038560807705,0.002937105018645525,0.0016807673964649439,0.0012945829657837749,0.0011729779653251171,0.0012768087908625603,0.000907790963537991,0.001575854024849832,0.0015454638050869107,0.0023098622914403677,0.0007331091910600662,0.0009862210135906935,0.001095736981369555,0.0008932866621762514,0.001982024870812893,0.0016652989434078336,0.0011445333948358893,0.0006894203252159059,0.000525313604157418,0.001506381668150425,0.0007127673015929759,0.0005252042901702225,0.0005403492832556367,0.0008516215020790696,0.0009304338600486517,0.0007351894746534526,0.0006659615901298821,0.0008082002168521285,0.0007443826179951429,0.0005901343538425863,0.00048044967115856707,0.0003600518684834242,0.00023897156643215567,0.0002447643200866878,0.00036863007699139416,0.0002702086349017918,0.00034378646523691714,0.00019661476835608482,0.00019467920355964452,0.00034739432157948613,0.0002712019777391106,0.0005383873940445483,0.0006047365604899824,0.0004865065566264093,0.0005230655078776181,0.00037077124579809606,0.00029850832652300596,0.0003197174519300461,0.0004031391581520438,0.0004146713763475418,0.0003056380955968052,0.00042298153857700527,0.00040821637958288193,0.0002868537849280983,0.00027501932345330715,0.00036227068630978465,0.00023784447694197297,0.00021334436314646155,0.0003469099465291947,0.00027689282433129847,0.000374675466446206,0.0002756246249191463,0.0001793788542272523,0.0001652997307246551,0.00019625235290732235,0.00020915210188832134,0.0002258443710161373,0.0002654974814504385,0.00033709846320562065,0.00028405978810042143,0.0003002018202096224,0.0003029758809134364,0.0002285795344505459,0.00016518244228791445,0.00013044694787822664,0.00010356341226724908,0.00014492578338831663,0.0001670957135502249,0.0002835696504916996,0.00029533178894780576,0.0002517190296202898,0.00017025848501361907,0.00014056834334041923,0.00016571994638070464,0.00024958018912002444,0.00018918488058261573,0.00026828571571968496,0.00024239135382231325,0.0001604013959877193,0.0002375839394517243,0.00016698155377525836,0.00012639434135053307,0.00012886078911833465,0.00014658586587756872,0.00012770103057846427,
mae,0.3356623947620392,0.1117725744843483,0.0868619978427887,0.06735873967409134,0.06781051307916641,0.04766838997602463,0.04336235672235489,0.03392025828361511,0.02957034856081009,0.027674362063407898,0.029010526835918427,0.025048373267054558,0.03163617476820946,0.029981298372149467,0.03905726596713066,0.021753275766968727,0.02534683234989643,0.025835037231445312,0.023723646998405457,0.035770297050476074,0.03382032737135887,0.02680111862719059,0.021360091865062714,0.019030103459954262,0.03108241595327854,0.020657744258642197,0.018204163759946823,0.018634147942066193,0.023815669119358063,0.02565557323396206,0.022063853219151497,0.020638518035411835,0.023285575211048126,0.02272760681807995,0.0198829285800457,0.017430491745471954,0.015091477893292904,0.012048467062413692,0.011983323842287064,0.015367399901151657,0.013017421588301659,0.01454894058406353,0.011170558631420135,0.011300594545900822,0.014359664171934128,0.012899791821837425,0.018808193504810333,0.02053435891866684,0.01822899840772152,0.019557319581508636,0.015429376624524593,0.013817860744893551,0.014006465673446655,0.016246909275650978,0.016913684085011482,0.013671007007360458,0.017143946141004562,0.017315572127699852,0.013519249856472015,0.013357682153582573,0.015840379521250725,0.012067985720932484,0.011732634156942368,0.015096031129360199,0.013545253314077854,0.01617526076734066,0.013785488903522491,0.010694536380469799,0.010110249742865562,0.011119859293103218,0.011657766066491604,0.011993534862995148,0.013141377829015255,0.015339599922299385,0.013796459883451462,0.014366477727890015,0.015142720192670822,0.01235795859247446,0.0100715896114707,0.008993311785161495,0.007912836968898773,0.009278848767280579,0.010217513889074326,0.013952369801700115,0.014613684266805649,0.013097644783556461,0.010745758190751076,0.00981548149138689,0.010016417130827904,0.012894010171294212,0.010901658795773983,0.01362439151853323,0.012820661067962646,0.009978551417589188,0.012930129654705524,0.01024919655174017,0.00890734139829874,0.008930545300245285,0.009691388346254826,0.009067283011972904,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 29867     
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 29868     
=================================================================
Total params: 59,735
Trainable params: 59,735
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 8)                 40        
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 32)                288       
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 128)               4224      
_________________________________________________________________
encoding_layer_4 (Dense)     (None, 128)               16512     
_________________________________________________________________
encoding_layer_5 (Dense)     (None, 64)                8256      
_________________________________________________________________
encoding_layer_6 (Dense)     (None, 8)                 520       
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 27        
=================================================================
Total params: 29,867
Trainable params: 29,867
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 8)                 32        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 64)                576       
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 128)               8320      
_________________________________________________________________
decoding_layer_4 (Dense)     (None, 128)               16512     
_________________________________________________________________
decoding_layer_5 (Dense)     (None, 32)                4128      
_________________________________________________________________
decoding_layer_6 (Dense)     (None, 8)                 264       
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 36        
=================================================================
Total params: 29,868
Trainable params: 29,868
Non-trainable params: 0
_________________________________________________________________
