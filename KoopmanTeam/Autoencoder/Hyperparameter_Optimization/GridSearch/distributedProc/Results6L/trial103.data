2021-06-26
loss,4.299695014953613,3.884553909301758,3.829404592514038,3.8288609981536865,3.8290090560913086,3.828902244567871,4.033838748931885,3.8314321041107178,3.828925371170044,3.829529047012329,3.8291656970977783,3.828754186630249,3.8289711475372314,3.828803062438965,3.8284695148468018,3.8284482955932617,3.8293676376342773,3.8286588191986084,3.8294026851654053,3.8289740085601807,3.8327829837799072,3.8289217948913574,3.829044818878174,3.8292171955108643,4.211821556091309,4.451745510101318,4.4242682456970215,4.424072742462158,4.424121856689453,4.424233436584473,4.424181938171387,4.424076080322266,4.424232482910156,4.424199104309082,4.424288749694824,4.42424201965332,4.424279689788818,4.4244303703308105,4.424306392669678,4.424102306365967,4.424200057983398,4.424226760864258,4.424354076385498,4.424201488494873,4.423994541168213,4.424228191375732,4.424242973327637,4.424259662628174,4.424350738525391,4.424159049987793,4.424199104309082,4.423949718475342,4.424304008483887,4.424005508422852,4.424399375915527,4.4242401123046875,4.424365043640137,4.424332141876221,4.424149990081787,4.423969268798828,4.424187660217285,4.424149513244629,4.42404317855835,4.424079418182373,4.424252986907959,4.424220085144043,4.4243550300598145,4.423992156982422,4.424119472503662,4.424293041229248,4.424237251281738,4.424350738525391,4.424047470092773,4.424279689788818,4.424118518829346,4.424370765686035,4.4241557121276855,4.424274444580078,4.424240589141846,4.42392110824585,4.42424201965332,4.424312114715576,4.424316883087158,4.424148082733154,4.424063682556152,4.424117088317871,4.424245834350586,4.424058437347412,4.424099922180176,4.424190521240234,4.424290657043457,4.424215316772461,4.424304008483887,4.424158096313477,4.424193859100342,4.424211025238037,4.424200057983398,4.424241542816162,4.424288749694824,4.4240641593933105,
mse,4.700222969055176,3.7795257568359375,3.6713619232177734,3.670340061187744,3.670620918273926,3.6703994274139404,4.537471771240234,3.6752190589904785,3.6704742908477783,3.671621799468994,3.6709232330322266,3.670144557952881,3.670553207397461,3.670217990875244,3.6696178913116455,3.669546127319336,3.6713171005249023,3.669978618621826,3.6713716983795166,3.6705682277679443,3.677858829498291,3.670449733734131,3.6706643104553223,3.670985221862793,4.872812271118164,5.053609848022461,4.894426345825195,4.894002437591553,4.894105434417725,4.894354820251465,4.894243240356445,4.894008636474609,4.894353866577148,4.894274711608887,4.894481658935547,4.8943705558776855,4.89445686340332,4.89479398727417,4.894511699676514,4.894061088562012,4.894282817840576,4.894331932067871,4.894619941711426,4.894285678863525,4.893828868865967,4.894348621368408,4.89437198638916,4.894412040710449,4.894608974456787,4.89418363571167,4.89427375793457,4.893728733062744,4.894505500793457,4.8938517570495605,4.894714832305908,4.894373416900635,4.894641876220703,4.894572734832764,4.894165515899658,4.893778324127197,4.894251823425293,4.894169807434082,4.893932819366455,4.894011974334717,4.894393444061279,4.894325256347656,4.894618034362793,4.893819808959961,4.89410400390625,4.894491672515869,4.894365310668945,4.894612789154053,4.893942356109619,4.8944597244262695,4.894102096557617,4.894654750823975,4.8941802978515625,4.894442081451416,4.894371509552002,4.893667221069336,4.894371032714844,4.894522190093994,4.894537448883057,4.894163608551025,4.893979549407959,4.894092559814453,4.894379138946533,4.893966197967529,4.894056797027588,4.894257068634033,4.894479751586914,4.894314289093018,4.894507884979248,4.894189357757568,4.894267559051514,4.894302845001221,4.894281387329102,4.894374847412109,4.894471645355225,4.893988609313965,
mae,2.104978561401367,1.7725577354431152,1.6641333103179932,1.6582350730895996,1.6570227146148682,1.655996561050415,1.7721737623214722,1.671961784362793,1.655107855796814,1.6530338525772095,1.6522419452667236,1.6517128944396973,1.6516435146331787,1.6514456272125244,1.6512272357940674,1.6511725187301636,1.6515427827835083,1.6512330770492554,1.6515454053878784,1.6513324975967407,1.6743282079696655,1.659116268157959,1.6518287658691406,1.6517775058746338,1.8359732627868652,2.196678638458252,2.199915647506714,2.199794054031372,2.199824333190918,2.199894905090332,2.1998631954193115,2.199796438217163,2.199894666671753,2.1998724937438965,2.1999309062957764,2.199899435043335,2.1999239921569824,2.2000198364257812,2.199939012527466,2.1998114585876465,2.1998746395111084,2.1998887062072754,2.1999704837799072,2.199875831604004,2.1997454166412354,2.1998934745788574,2.199899911880493,2.19991135597229,2.199967384338379,2.1998467445373535,2.1998722553253174,2.199716806411743,2.1999382972717285,2.1997523307800293,2.199997901916504,2.1999001502990723,2.199976682662964,2.1999573707580566,2.1998414993286133,2.19973087310791,2.1998658180236816,2.1998424530029297,2.199774980545044,2.1997976303100586,2.19990611076355,2.1998867988586426,2.19996976852417,2.1997430324554443,2.1998238563537598,2.1999335289001465,2.1998980045318604,2.1999683380126953,2.199777603149414,2.1999247074127197,2.1998233795166016,2.1999802589416504,2.199845314025879,2.199920177459717,2.199899911880493,2.199699640274048,2.199899673461914,2.1999425888061523,2.1999471187591553,2.199840545654297,2.1997885704040527,2.1998207569122314,2.199902296066284,2.199784755706787,2.199810266494751,2.1998674869537354,2.1999311447143555,2.199883222579956,2.1999387741088867,2.199848175048828,2.1998701095581055,2.1998798847198486,2.19987416267395,2.1999008655548096,2.1999282836914062,2.199791193008423,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 534163    
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 534164    
=================================================================
Total params: 1,068,327
Trainable params: 1,068,327
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 16)                80        
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 256)               4352      
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 512)               131584    
_________________________________________________________________
encoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
encoding_layer_5 (Dense)     (None, 256)               131328    
_________________________________________________________________
encoding_layer_6 (Dense)     (None, 16)                4112      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 51        
=================================================================
Total params: 534,163
Trainable params: 534,163
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 16)                64        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 256)               4352      
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 512)               131584    
_________________________________________________________________
decoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
decoding_layer_5 (Dense)     (None, 256)               131328    
_________________________________________________________________
decoding_layer_6 (Dense)     (None, 16)                4112      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 68        
=================================================================
Total params: 534,164
Trainable params: 534,164
Non-trainable params: 0
_________________________________________________________________
