2021-06-26
loss,0.5058151483535767,0.2521440088748932,0.23460310697555542,0.2293682098388672,0.22333262860774994,0.2141120731830597,0.20477330684661865,0.1902686059474945,0.16942639648914337,0.1444278359413147,0.13518105447292328,0.08596120029687881,0.07514195144176483,0.07978013157844543,0.10630340874195099,0.09091613441705704,0.06293889880180359,0.04125572741031647,0.0407424159348011,0.05141123756766319,0.05838683247566223,0.04517991840839386,0.045865654945373535,0.06928378343582153,0.05665760859847069,0.05776088684797287,0.05453015863895416,0.04520423710346222,0.03878376632928848,0.04230041801929474,0.049520477652549744,0.04591372609138489,0.046773966401815414,0.04875488579273224,0.041832346469163895,0.03568699210882187,0.039918288588523865,0.03824479505419731,0.03505595028400421,0.035671066492795944,0.02920442633330822,0.0318140983581543,0.04005991294980049,0.04576548561453819,0.041986994445323944,0.03749074786901474,0.03505049645900726,0.03203901648521423,0.031510286033153534,0.035626307129859924,0.03779899701476097,0.03154955431818962,0.03290976956486702,0.036112431436777115,0.032519467175006866,0.030188018456101418,0.03484809026122093,0.030294546857476234,0.01948927715420723,0.018541069701313972,0.01894722506403923,0.02164478786289692,0.027128463611006737,0.03053561970591545,0.03211484104394913,0.03313868120312691,0.03186599910259247,0.02601144090294838,0.02918907441198826,0.031786542385816574,0.03420312702655792,0.029813675209879875,0.02731163054704666,0.023856494575738907,0.018080713227391243,0.027867518365383148,0.030197754502296448,0.032388851046562195,0.029984332621097565,0.02494276501238346,0.025355562567710876,0.02408662810921669,0.025685526430606842,0.02303147315979004,0.03132554516196251,0.030706482008099556,0.024851245805621147,0.022212494164705276,0.02474249340593815,0.022724008187651634,0.02407444268465042,0.02348683401942253,0.03289283439517021,0.030236482620239258,0.02561548724770546,0.02253311313688755,0.02291829138994217,0.026395926252007484,0.02479235641658306,0.027339495718479156,
mse,0.13837532699108124,0.020874055102467537,0.018941884860396385,0.018399620428681374,0.01764662377536297,0.016446063295006752,0.015304988250136375,0.013173445127904415,0.008832847699522972,0.0064561921171844006,0.005683131981641054,0.002164965495467186,0.0016453538555651903,0.001893809181638062,0.003298258874565363,0.00229034386575222,0.001204415108077228,0.000504932424519211,0.00048066972522065043,0.0007997644133865833,0.0009903202299028635,0.0006045560585334897,0.0006209113635122776,0.0013252812204882503,0.0009509476949460804,0.0009342126431874931,0.0008285432122647762,0.0005899809766560793,0.0004492713196668774,0.0005263570928946137,0.0006787629681639373,0.0005888614105060697,0.0006131685804575682,0.0006463855970650911,0.0005022470140829682,0.0003882042074110359,0.0004595725331455469,0.0004195947840344161,0.00036022518179379404,0.00036462186835706234,0.0002548507764004171,0.0002941117563750595,0.00045926979510113597,0.0005742059438489377,0.0004880726628471166,0.00039583269972354174,0.00034688401501625776,0.000297508027870208,0.0002868878946173936,0.0003631599247455597,0.00039558165008202195,0.00029517675284296274,0.0003131988341920078,0.000362717779353261,0.0002959785342682153,0.00026434697792865336,0.0003377316752448678,0.0002675944415386766,0.00011367185652488843,0.00010256451787427068,0.00010279950947733596,0.00014374926104210317,0.0002207162178819999,0.0002668471715878695,0.0002969862543977797,0.0003056775312870741,0.0002856339851859957,0.00020112391212023795,0.00024776451755315065,0.0002861458924598992,0.00032385808299295604,0.0002584031317383051,0.0002155588153982535,0.00016259816766250879,0.0001006670790957287,0.0002165971091017127,0.0002592169039417058,0.0002863554982468486,0.0002513582876417786,0.00018788939632941037,0.00018658916815184057,0.0001669573102844879,0.00018240101053379476,0.00015730898303445429,0.0002740912896115333,0.0002621910534799099,0.00018038900452665985,0.00014185340842232108,0.0001759985607350245,0.00015277859347406775,0.00016585482808295637,0.0001639133261051029,0.0002986020117532462,0.0002546022878959775,0.00018700606597121805,0.00014559135888703167,0.00015421828720718622,0.00019579798390623182,0.00017841231601778418,0.00020747272355947644,
mae,0.21994617581367493,0.10764497518539429,0.09911543875932693,0.09704992920160294,0.09492550790309906,0.09155324101448059,0.08774898201227188,0.08161452412605286,0.07005664706230164,0.060179270803928375,0.05642561614513397,0.03619220852851868,0.0316125750541687,0.03320521488785744,0.04296416416764259,0.037572771310806274,0.025991039350628853,0.01734025962650776,0.017082879319787025,0.021482212468981743,0.02487894706428051,0.019135739654302597,0.019651563838124275,0.028312118723988533,0.02310631424188614,0.024266695603728294,0.02244533970952034,0.01903160661458969,0.016146251931786537,0.01758360117673874,0.02065911516547203,0.019445400685071945,0.019355222582817078,0.019903045147657394,0.017607511952519417,0.015257734805345535,0.016635794192552567,0.01608247123658657,0.014542835764586926,0.015139278024435043,0.012373740784823895,0.013394482433795929,0.01671532168984413,0.018993759527802467,0.01733582653105259,0.015754692256450653,0.014770468696951866,0.013300803489983082,0.01321331039071083,0.014883624389767647,0.015591501258313656,0.01330533530563116,0.014047479256987572,0.015208150260150433,0.01373127568513155,0.012803333811461926,0.0145439263433218,0.012908685952425003,0.008202522061765194,0.007855763658881187,0.007968964986503124,0.009134027175605297,0.011455008760094643,0.012796098366379738,0.013726599514484406,0.013801549561321735,0.013452245853841305,0.010825181379914284,0.012558674439787865,0.01335268933326006,0.014071840792894363,0.012471940368413925,0.011751439422369003,0.010259738191962242,0.007620376534759998,0.01181704644113779,0.012892693281173706,0.013664858415722847,0.013011486269533634,0.0104948990046978,0.010694677010178566,0.010182959027588367,0.011047852225601673,0.00987330637872219,0.01305977813899517,0.012683586217463017,0.010510279797017574,0.00952801015228033,0.010417223908007145,0.009603668935596943,0.010311536490917206,0.009924070909619331,0.013738981448113918,0.01244249939918518,0.010861041955649853,0.00952533446252346,0.009719137102365494,0.011200368404388428,0.010495171882212162,0.011540403589606285,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 78843     
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 78844     
=================================================================
Total params: 157,687
Trainable params: 157,687
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 8)                 40        
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 32)                288       
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 256)               8448      
_________________________________________________________________
encoding_layer_4 (Dense)     (None, 256)               65792     
_________________________________________________________________
encoding_layer_5 (Dense)     (None, 16)                4112      
_________________________________________________________________
encoding_layer_6 (Dense)     (None, 8)                 136       
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 27        
=================================================================
Total params: 78,843
Trainable params: 78,843
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 8)                 32        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 16)                144       
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 256)               4352      
_________________________________________________________________
decoding_layer_4 (Dense)     (None, 256)               65792     
_________________________________________________________________
decoding_layer_5 (Dense)     (None, 32)                8224      
_________________________________________________________________
decoding_layer_6 (Dense)     (None, 8)                 264       
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 36        
=================================================================
Total params: 78,844
Trainable params: 78,844
Non-trainable params: 0
_________________________________________________________________
