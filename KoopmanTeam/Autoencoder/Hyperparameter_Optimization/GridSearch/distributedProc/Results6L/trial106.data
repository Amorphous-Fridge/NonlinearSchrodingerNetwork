2021-06-26
loss,4.024630546569824,3.8296213150024414,3.8287534713745117,3.826263427734375,4.404046535491943,4.424246788024902,4.424047470092773,4.424132823944092,4.424153804779053,4.4245381355285645,4.424231052398682,4.424194812774658,4.424046516418457,4.424358367919922,4.424142360687256,4.424075126647949,4.424138069152832,4.424113750457764,4.424257278442383,4.423978805541992,4.424347877502441,4.424165725708008,4.424213409423828,4.424184799194336,4.424266338348389,4.424212455749512,4.42437219619751,4.424350738525391,4.424233436584473,4.424201488494873,4.4239821434021,4.424090385437012,4.423923015594482,4.4240946769714355,4.424322128295898,4.424309253692627,4.424190044403076,4.424239635467529,4.424059867858887,4.424276351928711,4.424281597137451,4.424121856689453,4.4242777824401855,4.424178600311279,4.42441987991333,4.424153804779053,4.424251556396484,4.424336910247803,4.424294471740723,4.424106597900391,4.424232006072998,4.424160003662109,4.424108982086182,4.4244489669799805,4.424175262451172,4.424402236938477,4.424193382263184,4.424142837524414,4.4241557121276855,4.423868179321289,4.424188137054443,4.424068927764893,4.423947334289551,4.4242448806762695,4.424313068389893,4.42418098449707,4.424313068389893,4.424208164215088,4.424325466156006,4.424195766448975,4.424250602722168,4.424500942230225,4.4240875244140625,4.423980236053467,4.424272537231445,4.424165725708008,4.424200057983398,4.424250602722168,4.424185276031494,4.424108982086182,4.424077033996582,4.424005031585693,4.4243364334106445,4.424164295196533,4.424124240875244,4.424205780029297,4.4244160652160645,4.424142837524414,4.424333095550537,4.423986434936523,4.424318313598633,4.424089431762695,4.424321174621582,4.424276828765869,4.424135684967041,4.424403190612793,4.424147129058838,4.424370765686035,4.424323558807373,4.424058437347412,
mse,4.354074478149414,3.6717703342437744,3.6701385974884033,3.666208505630493,4.854658126831055,4.894382953643799,4.893941879272461,4.894131660461426,4.894179821014404,4.895021438598633,4.894350051879883,4.894270420074463,4.893944263458252,4.894626617431641,4.894150257110596,4.894004821777344,4.894147872924805,4.894084453582764,4.894403457641602,4.893794059753418,4.894607067108154,4.894205093383789,4.8943095207214355,4.894247055053711,4.89442253112793,4.894311904907227,4.894651889801025,4.89461088180542,4.894350528717041,4.894284248352051,4.893803596496582,4.894038200378418,4.893672943115234,4.894047737121582,4.894549369812012,4.894521713256836,4.894255638122559,4.894372940063477,4.8939714431762695,4.894444942474365,4.8944621086120605,4.894103050231934,4.894449710845947,4.8942341804504395,4.894763469696045,4.8941802978515625,4.8943963050842285,4.894585132598877,4.894486904144287,4.894075393676758,4.894351959228516,4.894190788269043,4.894078731536865,4.894827365875244,4.894223213195801,4.8947248458862305,4.89426326751709,4.8941545486450195,4.894180774688721,4.893550872802734,4.894254684448242,4.89399528503418,4.893726825714111,4.894380569458008,4.894526958465576,4.894238471984863,4.894527435302734,4.894294261932373,4.89456033706665,4.894272327423096,4.8943891525268555,4.894941329956055,4.894035816192627,4.893798351287842,4.894441604614258,4.894200325012207,4.894278049468994,4.894393444061279,4.8942484855651855,4.8940815925598145,4.894012928009033,4.893848896026611,4.894573211669922,4.894200325012207,4.894114017486572,4.894296646118164,4.894756317138672,4.894153118133545,4.894571304321289,4.8938069343566895,4.894542217254639,4.894031524658203,4.89454460144043,4.894442558288574,4.894145965576172,4.894725322723389,4.894164562225342,4.89465856552124,4.894554615020752,4.893963813781738,
mae,1.8151135444641113,1.6617498397827148,1.6555296182632446,1.653875708580017,2.187065839767456,2.1999030113220215,2.199777841567993,2.19983172416687,2.199845314025879,2.200084686279297,2.1998937129974365,2.1998708248138428,2.1997780799865723,2.199972152709961,2.1998369693756104,2.1997950077056885,2.199836492538452,2.1998183727264404,2.19990873336792,2.1997358798980713,2.1999669075012207,2.199852228164673,2.1998820304870605,2.199864387512207,2.19991397857666,2.1998822689056396,2.199979782104492,2.199968099594116,2.1998939514160156,2.1998748779296875,2.1997385025024414,2.19980525970459,2.1997013092041016,2.199807643890381,2.1999502182006836,2.1999428272247314,2.1998672485351562,2.199899911880493,2.1997857093811035,2.199920415878296,2.199925184249878,2.1998233795166016,2.1999223232269287,2.1998610496520996,2.200011730194092,2.1998450756073,2.199907064437866,2.199960708618164,2.199932098388672,2.1998157501220703,2.1998941898345947,2.1998484134674072,2.1998167037963867,2.2000293731689453,2.1998581886291504,2.200000524520874,2.19986891746521,2.199838161468506,2.199845552444458,2.199666738510132,2.199866771697998,2.1997923851013184,2.199716091156006,2.1999025344848633,2.199943780899048,2.199862003326416,2.199943780899048,2.199877977371216,2.199953556060791,2.199871301651001,2.199904680252075,2.200061559677124,2.1998045444488525,2.1997368335723877,2.1999194622039795,2.1998510360717773,2.1998729705810547,2.19990611076355,2.1998648643493652,2.199817419052124,2.1997978687286377,2.199751138687134,2.1999573707580566,2.1998510360717773,2.199826240539551,2.199878454208374,2.200009346008301,2.1998376846313477,2.1999564170837402,2.1997392177581787,2.19994854927063,2.199803352355957,2.199948787689209,2.1999199390411377,2.199836015701294,2.200000047683716,2.199841022491455,2.199981689453125,2.199951410293579,2.19978404045105,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 538339    
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 538340    
=================================================================
Total params: 1,076,679
Trainable params: 1,076,679
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 32)                160       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 256)               8448      
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 512)               131584    
_________________________________________________________________
encoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
encoding_layer_5 (Dense)     (None, 256)               131328    
_________________________________________________________________
encoding_layer_6 (Dense)     (None, 16)                4112      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 51        
=================================================================
Total params: 538,339
Trainable params: 538,339
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 16)                64        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 256)               4352      
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 512)               131584    
_________________________________________________________________
decoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
decoding_layer_5 (Dense)     (None, 256)               131328    
_________________________________________________________________
decoding_layer_6 (Dense)     (None, 32)                8224      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 132       
=================================================================
Total params: 538,340
Trainable params: 538,340
Non-trainable params: 0
_________________________________________________________________
