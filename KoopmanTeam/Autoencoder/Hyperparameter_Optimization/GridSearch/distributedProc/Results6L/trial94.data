2021-06-26
loss,3.5534000396728516,2.2947354316711426,2.274486780166626,2.253775119781494,2.241607904434204,2.2379376888275146,2.237555742263794,2.2379822731018066,2.237255811691284,2.2378432750701904,2.2370145320892334,2.2375946044921875,2.237536907196045,2.23796010017395,2.2377724647521973,2.23760986328125,2.2381839752197266,2.23738431930542,2.237424850463867,2.2384510040283203,2.2387585639953613,2.237954616546631,2.2370994091033936,2.237039566040039,2.2384891510009766,2.2380058765411377,2.2381644248962402,2.2389168739318848,2.2388439178466797,2.23835825920105,2.238982915878296,2.237650156021118,2.2393784523010254,2.2375974655151367,2.238595485687256,2.2385313510894775,2.2385447025299072,2.237715005874634,2.2397263050079346,2.2384908199310303,2.238812208175659,2.243422031402588,2.238612413406372,2.225062608718872,0.7491236329078674,0.4448481500148773,0.4448826313018799,0.4446672797203064,0.44498977065086365,0.44396987557411194,0.4450211226940155,0.44432970881462097,0.4447469413280487,0.44452765583992004,0.4448525607585907,0.4440804421901703,0.4443620443344116,0.4449170231819153,0.4451834559440613,0.4451010227203369,0.4446987807750702,0.4441026747226715,0.4443698823451996,0.4450514614582062,0.4445074498653412,0.4446178376674652,0.44409143924713135,0.4448574185371399,0.4446248710155487,0.44474709033966064,0.4456321597099304,0.4446653127670288,0.44494959712028503,0.44469714164733887,0.4456411302089691,0.44531017541885376,0.44539564847946167,0.4445856511592865,0.44484442472457886,0.44485747814178467,0.44586294889450073,0.4447847008705139,0.44525790214538574,0.4450749158859253,0.4446607530117035,0.4453586935997009,0.44492411613464355,0.4456655979156494,0.4448641538619995,0.4452357888221741,0.4449650049209595,0.44555824995040894,0.4447077810764313,0.44459983706474304,0.44513943791389465,0.4451315402984619,0.44460946321487427,0.4451387822628021,0.44484877586364746,0.4442923665046692,
mse,3.6504831314086914,1.3322404623031616,1.3091109991073608,1.283194899559021,1.2691948413848877,1.2650854587554932,1.2646106481552124,1.265091061592102,1.264330267906189,1.2649842500686646,1.2640671730041504,1.2647216320037842,1.2646138668060303,1.2651177644729614,1.2648308277130127,1.26466703414917,1.2653309106826782,1.2644902467727661,1.2645058631896973,1.265743374824524,1.2660400867462158,1.2650752067565918,1.2641493082046509,1.2641003131866455,1.265718936920166,1.2651476860046387,1.2653160095214844,1.2661689519882202,1.266076922416687,1.2655857801437378,1.2662303447723389,1.2648032903671265,1.2667453289031982,1.2646936178207397,1.2657948732376099,1.265738606452942,1.265763521194458,1.2648119926452637,1.267094612121582,1.265702247619629,1.2660843133926392,1.2712180614471436,1.2658864259719849,1.2661871910095215,0.22207584977149963,0.05517397075891495,0.05514692887663841,0.05511721223592758,0.055195197463035583,0.05493864417076111,0.055186327546834946,0.05504259094595909,0.05511894077062607,0.05510798096656799,0.055150650441646576,0.0549439936876297,0.05504081770777702,0.055121928453445435,0.05521753802895546,0.05520409718155861,0.05512378737330437,0.05496674031019211,0.05501851439476013,0.05518560856580734,0.055080596357584,0.05510975793004036,0.054960306733846664,0.055153653025627136,0.05507016181945801,0.05513424798846245,0.0553477443754673,0.05510096997022629,0.055154476314783096,0.055107008665800095,0.0553482249379158,0.05523184686899185,0.05528448522090912,0.055060893297195435,0.05512542650103569,0.05516637861728668,0.055396534502506256,0.055129148066043854,0.055258698761463165,0.05521496757864952,0.05511101335287094,0.055285368114709854,0.05518889054656029,0.05529080331325531,0.05514470487833023,0.05526231229305267,0.05519618093967438,0.05530444160103798,0.05511199310421944,0.05509168654680252,0.055225271731615067,0.055214714258909225,0.05511438846588135,0.055224571377038956,0.05513518676161766,0.05502202361822128,
mae,1.4822601079940796,0.7635536789894104,0.7428591847419739,0.720381498336792,0.7044440507888794,0.6982378959655762,0.6981484293937683,0.6980302929878235,0.6980294585227966,0.6979953646659851,0.6978685259819031,0.6978346109390259,0.6981000304222107,0.698218584060669,0.6981441378593445,0.6980894207954407,0.6983577013015747,0.6978803873062134,0.6981386542320251,0.6984125375747681,0.6986171007156372,0.6986982822418213,0.6980369687080383,0.6981678009033203,0.6985072493553162,0.6988711357116699,0.6987144947052002,0.6989454030990601,0.6990498900413513,0.6982315182685852,0.6997573971748352,0.6982964873313904,0.6993559002876282,0.6986266374588013,0.6991217136383057,0.6986626386642456,0.6987683176994324,0.6985534429550171,0.6988857984542847,0.6987709999084473,0.6991411447525024,0.7078835964202881,0.6995659470558167,0.7251156568527222,0.3231656551361084,0.19727823138237,0.19738762080669403,0.19727623462677002,0.1973942369222641,0.19696208834648132,0.19745498895645142,0.19710804522037506,0.1972581148147583,0.19722987711429596,0.19732776284217834,0.1970358043909073,0.19707055389881134,0.19734857976436615,0.19748196005821228,0.19742876291275024,0.19724635779857635,0.1970188319683075,0.19711609184741974,0.19738361239433289,0.1971692442893982,0.19723264873027802,0.19696107506752014,0.1973465085029602,0.19724534451961517,0.197239488363266,0.1976955085992813,0.19731706380844116,0.19736309349536896,0.19725818932056427,0.1977294683456421,0.19749130308628082,0.19761964678764343,0.1971757709980011,0.19732053577899933,0.19734838604927063,0.19776003062725067,0.19729217886924744,0.19749756157398224,0.1974259316921234,0.1972047984600067,0.19761264324188232,0.19734661281108856,0.19766117632389069,0.19736488163471222,0.19749799370765686,0.1974664181470871,0.1976291537284851,0.19725117087364197,0.1971704512834549,0.19749118387699127,0.1974629908800125,0.19720609486103058,0.1974334865808487,0.19735866785049438,0.19708260893821716,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 472739    
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 472740    
=================================================================
Total params: 945,479
Trainable params: 945,479
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 32)                160       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 128)               4224      
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 512)               66048     
_________________________________________________________________
encoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
encoding_layer_5 (Dense)     (None, 256)               131328    
_________________________________________________________________
encoding_layer_6 (Dense)     (None, 32)                8224      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 99        
=================================================================
Total params: 472,739
Trainable params: 472,739
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 32)                128       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 256)               8448      
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 512)               131584    
_________________________________________________________________
decoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
decoding_layer_5 (Dense)     (None, 128)               65664     
_________________________________________________________________
decoding_layer_6 (Dense)     (None, 32)                4128      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 132       
=================================================================
Total params: 472,740
Trainable params: 472,740
Non-trainable params: 0
_________________________________________________________________
