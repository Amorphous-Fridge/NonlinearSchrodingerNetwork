2021-06-26
loss,4.537497043609619,4.424276351928711,4.424260139465332,4.424235820770264,4.424285411834717,4.424198627471924,4.424253940582275,4.424170017242432,4.424325466156006,4.424102783203125,4.424197196960449,4.424340724945068,4.424289703369141,4.424413204193115,4.424253940582275,4.424417018890381,4.424003601074219,4.424392223358154,4.423818588256836,4.4244914054870605,4.424177169799805,4.424373626708984,4.424292087554932,4.424348831176758,4.424401760101318,4.424325466156006,4.424289703369141,4.42396879196167,4.423824310302734,4.424084663391113,4.424208641052246,4.4241251945495605,4.424276351928711,4.4242377281188965,4.4242262840271,4.424330711364746,4.423935413360596,4.42412805557251,4.424553871154785,4.424209117889404,4.424263954162598,4.424190998077393,4.424112319946289,4.424108982086182,4.424048900604248,4.424263954162598,4.423994541168213,4.424238681793213,4.424252033233643,4.424217700958252,4.424172878265381,4.424178600311279,4.424149036407471,4.424312114715576,4.424196243286133,4.4241042137146,4.42425012588501,4.424309253692627,4.424197196960449,4.4244890213012695,4.424370288848877,4.424036979675293,4.42430305480957,4.424106597900391,4.424188137054443,4.424129962921143,4.424175262451172,4.424200057983398,4.4239301681518555,4.4242939949035645,4.424139499664307,4.424152374267578,4.424265384674072,4.423858165740967,4.4242329597473145,4.42427396774292,4.424396991729736,4.424234390258789,4.4244585037231445,4.424108982086182,4.424154758453369,4.424080848693848,4.424106597900391,4.4242424964904785,4.424344539642334,4.424111843109131,4.424309730529785,4.424269199371338,4.424325942993164,4.42433500289917,4.424213409423828,4.424393653869629,4.424215793609619,4.42402982711792,4.424409866333008,4.424293041229248,4.424134731292725,4.424225807189941,4.424179553985596,4.424102306365967,
mse,5.300194263458252,4.8944478034973145,4.894416332244873,4.89436149597168,4.894472122192383,4.894274711608887,4.894397735595703,4.894217491149902,4.894557952880859,4.894068717956543,4.894270420074463,4.894588947296143,4.894476413726807,4.894746780395508,4.894400119781494,4.894752502441406,4.893847465515137,4.89470100402832,4.893436431884766,4.894919395446777,4.894228935241699,4.894668102264404,4.894484996795654,4.894603252410889,4.894726753234863,4.894557952880859,4.894476413726807,4.893767356872559,4.893453598022461,4.894023895263672,4.894298553466797,4.894116401672363,4.894443511962891,4.8943634033203125,4.8943400382995605,4.894568920135498,4.8936991691589355,4.894126892089844,4.8950581550598145,4.894301891326904,4.894416332244873,4.8942670822143555,4.894088268280029,4.894074440002441,4.893947601318359,4.894420146942139,4.893828868865967,4.894369125366211,4.894393444061279,4.894317626953125,4.8942155838012695,4.894235134124756,4.894166469573975,4.89453125,4.89427375793457,4.894067764282227,4.894394874572754,4.894518852233887,4.894278049468994,4.894916534423828,4.894653797149658,4.8939208984375,4.894509792327881,4.894077301025391,4.894256114959717,4.894129753112793,4.894227981567383,4.894281387329102,4.893689155578613,4.894488334655762,4.894145965576172,4.894176959991455,4.894423961639404,4.893529415130615,4.894357204437256,4.894442558288574,4.894713878631592,4.894357204437256,4.894847869873047,4.894076347351074,4.894181728363037,4.894015789031982,4.894077301025391,4.894374847412109,4.894599914550781,4.894087314605713,4.894523620605469,4.894432544708252,4.8945631980896,4.89457368850708,4.894306659698486,4.894701957702637,4.894311904907227,4.893903732299805,4.894742965698242,4.894485950469971,4.894134998321533,4.8943400382995605,4.894238471984863,4.894067287445068,
mae,2.235593795776367,2.1999216079711914,2.1999127864837646,2.199897050857544,2.1999285221099854,2.1998722553253174,2.1999073028564453,2.1998560428619385,2.1999526023864746,2.1998136043548584,2.199871301651001,2.1999616622924805,2.199929714202881,2.2000067234039307,2.1999080181121826,2.200007915496826,2.199751377105713,2.19999361038208,2.199634313583374,2.2000558376312256,2.199859142303467,2.199984073638916,2.1999318599700928,2.199965715408325,2.2000010013580322,2.1999521255493164,2.199929714202881,2.199728012084961,2.199639081954956,2.199800729751587,2.1998791694641113,2.199827194213867,2.199920415878296,2.199897527694702,2.1998910903930664,2.1999564170837402,2.199708938598633,2.1998298168182373,2.2000951766967773,2.1998794078826904,2.1999127864837646,2.1998701095581055,2.199819564819336,2.1998157501220703,2.199779510498047,2.199913740158081,2.1997454166412354,2.199899435043335,2.19990611076355,2.1998844146728516,2.199855327606201,2.1998608112335205,2.1998417377471924,2.1999456882476807,2.1998722553253174,2.1998136043548584,2.199906349182129,2.199941635131836,2.1998729705810547,2.200054407119751,2.199979782104492,2.1997716426849365,2.199939250946045,2.1998162269592285,2.1998674869537354,2.199831008911133,2.1998589038848877,2.19987416267395,2.1997058391571045,2.1999332904815674,2.199835777282715,2.1998443603515625,2.1999144554138184,2.1996610164642334,2.1998956203460693,2.1999199390411377,2.1999967098236084,2.1998960971832275,2.200035333633423,2.1998157501220703,2.199846029281616,2.199799060821533,2.1998167037963867,2.1999006271362305,2.199965000152588,2.1998190879821777,2.1999425888061523,2.1999166011810303,2.1999542713165283,2.1999571323394775,2.1998813152313232,2.19999361038208,2.1998822689056396,2.1997668743133545,2.200005531311035,2.199932336807251,2.1998322010040283,2.1998908519744873,2.199861526489258,2.1998131275177,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 575811    
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 575812    
=================================================================
Total params: 1,151,623
Trainable params: 1,151,623
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 64)                320       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 256)               16640     
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 512)               131584    
_________________________________________________________________
encoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
encoding_layer_5 (Dense)     (None, 256)               131328    
_________________________________________________________________
encoding_layer_6 (Dense)     (None, 128)               32896     
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 387       
=================================================================
Total params: 575,811
Trainable params: 575,811
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 128)               512       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 256)               33024     
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 512)               131584    
_________________________________________________________________
decoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
decoding_layer_5 (Dense)     (None, 256)               131328    
_________________________________________________________________
decoding_layer_6 (Dense)     (None, 64)                16448     
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 260       
=================================================================
Total params: 575,812
Trainable params: 575,812
Non-trainable params: 0
_________________________________________________________________
