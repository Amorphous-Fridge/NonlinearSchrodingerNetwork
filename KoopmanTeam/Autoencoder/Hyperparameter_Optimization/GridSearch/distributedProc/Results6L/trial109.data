2021-06-26
loss,4.0344557762146,3.8392105102539062,3.835602045059204,3.8305301666259766,3.8290719985961914,3.828953504562378,3.829577922821045,3.8293275833129883,3.8292646408081055,3.8287429809570312,3.8293297290802,3.8286690711975098,3.851058006286621,3.8421132564544678,3.866380214691162,3.8313798904418945,3.82832407951355,3.829115629196167,3.8289003372192383,3.8289573192596436,3.8292202949523926,3.829252243041992,3.82932186126709,3.8291008472442627,3.8289217948913574,3.829235553741455,3.8296289443969727,3.8290374279022217,3.8288772106170654,3.8287386894226074,3.82952880859375,5.093966960906982,6.638489246368408,4.424187660217285,4.424248218536377,4.424285888671875,4.4240522384643555,4.4243550300598145,4.424203395843506,4.424153804779053,4.4240336418151855,4.424193382263184,4.424178600311279,4.424185276031494,4.424239158630371,4.4242658615112305,4.424102306365967,4.42424201965332,4.424226760864258,4.424132347106934,4.424318790435791,4.424315452575684,4.424242973327637,4.424310684204102,4.424429893493652,4.424212455749512,4.424426078796387,4.424314975738525,4.424261569976807,4.424356937408447,4.42431116104126,4.424142360687256,4.4242143630981445,4.4243879318237305,4.424107551574707,4.4240851402282715,4.424132823944092,4.424066066741943,4.424122333526611,4.424230575561523,4.424345016479492,4.424169063568115,4.424542427062988,4.424039363861084,4.42402458190918,4.424112796783447,4.424208164215088,4.424282550811768,4.42407751083374,4.423981666564941,4.424352645874023,4.424259662628174,4.4241251945495605,4.424332618713379,4.424062252044678,4.424255847930908,4.424242973327637,4.424176216125488,4.424208164215088,4.424198150634766,4.424163341522217,4.424375057220459,4.424013614654541,4.424279689788818,4.424200534820557,4.42420768737793,4.424287796020508,4.424150466918945,4.424217224121094,4.424111843109131,
mse,4.148687362670898,3.6900649070739746,3.6831815242767334,3.6735172271728516,3.670738935470581,3.6705079078674316,3.671703815460205,3.671221971511841,3.6711208820343018,3.670159339904785,3.6712284088134766,3.669984817504883,3.713555335998535,3.696019172668457,3.748650550842285,3.675124406814575,3.669343948364258,3.6708078384399414,3.67042875289917,3.6705129146575928,3.6710076332092285,3.6710922718048096,3.671205520629883,3.670806646347046,3.670480966567993,3.6710829734802246,3.671809673309326,3.670664072036743,3.670358657836914,3.670133352279663,3.671571731567383,19.35576629638672,81.45491027832031,4.8942484855651855,4.894385814666748,4.894468307495117,4.893955230712891,4.894624710083008,4.894284248352051,4.8941826820373535,4.8939127922058105,4.894257068634033,4.8942365646362305,4.89424467086792,4.894362926483154,4.894425868988037,4.894065856933594,4.894378185272217,4.894340991973877,4.89412784576416,4.894546031951904,4.894532203674316,4.894375324249268,4.894524097442627,4.894786834716797,4.89430046081543,4.894776344299316,4.894537925720215,4.894421577453613,4.894623279571533,4.894521236419678,4.8941521644592285,4.894305229187012,4.894694805145264,4.894077301025391,4.894026279449463,4.8941330909729,4.893985748291016,4.894110202789307,4.894350528717041,4.894601345062256,4.89420747756958,4.895037651062012,4.893925189971924,4.893893241882324,4.8940839767456055,4.894299507141113,4.8944597244262695,4.894005298614502,4.893795967102051,4.8946146965026855,4.894412517547607,4.894112586975098,4.894574165344238,4.893973350524902,4.894402027130127,4.894372940063477,4.894221305847168,4.8942975997924805,4.894282341003418,4.894199848175049,4.894659042358398,4.89387321472168,4.894457817077637,4.894278049468994,4.894294738769531,4.894468784332275,4.894171714782715,4.8943190574646,4.894083023071289,
mae,1.882972002029419,1.7064154148101807,1.6948082447052002,1.66650390625,1.658068060874939,1.6562247276306152,1.6551268100738525,1.6540244817733765,1.6532576084136963,1.6524986028671265,1.6523540019989014,1.6518391370773315,1.7058899402618408,1.7048242092132568,1.7215253114700317,1.6729681491851807,1.6538528203964233,1.6529755592346191,1.6523230075836182,1.6518774032592773,1.6516937017440796,1.6514859199523926,1.6513752937316895,1.651169776916504,1.6510214805603027,1.651088833808899,1.6512539386749268,1.6509079933166504,1.6508058309555054,1.650749683380127,1.651179313659668,2.2650346755981445,3.078784704208374,2.199864625930786,2.199904203414917,2.19992733001709,2.1997814178466797,2.1999716758728027,2.1998751163482666,2.1998462677001953,2.1997690200805664,2.1998674869537354,2.199861526489258,2.1998636722564697,2.199897527694702,2.199915885925293,2.1998131275177,2.199901819229126,2.1998913288116455,2.199831008911133,2.199949264526367,2.1999454498291016,2.1999008655548096,2.1999433040618896,2.2000184059143066,2.1998801231384277,2.20001482963562,2.1999473571777344,2.1999142169952393,2.1999716758728027,2.1999423503875732,2.1998374462127686,2.199881076812744,2.1999917030334473,2.1998159885406494,2.1998016834259033,2.1998322010040283,2.1997900009155273,2.1998255252838135,2.1998939514160156,2.199965000152588,2.199852705001831,2.200089454650879,2.1997733116149902,2.199763536453247,2.1998178958892822,2.1998789310455322,2.1999244689941406,2.199795722961426,2.1997361183166504,2.1999688148498535,2.199911117553711,2.199826240539551,2.1999568939208984,2.199786424636841,2.1999082565307617,2.1999001502990723,2.199857234954834,2.199878454208374,2.1998746395111084,2.1998510360717773,2.199981451034546,2.1997580528259277,2.1999247074127197,2.199873208999634,2.1998772621154785,2.199927568435669,2.199842929840088,2.1998844146728516,2.199817419052124,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 485315    
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 485316    
=================================================================
Total params: 970,631
Trainable params: 970,631
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 64)                320       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 256)               16640     
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 512)               131584    
_________________________________________________________________
encoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
encoding_layer_5 (Dense)     (None, 128)               65664     
_________________________________________________________________
encoding_layer_6 (Dense)     (None, 64)                8256      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 195       
=================================================================
Total params: 485,315
Trainable params: 485,315
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 64)                256       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 128)               8320      
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 512)               66048     
_________________________________________________________________
decoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
decoding_layer_5 (Dense)     (None, 256)               131328    
_________________________________________________________________
decoding_layer_6 (Dense)     (None, 64)                16448     
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 260       
=================================================================
Total params: 485,316
Trainable params: 485,316
Non-trainable params: 0
_________________________________________________________________
