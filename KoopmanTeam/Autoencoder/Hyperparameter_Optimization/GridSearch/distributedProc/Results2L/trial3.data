2021-06-26
loss,0.35399582982063293,0.22466252744197845,0.2092156857252121,0.1441272795200348,0.07166675478219986,0.05171714350581169,0.04408407583832741,0.039507340639829636,0.03602953255176544,0.03305227681994438,0.03065929003059864,0.0287779588252306,0.026823896914720535,0.025243591517210007,0.02438271790742874,0.023423124104738235,0.02270575799047947,0.022034738212823868,0.021189449355006218,0.020789897069334984,0.020047249272465706,0.019647710025310516,0.019180968403816223,0.018881211057305336,0.018450036644935608,0.01797286793589592,0.01770176738500595,0.017466863617300987,0.01719810999929905,0.017019277438521385,0.01663585752248764,0.01658148318529129,0.016263574361801147,0.015969330444931984,0.015957986935973167,0.01574227772653103,0.01563960872590542,0.015398535877466202,0.015314844436943531,0.015113042667508125,0.015003448352217674,0.014831731095910072,0.014757483266294003,0.01464295107871294,0.0145026333630085,0.014459937810897827,0.01427998673170805,0.014306413941085339,0.014089963398873806,0.01407385803759098,0.013889760710299015,0.01385331992059946,0.013712461106479168,0.013714684173464775,0.013577266596257687,0.013514629565179348,0.013473019935190678,0.013356871902942657,0.013310176320374012,0.013176314532756805,0.013171612285077572,0.01304478757083416,0.01303926669061184,0.01299290917813778,0.012972526252269745,0.012891653925180435,0.01279843132942915,0.012745031155645847,0.012656103819608688,0.012615356594324112,0.012597509659826756,0.012487825006246567,0.012445791624486446,0.012498420663177967,0.01235920563340187,0.012387348338961601,0.012315576896071434,0.012235267087817192,0.012278921902179718,0.012116778641939163,0.012161574326455593,0.012064573355019093,0.01209118403494358,0.012062418274581432,0.012010855600237846,0.011979352682828903,0.011829204857349396,0.011943526566028595,0.011753743514418602,0.011780763976275921,0.011771418154239655,0.011700876988470554,0.011756130494177341,0.011686202138662338,0.011584289371967316,0.011607890948653221,0.011533289216458797,0.011530130170285702,0.011500873602926731,0.011477050371468067,
mse,0.043747272342443466,0.01836046762764454,0.016315380111336708,0.00782181229442358,0.0018357675289735198,0.0010053557343780994,0.0007390020764432847,0.0005975335952825844,0.0004905466339550912,0.00040123879443854094,0.0003393632941879332,0.00029020413057878613,0.0002465910802129656,0.000217738765059039,0.0001991313329199329,0.0001816343137761578,0.00016913241415750235,0.00015829841140657663,0.00014510755136143416,0.0001391962869092822,0.00012766917643602937,0.0001229132030857727,0.00011602151789702475,0.00011223406181670725,0.00010656253289198503,0.00010084682435262948,9.715765918372199e-05,9.399259579367936e-05,9.030588989844546e-05,8.86087364051491e-05,8.494636131217703e-05,8.376195910386741e-05,8.04204901214689e-05,7.732540689175949e-05,7.698681292822585e-05,7.457677565980703e-05,7.364374323515221e-05,7.160820678109303e-05,7.03422338119708e-05,6.831853534094989e-05,6.755312642781064e-05,6.557695451192558e-05,6.480228330474347e-05,6.366425077430904e-05,6.245785334613174e-05,6.22821316937916e-05,6.0092046624049544e-05,6.0236561694182456e-05,5.844900442752987e-05,5.840468293172307e-05,5.700019028154202e-05,5.643039185088128e-05,5.535360469366424e-05,5.536728713195771e-05,5.4028318118071184e-05,5.370790677261539e-05,5.2835432143183425e-05,5.223920015851036e-05,5.173497265786864e-05,5.0403858040226623e-05,5.0593444029800594e-05,4.960777368978597e-05,4.929885108140297e-05,4.8905916628427804e-05,4.8983081796905026e-05,4.80414237244986e-05,4.7650260967202485e-05,4.6768825995968655e-05,4.633840580936521e-05,4.584899215842597e-05,4.594715210259892e-05,4.497492773225531e-05,4.464610537979752e-05,4.4880915083922446e-05,4.3952171836281195e-05,4.420741606736556e-05,4.359491140348837e-05,4.296202678233385e-05,4.33439745393116e-05,4.209867256577127e-05,4.2301518988097087e-05,4.169148451182991e-05,4.18145427829586e-05,4.167333827354014e-05,4.1135819628834724e-05,4.1075727494899184e-05,3.981006739195436e-05,4.0892980905482545e-05,3.941034447052516e-05,3.969324825447984e-05,3.9540482248412445e-05,3.9006386941764504e-05,3.943454794352874e-05,3.9031292544677854e-05,3.81661084247753e-05,3.844906677841209e-05,3.777430538320914e-05,3.796081364271231e-05,3.760993422474712e-05,3.7522480852203444e-05,
mae,0.1557377725839615,0.10861121863126755,0.09909627586603165,0.060059405863285065,0.030604353174567223,0.022437216714024544,0.019039923325181007,0.017038468271493912,0.015497708693146706,0.014090292155742645,0.013016791082918644,0.012236193753778934,0.011327589862048626,0.010705606080591679,0.010326823219656944,0.009937234222888947,0.009633777663111687,0.009390813298523426,0.009011712856590748,0.008860452100634575,0.008576162159442902,0.008341646753251553,0.008157617412507534,0.008115669712424278,0.007893601432442665,0.007683152332901955,0.007527611218392849,0.007500361185520887,0.0073182666674256325,0.007254047319293022,0.007120969705283642,0.007140779867768288,0.00694988202303648,0.00679468410089612,0.0067846463061869144,0.006712236907333136,0.006707541178911924,0.006586255505681038,0.0065675778314471245,0.0063927872106432915,0.006376773584634066,0.006285873707383871,0.00630460400134325,0.0063415635377168655,0.006191201973706484,0.006182377226650715,0.0060932813212275505,0.006130849942564964,0.006048368290066719,0.006039762869477272,0.005925058852881193,0.005810990463942289,0.005813769064843655,0.005846148356795311,0.005820779129862785,0.005749044474214315,0.005747884977608919,0.005727177951484919,0.005690218880772591,0.005660686641931534,0.0055799949914216995,0.005576660856604576,0.0055637722834944725,0.0055692452006042,0.005484218709170818,0.00551823852583766,0.005466299597173929,0.005446264054626226,0.005395774729549885,0.005433359183371067,0.005373939406126738,0.0053425198420882225,0.005322213284671307,0.005302335601300001,0.005279104690998793,0.005240263883024454,0.005230898968875408,0.005234354641288519,0.00523432157933712,0.005247135180979967,0.005083895288407803,0.0051614572294056416,0.005143699701875448,0.005150118842720985,0.0051619731821119785,0.005082338582724333,0.004996264819055796,0.005094361957162619,0.005059530958533287,0.0050250268541276455,0.0050112600438296795,0.004911974538117647,0.004967317450791597,0.004999322816729546,0.00498436251655221,0.005011478438973427,0.004892996046692133,0.004944090731441975,0.004908313043415546,0.004852302372455597,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 2643      
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 2644      
=================================================================
Total params: 5,287
Trainable params: 5,287
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 16)                80        
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 128)               2176      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 387       
=================================================================
Total params: 2,643
Trainable params: 2,643
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 128)               512       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 16)                2064      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 68        
=================================================================
Total params: 2,644
Trainable params: 2,644
Non-trainable params: 0
_________________________________________________________________
