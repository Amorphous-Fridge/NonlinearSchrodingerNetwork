2021-06-26
loss,0.2899884879589081,0.06944018602371216,0.03506079316139221,0.03102545440196991,0.030315350741147995,0.028444113209843636,0.0357479564845562,0.03482989966869354,0.03889777138829231,0.0303421001881361,0.027218148112297058,0.03006902150809765,0.030370984226465225,0.03646963834762573,0.03332117199897766,0.029740093275904655,0.023453405126929283,0.025180188938975334,0.029955746605992317,0.028943639248609543,0.030162405222654343,0.028872406110167503,0.02527974545955658,0.02195640839636326,0.019353613257408142,0.021823925897479057,0.024806270375847816,0.02675243467092514,0.023798637092113495,0.021444277837872505,0.024216454476118088,0.024304885417222977,0.020341603085398674,0.019606906920671463,0.02087022364139557,0.02175091952085495,0.02175631746649742,0.02519627846777439,0.021665725857019424,0.019144736230373383,0.017321325838565826,0.01701018027961254,0.0176252294331789,0.02405952475965023,0.0241838451474905,0.021240603178739548,0.01896771229803562,0.017326315864920616,0.0173955038189888,0.019575951620936394,0.019182786345481873,0.01793629862368107,0.01690574362874031,0.016088001430034637,0.015176908113062382,0.01430087722837925,0.019737323746085167,0.019248932600021362,0.017679302021861076,0.0165519118309021,0.015104115009307861,0.013624705374240875,0.015009871684014797,0.01884644106030464,0.019028598442673683,0.020059065893292427,0.02042112499475479,0.018602032214403152,0.016967272385954857,0.01601535640656948,0.015044179745018482,0.014129596762359142,0.013189712539315224,0.012290260754525661,0.011487581767141819,0.01439262367784977,0.017608070746064186,0.015978392213582993,0.014598468318581581,0.012509618885815144,0.014061041176319122,0.016366925090551376,0.017542961984872818,0.016061583533883095,0.01503921952098608,0.015464896336197853,0.017469050362706184,0.015967518091201782,0.015024607069790363,0.01433354988694191,0.013812619261443615,0.016310201957821846,0.01569611392915249,0.015490803867578506,0.014243514277040958,0.01359721552580595,0.013652128167450428,0.012820946052670479,0.012955540791153908,0.011171374469995499,
mse,0.03184228017926216,0.0016965547110885382,0.00036810338497161865,0.00028686749283224344,0.0002765240496955812,0.00024329939333256334,0.00039114485844038427,0.00036431741318665445,0.00045425587450154126,0.0002776259498205036,0.0002199198497692123,0.0002731192798819393,0.00026489378069527447,0.00038073596078902483,0.0003220137150492519,0.0002632467949297279,0.000171125924680382,0.0001878456532722339,0.0002582199522294104,0.00024328265863005072,0.0002543831942602992,0.00023631119984202087,0.00018829434702638537,0.00014320969057735056,0.00011395083856768906,0.00014343761722557247,0.00017508411838207394,0.00020393231534399092,0.00015643499500583857,0.00013654127542395145,0.00017229875084012747,0.00016190006863325834,0.00012241308286320418,0.00011639903095783666,0.00013072136789560318,0.00013724211021326482,0.00013944368402007967,0.0001779689482646063,0.00013238604879006743,0.00010448144894326106,9.050770313479006e-05,8.810434519546106e-05,9.75613875198178e-05,0.00016537342162337154,0.00016220718680415303,0.00012792892812285572,0.00010344353358959779,9.15273922146298e-05,9.325167775386944e-05,0.00011018016084562987,0.00010461482452228665,9.312378824688494e-05,8.350431744474918e-05,7.478354382328689e-05,6.792858039261773e-05,6.335726357065141e-05,0.00011407723650336266,0.00010334902617614716,8.947083551902324e-05,8.053687633946538e-05,6.928444054210559e-05,5.765327296103351e-05,7.01559183653444e-05,0.00010132584429811686,0.00010160106467083097,0.0001154048295575194,0.00011583793821046129,9.803120337892324e-05,8.384430111618713e-05,7.315648690564558e-05,6.632020813412964e-05,6.044479232514277e-05,5.350084029487334e-05,4.77509674965404e-05,4.209247708786279e-05,6.450057117035612e-05,8.844713011058047e-05,7.257154356921092e-05,6.24279782641679e-05,4.926326801069081e-05,6.26559994998388e-05,7.740771980024874e-05,8.881850953912362e-05,7.527149136876687e-05,6.685953849228099e-05,7.059781637508422e-05,8.4050559962634e-05,7.287646440090612e-05,6.559123721672222e-05,6.0694503190461546e-05,5.7231696700910106e-05,7.645101868547499e-05,7.111232844181359e-05,6.93725814926438e-05,6.034850957803428e-05,5.4702497436665e-05,5.6985234550666064e-05,5.0731669034576043e-05,5.046390288043767e-05,4.076530603924766e-05,
mae,0.12409774959087372,0.03013043850660324,0.015000262297689915,0.013285818509757519,0.01301254890859127,0.012136004865169525,0.015324754640460014,0.0148385688662529,0.016491563990712166,0.012871128506958485,0.011588945053517818,0.012833493761718273,0.01280310470610857,0.015614346601068974,0.014162800274789333,0.012520636431872845,0.009936684742569923,0.0106508182361722,0.012759018689393997,0.012358342297375202,0.01290441770106554,0.012428016401827335,0.01066560298204422,0.009304327890276909,0.008253139443695545,0.009350401349365711,0.010467235930263996,0.011513752862811089,0.010021375492215157,0.009062698110938072,0.010153496637940407,0.010568449273705482,0.008737916126847267,0.008407332003116608,0.008759154938161373,0.00898467842489481,0.009001296944916248,0.010800289921462536,0.009324035607278347,0.008085640147328377,0.007170173339545727,0.007229447364807129,0.007486853748559952,0.010375597514212132,0.010636529885232449,0.0090707428753376,0.008013471029698849,0.007260395213961601,0.007449605036526918,0.008210938423871994,0.008180112577974796,0.007722031325101852,0.007257940247654915,0.0068357838317751884,0.006435490678995848,0.006133126560598612,0.008393709547817707,0.008279185742139816,0.007557256147265434,0.007074420340359211,0.006277176551520824,0.005666148848831654,0.006337339989840984,0.007915631867945194,0.008137363940477371,0.008560284972190857,0.008866522461175919,0.008038828149437904,0.007176986429840326,0.006717023905366659,0.006315356586128473,0.006047965958714485,0.005622580647468567,0.005251196678727865,0.004898690618574619,0.006095860153436661,0.0073500205762684345,0.006667101755738258,0.005978561006486416,0.005220959894359112,0.005900018382817507,0.006699664052575827,0.00751256151124835,0.006851560901850462,0.006464923731982708,0.0065316795371472836,0.007445111405104399,0.006794943939894438,0.006349629256874323,0.00606846809387207,0.005836183205246925,0.006890324875712395,0.0065827383659780025,0.006497074384242296,0.0058674984611570835,0.005699789617210627,0.0057514929212629795,0.0054666344076395035,0.005499070510268211,0.004785455297678709,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 41043     
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 41044     
=================================================================
Total params: 82,087
Trainable params: 82,087
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 16)                80        
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 2048)              34816     
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 6147      
=================================================================
Total params: 41,043
Trainable params: 41,043
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 2048)              8192      
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 16)                32784     
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 68        
=================================================================
Total params: 41,044
Trainable params: 41,044
Non-trainable params: 0
_________________________________________________________________
