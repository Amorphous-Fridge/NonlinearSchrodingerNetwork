2021-06-26
loss,0.643546998500824,0.28417283296585083,0.22870038449764252,0.22379034757614136,0.21722310781478882,0.1975342184305191,0.1291036456823349,0.07682938873767853,0.0593094639480114,0.05150282010436058,0.04641858488321304,0.04265893995761871,0.03948718681931496,0.03657625988125801,0.03399401530623436,0.03167646378278732,0.029389137402176857,0.027677791193127632,0.02638936974108219,0.024852324277162552,0.024054160341620445,0.023160433396697044,0.02237805165350437,0.021733256056904793,0.02100812830030918,0.020741064101457596,0.020172759890556335,0.01979493536055088,0.019325586035847664,0.018955884501338005,0.018581561744213104,0.018387801945209503,0.01812858320772648,0.01787695102393627,0.017701998353004456,0.017458470538258553,0.017170267179608345,0.017032625153660774,0.016714269295334816,0.01673908531665802,0.01656630076467991,0.016360986977815628,0.01619604602456093,0.016064846888184547,0.015847088769078255,0.015751879662275314,0.0156889408826828,0.015577522106468678,0.01547989808022976,0.015328063629567623,0.015174426138401031,0.015092547982931137,0.014943446964025497,0.014966743998229504,0.014935302548110485,0.014709950424730778,0.014632654376327991,0.014552372507750988,0.014539221301674843,0.014398230239748955,0.014246988110244274,0.014241193421185017,0.01410894375294447,0.014097757637500763,0.014077904634177685,0.013887368142604828,0.013921269215643406,0.013834714889526367,0.013631148263812065,0.013721220195293427,0.013610546477138996,0.01351628266274929,0.013499300926923752,0.013428806327283382,0.013292074203491211,0.013257124461233616,0.013253658078610897,0.013223894871771336,0.013108830899000168,0.013092306442558765,0.012969285249710083,0.012973788194358349,0.012826967984437943,0.012839658185839653,0.012864396907389164,0.012705300934612751,0.012672039680182934,0.01268397830426693,0.012564297765493393,0.012525186873972416,0.012439984828233719,0.012425384484231472,0.012346910312771797,0.012360990978777409,0.012257608585059643,0.012215840630233288,0.012177369557321072,0.01220670249313116,0.012043450027704239,0.012141330167651176,
mse,0.15512673556804657,0.025512440130114555,0.018824154511094093,0.018300635740160942,0.01740395836532116,0.014440237544476986,0.006270040757954121,0.002250644378364086,0.0013259309343993664,0.0010170777095481753,0.0008497090893797576,0.0007279824931174517,0.0006311359466053545,0.0005512339994311333,0.00047807226656004786,0.000420529511757195,0.00035975821083411574,0.0003194388700649142,0.00028697721427306533,0.00025675934739410877,0.00023831342696212232,0.00021976932475809008,0.00020484546257648617,0.00019275236991234124,0.00018062068556901067,0.00017204390314873308,0.00016278961265925318,0.00015570930554531515,0.00014926999574527144,0.0001423121284460649,0.0001363102055620402,0.00013256161764729768,0.00012782856356352568,0.00012408947804942727,0.00012146048538852483,0.00011780040222220123,0.00011318889301037416,0.00011141285358462483,0.00010758821008494124,0.00010644653957569972,0.00010454838775331154,0.0001011467757052742,9.881509322440252e-05,9.693743049865589e-05,9.515701094642282e-05,9.301514364778996e-05,9.237681661034003e-05,9.053546818904579e-05,8.932349010137841e-05,8.7400694610551e-05,8.539819828001782e-05,8.456991781713441e-05,8.27025796752423e-05,8.244611672125757e-05,8.224670455092564e-05,8.029463788261637e-05,7.950481813168153e-05,7.805428322171792e-05,7.70995466155e-05,7.581615500384942e-05,7.424293289659545e-05,7.412380364257842e-05,7.307123451028019e-05,7.278306293301284e-05,7.193854980869219e-05,6.995686999289319e-05,7.041852950351313e-05,6.89128864905797e-05,6.733512418577448e-05,6.765521538909525e-05,6.68061911710538e-05,6.612476136069745e-05,6.573905557161197e-05,6.47931155981496e-05,6.361315172398463e-05,6.327518349280581e-05,6.304090493358672e-05,6.22467341599986e-05,6.163062789710239e-05,6.180090713314712e-05,6.039062645868398e-05,6.015436156303622e-05,5.911423431825824e-05,5.9092239098390564e-05,5.87731265113689e-05,5.778664126410149e-05,5.6898847105912864e-05,5.6735752878012136e-05,5.5408061598427594e-05,5.5397638789145276e-05,5.480459731188603e-05,5.479354877024889e-05,5.398489520302974e-05,5.410235462477431e-05,5.334612797014415e-05,5.301757119013928e-05,5.214606426307e-05,5.306053935782984e-05,5.128526026965119e-05,5.203447290114127e-05,
mae,0.2706073522567749,0.12817029654979706,0.10932691395282745,0.10805773735046387,0.10496442019939423,0.09181275218725204,0.052459415048360825,0.03229178488254547,0.025802919641137123,0.022644877433776855,0.020494161173701286,0.018746962770819664,0.017263263463974,0.015938248485326767,0.014735030941665173,0.013695803470909595,0.012672913260757923,0.011907629668712616,0.01133638247847557,0.010676723904907703,0.010323634371161461,0.009945894591510296,0.009605765342712402,0.009332503192126751,0.009034745395183563,0.008909234777092934,0.008669438771903515,0.008502446115016937,0.008306397125124931,0.008141632191836834,0.007985185831785202,0.007893589325249195,0.007778572849929333,0.007671739440411329,0.007590375375002623,0.007492177654057741,0.00735851563513279,0.007308186497539282,0.007160145789384842,0.007176942192018032,0.007098405156284571,0.007007855921983719,0.006936012301594019,0.006883219350129366,0.0067805759608745575,0.006741577293723822,0.0067168730311095715,0.006670068483799696,0.00662707444280386,0.006565531715750694,0.006498247850686312,0.006460757926106453,0.00639770645648241,0.006409996654838324,0.006406611297279596,0.006292207632213831,0.006265459582209587,0.006238219328224659,0.006239904090762138,0.006172203924506903,0.006106938701122999,0.006098057143390179,0.006040368694812059,0.006042044144123793,0.00604440039023757,0.005960575304925442,0.005965697579085827,0.005946337711066008,0.005855130963027477,0.005884758662432432,0.005839635618031025,0.005792890675365925,0.00579486321657896,0.005771792959421873,0.005705608054995537,0.00567654799669981,0.005685390438884497,0.005687350407242775,0.005629344377666712,0.005614422727376223,0.0055675022304058075,0.005570592824369669,0.005500143393874168,0.0055146836675703526,0.00553353363648057,0.005447906907647848,0.005437632091343403,0.005438781343400478,0.005400137975811958,0.005378955975174904,0.005342010874301195,0.005345178302377462,0.005283324047923088,0.005304691381752491,0.005264574661850929,0.005244650412350893,0.005228964611887932,0.00523262657225132,0.005165501963347197,0.005226667504757643,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 723       
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 724       
=================================================================
Total params: 1,447
Trainable params: 1,447
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 16)                80        
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 32)                544       
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 99        
=================================================================
Total params: 723
Trainable params: 723
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 32)                128       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 16)                528       
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 68        
=================================================================
Total params: 724
Trainable params: 724
Non-trainable params: 0
_________________________________________________________________
