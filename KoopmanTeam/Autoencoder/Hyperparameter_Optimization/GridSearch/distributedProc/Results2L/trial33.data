2021-06-26
loss,0.33688995242118835,0.19710752367973328,0.09036240726709366,0.05152713134884834,0.04794313386082649,0.04897259175777435,0.052864931523799896,0.04372435063123703,0.0383559949696064,0.034938227385282516,0.032662827521562576,0.028135215863585472,0.02567942440509796,0.02401389554142952,0.02305247075855732,0.02252037264406681,0.0219408106058836,0.021347077563405037,0.020986849442124367,0.020676888525485992,0.020391197875142097,0.020187193527817726,0.019967984408140182,0.01966029591858387,0.019438069313764572,0.019260641187429428,0.0191920418292284,0.018850861117243767,0.018645621836185455,0.018553663045167923,0.01834569126367569,0.018142903223633766,0.018045807257294655,0.017784111201763153,0.017874691635370255,0.017712054774165154,0.017543431371450424,0.01740105263888836,0.017219161614775658,0.017094137147068977,0.01693340763449669,0.0167569387704134,0.016703087836503983,0.01660420373082161,0.016627686098217964,0.01647435873746872,0.016330914571881294,0.01623031683266163,0.01606244407594204,0.0160897895693779,0.015828654170036316,0.01580093614757061,0.015705060213804245,0.01563951186835766,0.01562388427555561,0.015587683767080307,0.01536138728260994,0.015331650152802467,0.015197794884443283,0.015216109342873096,0.015128156170248985,0.014905576594173908,0.015001451596617699,0.014941748231649399,0.014770429581403732,0.014751509763300419,0.014669163152575493,0.01461357343941927,0.014569220133125782,0.014438645914196968,0.014369369484484196,0.014343615621328354,0.014305751770734787,0.014249204657971859,0.014176437631249428,0.014008787460625172,0.014172954484820366,0.014030351303517818,0.013985243625938892,0.013953176327049732,0.013852639123797417,0.01373337022960186,0.013745984062552452,0.015072996728122234,0.023930318653583527,0.021945487707853317,0.020571554079651833,0.019477514550089836,0.01880846917629242,0.018187738955020905,0.017677458003163338,0.017764413729310036,0.0231096800416708,0.024243228137493134,0.020872993394732475,0.024075515568256378,0.02363639883697033,0.02114209719002247,0.019648298621177673,0.01898665353655815,
mse,0.04053811728954315,0.014052759855985641,0.0028597062919288874,0.0008980704587884247,0.0007495561148971319,0.000732239568606019,0.0008122360450215638,0.0005535208620131016,0.0004274543025530875,0.00035712734097614884,0.00031941942870616913,0.00024061497242655605,0.00019967154366895556,0.000176128392922692,0.00016204094572458416,0.00015299765800591558,0.00014551234198734164,0.00013786638737656176,0.00013228695024736226,0.00012756358773913234,0.00012314345804043114,0.00012187010725028813,0.0001187757370644249,0.00011448332952568308,0.00011112431093351915,0.00010982800449710339,0.0001081888476619497,0.00010420628677820787,0.00010167158325202763,0.00010053568257717416,9.80639670160599e-05,9.617931937100366e-05,9.5829222118482e-05,9.219064668286592e-05,9.290897287428379e-05,9.126981603913009e-05,8.949099719757214e-05,8.716011507203802e-05,8.556063403375447e-05,8.406936831306666e-05,8.2443977589719e-05,8.157947013387457e-05,8.066096052061766e-05,7.998703949851915e-05,7.96990207163617e-05,7.77613022364676e-05,7.734250539215282e-05,7.573054608656093e-05,7.41358453524299e-05,7.49032769817859e-05,7.208342867670581e-05,7.084881508490071e-05,7.051579450489953e-05,7.061832002364099e-05,7.017092866590247e-05,6.91958048264496e-05,6.729293090756983e-05,6.778919487260282e-05,6.671578739769757e-05,6.647426926065236e-05,6.552020931849256e-05,6.374920485541224e-05,6.382645369740203e-05,6.325520371319726e-05,6.206158286659047e-05,6.189569103298709e-05,6.043645043973811e-05,6.0495523939607665e-05,6.0114412917755544e-05,5.916594454902224e-05,5.8297853684052825e-05,5.851127207279205e-05,5.847621650900692e-05,5.773700831923634e-05,5.711224730475806e-05,5.578961645369418e-05,5.727551251766272e-05,5.5621399951633066e-05,5.511599010787904e-05,5.5703370890114456e-05,5.4124298912938684e-05,5.4355012252926826e-05,5.348747072275728e-05,7.008716784184799e-05,0.00016535173926968127,0.00013342045713216066,0.00011723530769813806,0.00010623846901580691,9.861973376246169e-05,9.23329935176298e-05,8.766227256273851e-05,9.094145934795961e-05,0.00016406724171247333,0.00017274022684432566,0.00012893466919194907,0.00016579536895733327,0.00015889799396973103,0.00013359870354179293,0.00011706572695402429,0.00011050727334804833,
mae,0.14537635445594788,0.08237253129482269,0.03677501529455185,0.021664749830961227,0.0204115342348814,0.020623965188860893,0.02233785204589367,0.019402455538511276,0.017019473016262054,0.015146022662520409,0.013646537438035011,0.011908894404768944,0.01072872057557106,0.010067629627883434,0.00977261457592249,0.009577964432537556,0.009276062250137329,0.008941142819821835,0.008837930858135223,0.008613093756139278,0.008555611595511436,0.008441324345767498,0.008427540771663189,0.008416963741183281,0.00831359252333641,0.008170616813004017,0.008134083822369576,0.007989616133272648,0.007833660580217838,0.007845803163945675,0.007771053351461887,0.007728182710707188,0.007766708265990019,0.007596573792397976,0.0075475662015378475,0.007445282302796841,0.007442676927894354,0.007393929176032543,0.0073576681315898895,0.007179253734648228,0.00719435466453433,0.007184768095612526,0.0071712518110871315,0.007090216968208551,0.006953753065317869,0.006991520058363676,0.007031962275505066,0.006826601456850767,0.006782574579119682,0.006862651091068983,0.0067159016616642475,0.006688594818115234,0.006616263184696436,0.006624446716159582,0.006702154874801636,0.006489613093435764,0.006468136794865131,0.0066063678823411465,0.006518264301121235,0.006483146455138922,0.006473541259765625,0.0063095372170209885,0.0064289141446352005,0.006383171770721674,0.006232553161680698,0.006268678233027458,0.0062093292362987995,0.006057153921574354,0.006175035610795021,0.0060042631812393665,0.006110250949859619,0.006120617501437664,0.005985596217215061,0.006149072665721178,0.005894896574318409,0.005920812953263521,0.005962875671684742,0.005897396709769964,0.005968016572296619,0.005870324559509754,0.006048872601240873,0.00590188754722476,0.005853674374520779,0.006345855072140694,0.009603247977793217,0.008588817901909351,0.00808520708233118,0.007790097035467625,0.007488992065191269,0.007223831955343485,0.007121887058019638,0.007597068324685097,0.010017702355980873,0.010775837115943432,0.008879339322447777,0.010347716510295868,0.010164010338485241,0.00926442351192236,0.009087144397199154,0.008585786446928978,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 9603      
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 9604      
=================================================================
Total params: 19,207
Trainable params: 19,207
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 256)               1280      
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 32)                8224      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 99        
=================================================================
Total params: 9,603
Trainable params: 9,603
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 32)                128       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 256)               8448      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 1028      
=================================================================
Total params: 9,604
Trainable params: 9,604
Non-trainable params: 0
_________________________________________________________________
