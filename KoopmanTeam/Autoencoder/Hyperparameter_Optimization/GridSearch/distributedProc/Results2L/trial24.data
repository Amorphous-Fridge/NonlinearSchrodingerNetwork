2021-06-26
loss,0.34115639328956604,0.21462006866931915,0.1516672670841217,0.05887275189161301,0.04078087955713272,0.034585222601890564,0.030504487454891205,0.027690794318914413,0.02558310702443123,0.02324535883963108,0.021676907315850258,0.02032608911395073,0.02029443345963955,0.019193308427929878,0.01847277581691742,0.017870917916297913,0.01722167804837227,0.016932720318436623,0.016597894951701164,0.016292262822389603,0.01600937359035015,0.015577570535242558,0.01535266637802124,0.015263203531503677,0.014967081137001514,0.014806644059717655,0.014631233178079128,0.014346044510602951,0.014437219128012657,0.014194161631166935,0.014032912440598011,0.013806560076773167,0.013861383311450481,0.013686767779290676,0.013458487577736378,0.013447045348584652,0.013161003589630127,0.013367991894483566,0.013174135237932205,0.013034787960350513,0.01301414892077446,0.012845064513385296,0.012776989489793777,0.012729802168905735,0.012664523907005787,0.012410002760589123,0.01259456668049097,0.012426034547388554,0.01240671519190073,0.01231725700199604,0.012195584364235401,0.012230337597429752,0.012054667808115482,0.012175338342785835,0.012054141610860825,0.011981048621237278,0.011916871182620525,0.011871596798300743,0.011836843565106392,0.011738309636712074,0.011727317236363888,0.011638044379651546,0.01161990500986576,0.01147638913244009,0.011585482396185398,0.011487866751849651,0.011505321599543095,0.011414523236453533,0.011315380223095417,0.011363339610397816,0.011312220245599747,0.011319390498101711,0.011230643838644028,0.011189327575266361,0.011182490736246109,0.011047085747122765,0.011208695359528065,0.011062649078667164,0.010981527157127857,0.011039722710847855,0.010992206633090973,0.010966219939291477,0.010885100811719894,0.010947390459477901,0.01087428443133831,0.010857808403670788,0.010809511877596378,0.010800356976687908,0.01074614841490984,0.010786794126033783,0.010704648680984974,0.010730961337685585,0.010669337585568428,0.010609736666083336,0.010652129538357258,0.010582949034869671,0.010642561130225658,0.010519087314605713,0.010573381558060646,0.010439890436828136,
mse,0.0430520623922348,0.01688520982861519,0.00893264077603817,0.001312306965701282,0.000631959002930671,0.00045427968143485487,0.0003458819119259715,0.00027343002147972584,0.0002242716000182554,0.00018394917424302548,0.00015868674381636083,0.00014097438543103635,0.0001357796718366444,0.00011960941628785804,0.00011012223694706336,0.00010352645767852664,9.593740105628967e-05,9.157652675639838e-05,8.833598258206621e-05,8.442865510005504e-05,8.130990318022668e-05,7.73912834119983e-05,7.468504190910608e-05,7.399530295515433e-05,7.08979569026269e-05,6.906927592353895e-05,6.737057265127078e-05,6.459256110247225e-05,6.539175228681415e-05,6.321586988633499e-05,6.14668897469528e-05,5.9444104408612475e-05,6.001878864481114e-05,5.822968159918673e-05,5.645101919071749e-05,5.6412234698655084e-05,5.419142689788714e-05,5.549267734750174e-05,5.402440365287475e-05,5.255857468000613e-05,5.227348447078839e-05,5.072098065284081e-05,5.036219590692781e-05,4.980985977454111e-05,4.949813592247665e-05,4.713315865956247e-05,4.84454030811321e-05,4.722376252175309e-05,4.698282646131702e-05,4.6384648157982156e-05,4.543014438240789e-05,4.5561893784906715e-05,4.4288564822636545e-05,4.5183947804616764e-05,4.412925409269519e-05,4.353264012024738e-05,4.2872841731878e-05,4.2446550651220605e-05,4.220707342028618e-05,4.1582716221455485e-05,4.157146031502634e-05,4.0891380194807425e-05,4.06972503697034e-05,3.950201062252745e-05,4.0296414226759225e-05,3.947424193029292e-05,3.977055166615173e-05,3.8813821447547525e-05,3.825694147963077e-05,3.8567217416130006e-05,3.831410867860541e-05,3.837771146208979e-05,3.7505305954255164e-05,3.7173678720137104e-05,3.694458428071812e-05,3.632184234447777e-05,3.7589052226394415e-05,3.607774124247953e-05,3.5932534956373274e-05,3.678649591165595e-05,3.585022204788402e-05,3.553686474333517e-05,3.514391937642358e-05,3.54714720742777e-05,3.5057964851148427e-05,3.488691436359659e-05,3.441756416577846e-05,3.462800304987468e-05,3.396482497919351e-05,3.437929262872785e-05,3.361164635862224e-05,3.4034754207823426e-05,3.353457941557281e-05,3.322229895275086e-05,3.332454798510298e-05,3.2754818676039577e-05,3.3277898182859644e-05,3.238959834561683e-05,3.27601155731827e-05,3.2466250559082255e-05,
mae,0.15042735636234283,0.0978032797574997,0.06421014666557312,0.0250634104013443,0.017733164131641388,0.01497135404497385,0.013143467716872692,0.01188065018504858,0.010944067500531673,0.009929833933711052,0.009221065789461136,0.00861259363591671,0.008630359545350075,0.008162431418895721,0.007840943522751331,0.007609592285007238,0.007282090373337269,0.007183519657701254,0.007030059117823839,0.006908723618835211,0.006821027025580406,0.006608725991100073,0.006579031236469746,0.006476037669926882,0.006355472840368748,0.006292138248682022,0.0062189954333007336,0.006076121237128973,0.006133225746452808,0.006092212628573179,0.005944957956671715,0.005883731879293919,0.0058959596790373325,0.005912786349654198,0.005740263499319553,0.0057263500057160854,0.005618768744170666,0.0057369414716959,0.005673771724104881,0.0055272458121180534,0.005518993362784386,0.005472568329423666,0.0054954844526946545,0.0053830756805837154,0.0053991093300282955,0.00530620152130723,0.005341098178178072,0.005297507159411907,0.005272216163575649,0.005298910662531853,0.005249437876045704,0.005241425707936287,0.005138600245118141,0.005193782038986683,0.005127108655869961,0.005123204551637173,0.005098043009638786,0.005084153264760971,0.005015973933041096,0.00496661476790905,0.004956909455358982,0.004934138618409634,0.00497264601290226,0.004881886765360832,0.004936962388455868,0.004887383431196213,0.004953715018928051,0.004871397744864225,0.004866180010139942,0.004838738590478897,0.004829608369618654,0.004771556239575148,0.004916531033813953,0.004789522849023342,0.004764218349009752,0.004768791608512402,0.004790072329342365,0.004700127523392439,0.004657545126974583,0.004757510032504797,0.004694165196269751,0.004654651507735252,0.004590304102748632,0.004676986485719681,0.004628571681678295,0.00461813248693943,0.004632484167814255,0.004659117665141821,0.00457218149676919,0.00469865370541811,0.004582730121910572,0.004628827795386314,0.00455715786665678,0.004462493117898703,0.004589807707816362,0.004467679653316736,0.004618074279278517,0.004527270793914795,0.004458817653357983,0.0044421241618692875,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 2755      
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 2756      
=================================================================
Total params: 5,511
Trainable params: 5,511
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 128)               640       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 16)                2064      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 51        
=================================================================
Total params: 2,755
Trainable params: 2,755
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 16)                64        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 128)               2176      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 516       
=================================================================
Total params: 2,756
Trainable params: 2,756
Non-trainable params: 0
_________________________________________________________________
