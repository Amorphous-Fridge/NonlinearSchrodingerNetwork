2021-06-26
loss,0.48516562581062317,0.23772060871124268,0.21979787945747375,0.18353158235549927,0.10085633397102356,0.059730660170316696,0.04778696596622467,0.041416775435209274,0.036864880472421646,0.033489614725112915,0.029813775792717934,0.02710559219121933,0.025251278653740883,0.023485133424401283,0.022256942465901375,0.021232295781373978,0.02025771699845791,0.019591469317674637,0.01877816952764988,0.01850833371281624,0.01783834584057331,0.017433350905776024,0.01686817966401577,0.016441715881228447,0.016279472038149834,0.01583789847791195,0.015539640560746193,0.015221894718706608,0.014982379972934723,0.014906479977071285,0.014620581641793251,0.014485826715826988,0.014367200434207916,0.014173094183206558,0.01405778806656599,0.013837596401572227,0.013612981885671616,0.013496595434844494,0.013322127051651478,0.013241010718047619,0.012902434915304184,0.01299352664500475,0.012637975625693798,0.012785494327545166,0.012642746791243553,0.012431063689291477,0.01225477084517479,0.01219717413187027,0.012017173692584038,0.011893082410097122,0.011776710860431194,0.011682156473398209,0.011585068888962269,0.0115522351115942,0.011387281119823456,0.011289217509329319,0.011127538979053497,0.011090641841292381,0.010965580120682716,0.010918294079601765,0.010882663540542126,0.010788675397634506,0.010591680184006691,0.010627754032611847,0.010511276312172413,0.010522727854549885,0.010398969985544682,0.01037813350558281,0.010340406559407711,0.010213195346295834,0.010039896704256535,0.010166695341467857,0.010025528259575367,0.009946238249540329,0.009934738278388977,0.009896860457956791,0.00981072150170803,0.00980866514146328,0.009767409414052963,0.009674149565398693,0.009593211114406586,0.009543141350150108,0.009574538096785545,0.009590554051101208,0.009503950364887714,0.009418264031410217,0.00943027064204216,0.00935561303049326,0.009325799532234669,0.00921011995524168,0.009327858686447144,0.009309230372309685,0.009200921282172203,0.009178523905575275,0.009130087681114674,0.00910177268087864,0.009088677354156971,0.009061857126653194,0.008997832424938679,0.00900740921497345,
mse,0.08699032664299011,0.01948842592537403,0.01704493910074234,0.011857058852910995,0.0038689884822815657,0.0013710815692320466,0.0008776469621807337,0.0006698486395180225,0.0005303053185343742,0.000428244995418936,0.0003347607562318444,0.0002771044964902103,0.0002378649660386145,0.00020659263827838004,0.00018496139091439545,0.00016669943579472601,0.00015154494030866772,0.0001409942051395774,0.00013018380559515208,0.00012355996295809746,0.00011567355250008404,0.00010895323066506535,0.0001024773227982223,9.731811587698758e-05,9.343557758256793e-05,8.933395292842761e-05,8.581942529417574e-05,8.281883492600173e-05,7.915619062259793e-05,7.776259008096531e-05,7.512582669733092e-05,7.377391739282757e-05,7.178488158388063e-05,6.92082685418427e-05,6.829451740486547e-05,6.609361298615113e-05,6.388856854755431e-05,6.256016058614478e-05,6.072243195376359e-05,6.03093794779852e-05,5.694383798982017e-05,5.7695244322530925e-05,5.4776653996668756e-05,5.51498269487638e-05,5.397494533099234e-05,5.200123996473849e-05,5.0886639655800536e-05,5.0154350901721045e-05,4.890887066721916e-05,4.783305848832242e-05,4.6850134822307155e-05,4.604098285199143e-05,4.530293517746031e-05,4.472631189855747e-05,4.364917549537495e-05,4.317113780416548e-05,4.1721377783687785e-05,4.147799700149335e-05,4.0557515603723004e-05,4.002300192951225e-05,3.9347341953543946e-05,3.908822691300884e-05,3.784640284720808e-05,3.788743561017327e-05,3.7084289942868054e-05,3.717030631378293e-05,3.6097513657296076e-05,3.604031735449098e-05,3.569459659047425e-05,3.471045783953741e-05,3.367220415384509e-05,3.417491097934544e-05,3.3612664992688224e-05,3.285597631474957e-05,3.2737283618189394e-05,3.254198600188829e-05,3.18965976475738e-05,3.174359517288394e-05,3.152598947053775e-05,3.095585998380557e-05,3.030980224139057e-05,3.0154380510794e-05,3.0033266739337705e-05,3.0212413548724726e-05,2.9757986339973286e-05,2.9108903618180193e-05,2.920818587881513e-05,2.875983773265034e-05,2.84129855572246e-05,2.766603574855253e-05,2.82269738818286e-05,2.8339511118247174e-05,2.7538610083865933e-05,2.747936378000304e-05,2.716789640544448e-05,2.6898378564510494e-05,2.6727131626103073e-05,2.6600757337291725e-05,2.6280051315552555e-05,2.6168127078562975e-05,
mae,0.2073785960674286,0.09580551832914352,0.08559076488018036,0.06738030910491943,0.038822613656520844,0.024248875677585602,0.020034171640872955,0.017531925812363625,0.0156911201775074,0.01427854411303997,0.012772879563272,0.011615022085607052,0.010804002173244953,0.010068473406136036,0.009535581804811954,0.009083868004381657,0.008672717958688736,0.008384971879422665,0.00803070142865181,0.007910876534879208,0.007623589597642422,0.007449874188750982,0.007209243718534708,0.007022493984550238,0.006948948372155428,0.006768999621272087,0.006634527817368507,0.006502860225737095,0.006390055641531944,0.006356169003993273,0.006245117634534836,0.006184656638652086,0.006136441137641668,0.006053858436644077,0.005997973028570414,0.005912271793931723,0.0058191074058413506,0.005762425716966391,0.005687844008207321,0.005661951843649149,0.005502213258296251,0.005545354913920164,0.005386175587773323,0.0054563675075769424,0.005396813154220581,0.005300038959830999,0.0052206008695065975,0.005196843761950731,0.005126929841935635,0.005070988088846207,0.005029205698519945,0.004985099192708731,0.0049479384906589985,0.004928431939333677,0.0048624672926962376,0.004822426941245794,0.0047508240677416325,0.004724329803138971,0.00467182882130146,0.0046539707109332085,0.004640675615519285,0.004603235051035881,0.0045190006494522095,0.004538807086646557,0.004482447635382414,0.004488270729780197,0.004433203488588333,0.004433486610651016,0.004418719559907913,0.004353352356702089,0.004285769537091255,0.004329452756792307,0.004262038040906191,0.004241930320858955,0.004236708395183086,0.004219114314764738,0.004186401609331369,0.0041851685382425785,0.004156370181590319,0.004138096235692501,0.0040963212959468365,0.004076766781508923,0.004068708047270775,0.0040886057540774345,0.004067369736731052,0.004031649325042963,0.004024852067232132,0.003975867759436369,0.003970057237893343,0.003925234545022249,0.003979354631155729,0.003980102017521858,0.003931404557079077,0.003919681534171104,0.003889227518811822,0.0038861343637108803,0.003876875853165984,0.0038597648963332176,0.0038380513433367014,0.0038530644960701466,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 739       
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 740       
=================================================================
Total params: 1,479
Trainable params: 1,479
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 32)                160       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 16)                528       
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 51        
=================================================================
Total params: 739
Trainable params: 739
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 16)                64        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 32)                544       
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 132       
=================================================================
Total params: 740
Trainable params: 740
Non-trainable params: 0
_________________________________________________________________
