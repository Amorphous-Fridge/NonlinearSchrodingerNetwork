2021-06-26
loss,0.29114478826522827,0.18732894957065582,0.0824040025472641,0.054709114134311676,0.044544268399477005,0.03820635750889778,0.03380736708641052,0.0315554104745388,0.03622836992144585,0.04354260116815567,0.04071067273616791,0.03537800908088684,0.03200043365359306,0.030120566487312317,0.029475560411810875,0.027310919016599655,0.03222893923521042,0.03825260326266289,0.03576076030731201,0.031521011143922806,0.029584519565105438,0.02600640244781971,0.02776201069355011,0.028871016576886177,0.026372278109192848,0.02463122457265854,0.03000900335609913,0.029437635093927383,0.025797689333558083,0.024003345519304276,0.026242585852742195,0.026048319414258003,0.02554270252585411,0.024520868435502052,0.023216957226395607,0.021969886496663094,0.020884940400719643,0.020033011212944984,0.019279509782791138,0.023469410836696625,0.026811396703124046,0.023542914539575577,0.021556785330176353,0.020148970186710358,0.018817484378814697,0.014997073449194431,0.013869080692529678,0.013483742251992226,0.013151955790817738,0.0126948868855834,0.012652630917727947,0.012448388151824474,0.012402531690895557,0.012248757295310497,0.012203162536025047,0.01215881947427988,0.012132040224969387,0.012094177305698395,0.01204268541187048,0.011977073736488819,0.011934299021959305,0.011841246858239174,0.011695517227053642,0.011668730527162552,0.011751885525882244,0.011554628610610962,0.011541508138179779,0.011426741257309914,0.011382212862372398,0.01128531713038683,0.011318972334265709,0.011289596557617188,0.011207248084247112,0.011166296899318695,0.011066604405641556,0.0110469413921237,0.010927567258477211,0.010948418639600277,0.010915203019976616,0.010775872506201267,0.010859458707273006,0.010835876688361168,0.010800893418490887,0.010650832206010818,0.010623609647154808,0.010657966136932373,0.0105494549497962,0.010497914627194405,0.010441495105624199,0.010404585860669613,0.010366537608206272,0.010280578397214413,0.010345944203436375,0.012294689193367958,0.016559291630983353,0.017711196094751358,0.018629012629389763,0.018693722784519196,0.01716599054634571,0.01796894706785679,
mse,0.02976941503584385,0.013022896833717823,0.0022579922806471586,0.0009757676161825657,0.000661339785438031,0.00047302854363806546,0.0003515210410114378,0.0002977697877213359,0.00040606537368148565,0.0005414256011135876,0.0004671995993703604,0.0003485445922706276,0.0002886450965888798,0.0002570984361227602,0.0002529329212848097,0.0002233935083495453,0.0003056098648812622,0.00040450296364724636,0.00036365140113048255,0.00028712282073684037,0.00025141044170595706,0.00018986035138368607,0.00022028687817510217,0.0002300689957337454,0.0001936992775881663,0.00016986619448289275,0.00026590388733893633,0.00025305329472757876,0.00019023533968720585,0.00016364669136237353,0.0001962617097888142,0.00019433593843132257,0.0001856920716818422,0.0001667977630859241,0.00014950173499528319,0.00013578482321463525,0.00012266704288776964,0.00011294247815385461,0.00010676040255930275,0.00016587651043664664,0.0002038581296801567,0.00015218352200463414,0.0001274362439289689,0.00011336836905684322,0.00010229003964923322,6.586657400475815e-05,5.457814404508099e-05,5.315603266353719e-05,4.9441699957242236e-05,4.5826771383872256e-05,4.5148310164222494e-05,4.377441655378789e-05,4.3547541281441227e-05,4.246520984452218e-05,4.31840744568035e-05,4.145174170844257e-05,4.168885789113119e-05,4.119555524084717e-05,4.093891766387969e-05,3.999304317403585e-05,3.964698044001125e-05,3.9220150938490406e-05,3.874721369356848e-05,3.805219603236765e-05,3.812436625594273e-05,3.698743967106566e-05,3.686858690343797e-05,3.62898463208694e-05,3.594826193875633e-05,3.556359661160968e-05,3.625853787525557e-05,3.5321168979862705e-05,3.481159365037456e-05,3.4325752494623885e-05,3.410615317989141e-05,3.382925933692604e-05,3.314628702355549e-05,3.297078001196496e-05,3.300432945252396e-05,3.240224759792909e-05,3.298865703982301e-05,3.286215360276401e-05,3.218591518816538e-05,3.130242839688435e-05,3.176179961883463e-05,3.12771626340691e-05,3.0512321245623752e-05,3.068482328671962e-05,3.020517760887742e-05,2.9802797143929638e-05,2.980474164360203e-05,2.9603976145153865e-05,2.9641347282449715e-05,4.754586916533299e-05,8.119124686345458e-05,8.706898370292038e-05,9.697527275420725e-05,9.469766519032419e-05,8.298013563035056e-05,8.908358722692356e-05,
mae,0.1285213977098465,0.08228687196969986,0.03420179709792137,0.023498723283410072,0.019145747646689415,0.016235850751399994,0.014336872845888138,0.013285563327372074,0.015414157882332802,0.01858414337038994,0.016956450417637825,0.01465397235006094,0.013342437334358692,0.012621003203094006,0.012498846277594566,0.011577452532947063,0.013774576596915722,0.01629314012825489,0.015148084610700607,0.013585631735622883,0.01248991210013628,0.010830916464328766,0.01181284710764885,0.01277531124651432,0.011553231626749039,0.010597600601613522,0.012187403626739979,0.01223297044634819,0.010691516101360321,0.009641740471124649,0.011234416626393795,0.011069618165493011,0.011011581867933273,0.01063225045800209,0.01006687805056572,0.009368421509861946,0.00903729535639286,0.008652081713080406,0.00799204595386982,0.009628059342503548,0.011312664486467838,0.009713933803141117,0.00867584440857172,0.00818659644573927,0.00790423434227705,0.006321716122329235,0.005817560479044914,0.005703596398234367,0.005540015175938606,0.005365589167922735,0.005340137053281069,0.005262843798846006,0.005281838588416576,0.005174187943339348,0.005203309003263712,0.005175027064979076,0.005173088982701302,0.005183062981814146,0.0051056803204119205,0.005111627746373415,0.005083364434540272,0.0050222789868712425,0.004937540274113417,0.0049636028707027435,0.005027787759900093,0.004860518034547567,0.004899877123534679,0.0048524453304708,0.004892139229923487,0.004795849788933992,0.00483732670545578,0.004859279841184616,0.004742816090583801,0.0047303191386163235,0.004724636673927307,0.004716955590993166,0.004650191869586706,0.004696964286267757,0.004635937977582216,0.0046059382148087025,0.0046067689545452595,0.004589034244418144,0.004612787161022425,0.004459594376385212,0.004515521228313446,0.004549650475382805,0.004539921879768372,0.00449092872440815,0.004463386721909046,0.0043990882113575935,0.0043725366704165936,0.004318029619753361,0.004448347724974155,0.005227683112025261,0.006958988960832357,0.0073444657027721405,0.00790398195385933,0.008038208819925785,0.0070667145773768425,0.0076157147996127605,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 17539     
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 17540     
=================================================================
Total params: 35,079
Trainable params: 35,079
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 128)               640       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 128)               16512     
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 387       
=================================================================
Total params: 17,539
Trainable params: 17,539
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 128)               512       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 128)               16512     
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 516       
=================================================================
Total params: 17,540
Trainable params: 17,540
Non-trainable params: 0
_________________________________________________________________
