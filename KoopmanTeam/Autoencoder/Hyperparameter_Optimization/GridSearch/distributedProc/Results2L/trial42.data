2021-06-26
loss,0.3532015085220337,0.17827191948890686,0.08706028759479523,0.07058008760213852,0.06513741612434387,0.053050071001052856,0.05985487625002861,0.04746076464653015,0.04534051567316055,0.04152427613735199,0.044668298214673996,0.037555351853370667,0.03277823328971863,0.03681106120347977,0.03818412125110626,0.03633914142847061,0.039440788328647614,0.03379713371396065,0.03000538796186447,0.02769036591053009,0.02590182051062584,0.02487241104245186,0.026789331808686256,0.0326196625828743,0.03481453284621239,0.02886263094842434,0.025476692244410515,0.022912390530109406,0.020929211750626564,0.027564577758312225,0.026127176359295845,0.023943375796079636,0.022719230502843857,0.021517427638173103,0.019931510090827942,0.018236849457025528,0.018679514527320862,0.020435241982340813,0.024050896987318993,0.024429267272353172,0.02468886785209179,0.02221321128308773,0.024136271327733994,0.02150658331811428,0.01978461630642414,0.018069451674818993,0.015836799517273903,0.01503763161599636,0.01967567205429077,0.02098046988248825,0.02039889246225357,0.020364023745059967,0.01814858987927437,0.016790002584457397,0.01687457039952278,0.018287045881152153,0.018187273293733597,0.016974348574876785,0.01615508645772934,0.020837517455220222,0.022538207471370697,0.01955384574830532,0.016982458531856537,0.015478107146918774,0.014452231116592884,0.013847305439412594,0.013499878346920013,0.017101719975471497,0.018313506618142128,0.019429419189691544,0.018954886123538017,0.0172868799418211,0.01616811193525791,0.01572885923087597,0.014608656987547874,0.014726771041750908,0.014375627972185612,0.01717643439769745,0.016548562794923782,0.018523775041103363,0.01931464858353138,0.016740452498197556,0.01731361262500286,0.016375964507460594,0.014461716637015343,0.011840473860502243,0.013770285062491894,0.014084734953939915,0.015937305986881256,0.017041757702827454,0.016374459490180016,0.01642966829240322,0.01461457647383213,0.014475596137344837,0.015282934531569481,0.01575217768549919,0.015137718059122562,0.013547300361096859,0.013586264103651047,0.01580142229795456,
mse,0.04354775324463844,0.011499651707708836,0.0024130630772560835,0.0015447532059624791,0.0012985907960683107,0.0008618033607490361,0.0010750273941084743,0.00069114362122491,0.0006250856094993651,0.0005165767506696284,0.0005773528828285635,0.0004071149160154164,0.0003181247739121318,0.00039892763015814126,0.00041845568921417,0.0003764483844861388,0.0004325650224927813,0.00032342635677196085,0.00026133499341085553,0.00022448890376836061,0.00019694048387464136,0.00017794163431972265,0.00021109747467562556,0.0002987353946082294,0.0003320510732010007,0.00023519188107457012,0.00018606848607305437,0.00014972503413446248,0.0001322250027442351,0.0002161892334697768,0.00018825808365363628,0.0001607426966074854,0.00014454187476076186,0.00013068178668618202,0.00011548505426617339,9.936792775988579e-05,0.00010353625111747533,0.00012413752847351134,0.00016195086936932057,0.00016822294855955988,0.00017173301603179425,0.00014163818559609354,0.000163451477419585,0.00012866102042607963,0.00011109901242889464,9.797835809877142e-05,7.812367402948439e-05,6.825700984336436e-05,0.00011426735727582127,0.00012302050890866667,0.00011719551548594609,0.00011320917110424489,9.501256863586605e-05,8.448243170278147e-05,8.596030966145918e-05,9.26663851714693e-05,9.188078547595069e-05,8.408877329202369e-05,7.832954725017771e-05,0.00012583399075083435,0.00014049670426174998,0.00011178671411471441,8.528883336111903e-05,7.194698991952464e-05,6.31786824669689e-05,5.789379429188557e-05,5.4792279115645215e-05,8.602963498560712e-05,9.407803736394271e-05,0.0001044724413077347,9.998666064348072e-05,8.53292949614115e-05,7.543723768321797e-05,7.253573858179152e-05,6.551999831572175e-05,6.541094626300037e-05,6.22055449639447e-05,8.22291913209483e-05,7.884201477281749e-05,9.609819971956313e-05,0.00010433913848828524,7.973109313752502e-05,8.414008334511891e-05,7.449131953762844e-05,6.199665949679911e-05,4.4824424549005926e-05,5.7772598665906116e-05,5.9496305766515434e-05,7.37179143470712e-05,8.151597285177559e-05,7.670465129194781e-05,7.51572079025209e-05,6.327118899207562e-05,6.289614975685254e-05,6.738337833667174e-05,7.03937912476249e-05,6.494678382296115e-05,5.45901530131232e-05,5.523324944078922e-05,6.929571100044996e-05,
mae,0.14976966381072998,0.07519972324371338,0.0370924212038517,0.030318882316350937,0.027929188683629036,0.022765396162867546,0.025720203295350075,0.02036544866859913,0.019411714747548103,0.01769678294658661,0.019034940749406815,0.016110189259052277,0.013864031992852688,0.015544792637228966,0.016121337190270424,0.01549449097365141,0.016822736710309982,0.014520976692438126,0.012630126439034939,0.01161312498152256,0.010946312919259071,0.01059988047927618,0.011326087638735771,0.013867905363440514,0.014595193788409233,0.012289676815271378,0.0108171496540308,0.010072089731693268,0.009076339192688465,0.011752601712942123,0.011177667416632175,0.01032275054603815,0.009738050401210785,0.009203951805830002,0.008419829420745373,0.007726564537733793,0.007949781604111195,0.008645586669445038,0.010175265371799469,0.010385403409600258,0.010348383337259293,0.009509882889688015,0.010138257406651974,0.009176171384751797,0.008607137948274612,0.007734120357781649,0.00680533004924655,0.006458226591348648,0.008378761820495129,0.008880301378667355,0.008610755205154419,0.008498378098011017,0.00756658473983407,0.007036622613668442,0.007136283442378044,0.007880892604589462,0.007843922823667526,0.007180279120802879,0.006753341760486364,0.008953088894486427,0.00965932384133339,0.008447346277534962,0.00707336375489831,0.006491418927907944,0.006052755285054445,0.00582199078053236,0.005725824739784002,0.007189399562776089,0.007707461714744568,0.008250437676906586,0.008077796548604965,0.007344523444771767,0.006853177677839994,0.006646700669080019,0.006191117223352194,0.006248793564736843,0.006167552433907986,0.007198800332844257,0.006990906782448292,0.007808161433786154,0.008221984840929508,0.0071309101767838,0.007336663082242012,0.006961884442716837,0.006032863166183233,0.0050362516194581985,0.005791599862277508,0.006044159643352032,0.00678677624091506,0.007090994156897068,0.006835610140115023,0.0068628014996647835,0.006112250499427319,0.006177743896842003,0.006410704925656319,0.006719597615301609,0.006387561559677124,0.005716236773878336,0.0057475077919662,0.006672537419945002,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 35587     
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 35588     
=================================================================
Total params: 71,175
Trainable params: 71,175
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 512)               2560      
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 64)                32832     
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 195       
=================================================================
Total params: 35,587
Trainable params: 35,587
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 64)                256       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 512)               33280     
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 2052      
=================================================================
Total params: 35,588
Trainable params: 35,588
Non-trainable params: 0
_________________________________________________________________
