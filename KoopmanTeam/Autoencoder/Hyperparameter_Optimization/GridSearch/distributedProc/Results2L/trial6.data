2021-06-26
loss,0.25601065158843994,0.06604237854480743,0.042965322732925415,0.03586262837052345,0.0305610541254282,0.026870181784033775,0.024710487574338913,0.023575134575366974,0.02253688871860504,0.021643683314323425,0.020933926105499268,0.020377369597554207,0.01989942044019699,0.01933755725622177,0.018942758440971375,0.019089411944150925,0.028127577155828476,0.029020465910434723,0.02577984519302845,0.023868894204497337,0.021804895251989365,0.019794320687651634,0.01857190579175949,0.019039759412407875,0.02850504405796528,0.025147557258605957,0.028438949957489967,0.02400043234229088,0.024734295904636383,0.02207203209400177,0.02346416562795639,0.023690499365329742,0.01995076984167099,0.022332310676574707,0.02297762595117092,0.021357638761401176,0.01973813958466053,0.019213473424315453,0.018658582121133804,0.02074110321700573,0.022398315370082855,0.0247111264616251,0.022025804966688156,0.020166905596852303,0.01911846175789833,0.017545072361826897,0.021022308617830276,0.018901402130723,0.017620783299207687,0.01687518134713173,0.01625082641839981,0.015387891791760921,0.01527499407529831,0.015832727774977684,0.017733439803123474,0.019222470000386238,0.02193448506295681,0.020167462527751923,0.01864650286734104,0.017358966171741486,0.016635367646813393,0.01597544364631176,0.01540963165462017,0.014445439912378788,0.015504349023103714,0.015977537259459496,0.018624337390065193,0.020756296813488007,0.020027901977300644,0.018195519223809242,0.0167497918009758,0.01624194160103798,0.015011663548648357,0.015231014229357243,0.01600617915391922,0.017173675820231438,0.01719488576054573,0.01732996106147766,0.01620117388665676,0.015340120531618595,0.014677188359200954,0.014032047241926193,0.01345045119524002,0.012845185585319996,0.013228003866970539,0.016750160604715347,0.01573030650615692,0.015045041218400002,0.014491959474980831,0.013861031271517277,0.01353323832154274,0.01301150768995285,0.012444491498172283,0.011911774054169655,0.017571309581398964,0.016799526289105415,0.018936507403850555,0.016729701310396194,0.015551780350506306,0.015230871737003326,
mse,0.02606850117444992,0.0015485474141314626,0.0006530319806188345,0.000437156151747331,0.0002981342258863151,0.00022317493858281523,0.00018957887368742377,0.0001699006970738992,0.00015590917610097677,0.000142991790198721,0.00013428236707113683,0.00012626327225007117,0.00011896623618667945,0.00011294741125311702,0.00010793437832035124,0.00011298134631942958,0.00023664726177230477,0.00023881843662820756,0.00018848053878173232,0.0001644004078116268,0.0001416889572283253,0.00012232975859660655,0.00010360479063820094,0.00011190898658242077,0.0002330769202671945,0.00018596756854094565,0.0002280588960275054,0.0001651386555749923,0.0001729472423903644,0.00014367900439538062,0.00015616761811543256,0.00016487050743307918,0.00012113287084503099,0.0001420982152922079,0.00014813033340033144,0.00012825368321500719,0.00011329010158078745,0.00010894449223997071,0.00010357002611272037,0.00012755811621900648,0.0001437214232282713,0.00016778954886831343,0.000138166913529858,0.00011961788550252095,0.00010625402501318604,9.42965125432238e-05,0.00012518068251665682,0.00010371724056312814,9.200428030453622e-05,8.41650107759051e-05,7.854123396100476e-05,7.156028732424602e-05,7.266494503710419e-05,7.683315925532952e-05,9.288379806093872e-05,0.00011167929187649861,0.00013355420378502458,0.0001123383262893185,9.749471792019904e-05,8.643813634989783e-05,7.953439489938319e-05,7.307550549739972e-05,6.861769361421466e-05,6.443206075346097e-05,7.467516843462363e-05,8.06356401881203e-05,9.831586066866294e-05,0.00011813238961622119,0.00010972523887176067,9.410087659489363e-05,8.167103078449145e-05,7.605932478327304e-05,6.922602915437892e-05,7.007602107478306e-05,7.625717989867553e-05,8.198220166377723e-05,8.290662663057446e-05,8.613889804109931e-05,7.626446313224733e-05,6.899166328366846e-05,6.321645923890173e-05,5.8351819461677223e-05,5.414941915660165e-05,5.012505062040873e-05,5.408130164141767e-05,8.04887167760171e-05,7.071037543937564e-05,6.669655704172328e-05,6.179379124660045e-05,5.6904613302322105e-05,5.441509711090475e-05,5.046627120464109e-05,4.6720051614101976e-05,4.367412839201279e-05,9.150576806860045e-05,7.89212790550664e-05,9.744308044901118e-05,7.949018618091941e-05,7.063149678288028e-05,6.923764158273116e-05,
mae,0.11037367582321167,0.028289318084716797,0.018334979191422462,0.015356553718447685,0.013038912788033485,0.011401524767279625,0.010504468344151974,0.01006503775715828,0.009617323987185955,0.00916434358805418,0.008926882408559322,0.008656047284603119,0.008534267544746399,0.008225645869970322,0.008116246201097965,0.008189770393073559,0.011998336762189865,0.012368627823889256,0.01108161173760891,0.010284866206347942,0.009299216791987419,0.008462345227599144,0.007842243649065495,0.008130370639264584,0.011836784891784191,0.010743466205894947,0.012036922387778759,0.010265645571053028,0.010572786442935467,0.009338823147118092,0.00978735089302063,0.010069855488836765,0.008509313687682152,0.009539918042719364,0.009787315502762794,0.009216559119522572,0.008564574643969536,0.008371739648282528,0.007996312342584133,0.008691990748047829,0.009304541163146496,0.010634178295731544,0.009245539084076881,0.008476341143250465,0.008034419268369675,0.007470053620636463,0.00863466877490282,0.007738429121673107,0.007379146758466959,0.007215338759124279,0.007024230435490608,0.006577547173947096,0.006506368517875671,0.006765503436326981,0.007551442366093397,0.008252340368926525,0.009379853494465351,0.00851645041257143,0.007895108312368393,0.007437538355588913,0.0069970618933439255,0.006805559620261192,0.006558702792972326,0.006119485478848219,0.006410779897123575,0.006813457701355219,0.008047793991863728,0.008884533308446407,0.00861121155321598,0.007590606343001127,0.006881317589432001,0.006858707871288061,0.006374270189553499,0.006505219731479883,0.006832152139395475,0.007173832040280104,0.007225241046398878,0.007227086462080479,0.006698261946439743,0.006370076444000006,0.006110855843871832,0.005874659400433302,0.005725409835577011,0.005519564263522625,0.005640985909849405,0.007194685284048319,0.006737511605024338,0.006239715963602066,0.005992614198476076,0.005739775951951742,0.005654115695506334,0.005481808912009001,0.005309293977916241,0.005131927784532309,0.007457635831087828,0.007130715996026993,0.008203551173210144,0.007296431809663773,0.006509436294436455,0.006381214130669832,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 20563     
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 20564     
=================================================================
Total params: 41,127
Trainable params: 41,127
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 16)                80        
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 1024)              17408     
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 3075      
=================================================================
Total params: 20,563
Trainable params: 20,563
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 1024)              4096      
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 16)                16400     
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 68        
=================================================================
Total params: 20,564
Trainable params: 20,564
Non-trainable params: 0
_________________________________________________________________
