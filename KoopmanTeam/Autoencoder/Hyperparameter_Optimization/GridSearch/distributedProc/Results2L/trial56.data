2021-06-26
loss,0.3159593343734741,0.09071182459592819,0.058816537261009216,0.057223960757255554,0.04898345470428467,0.05467502772808075,0.044910136610269547,0.039722785353660583,0.04440765455365181,0.040153201669454575,0.04161737114191055,0.024878717958927155,0.023044856265187263,0.0220368392765522,0.021316777914762497,0.020463472232222557,0.01973038911819458,0.01892463117837906,0.018848612904548645,0.018192637711763382,0.01788780279457569,0.01757345162332058,0.017174165695905685,0.016979552805423737,0.016776548698544502,0.016839150339365005,0.024335511028766632,0.028703724965453148,0.025528859347105026,0.027409732341766357,0.02514537423849106,0.027881508693099022,0.026255983859300613,0.024170048534870148,0.023155612871050835,0.022121991962194443,0.022667014971375465,0.022922709584236145,0.022582883015275,0.02332242578268051,0.023387935012578964,0.024546651169657707,0.02292543463408947,0.023063329979777336,0.02069258876144886,0.015837671235203743,0.01712626963853836,0.022370869293808937,0.023402750492095947,0.021869923919439316,0.01955091953277588,0.018126418814063072,0.019858049228787422,0.016392996534705162,0.01569964922964573,0.014633497223258018,0.021455921232700348,0.020740507170557976,0.018915025517344475,0.019661245867609978,0.018122531473636627,0.020061425864696503,0.023090973496437073,0.018928246572613716,0.01716240495443344,0.018697820603847504,0.019835524260997772,0.017757894471287727,0.019834022969007492,0.01834888756275177,0.017804421484470367,0.01898014172911644,0.018642038106918335,0.017583223059773445,0.01704329438507557,0.016484957188367844,0.017796672880649567,0.017231134697794914,0.017604444175958633,0.018951548263430595,0.01705307327210903,0.015963487327098846,0.017950301989912987,0.01844329945743084,0.017043691128492355,0.01770430989563465,0.01603502593934536,0.016551872715353966,0.015377090312540531,0.014050635509192944,0.013256889767944813,0.01231627818197012,0.012139875441789627,0.018583768978714943,0.016756819561123848,0.01371005643159151,0.015202414244413376,0.015499694272875786,0.015751652419567108,0.015141589567065239,
mse,0.03717869892716408,0.002653262345120311,0.0010988822905346751,0.001013981644064188,0.0007345644407905638,0.0008807273698039353,0.0006157590541988611,0.0004569801385514438,0.0005635226261802018,0.0004616551741492003,0.0005268630920909345,0.0001837940508266911,0.00015764439012855291,0.00014437038043979555,0.00013607778237201273,0.00012447740300558507,0.0001165014473372139,0.00010997807839885354,0.00010646603186614811,9.911409870255738e-05,9.677228808868676e-05,9.128680540015921e-05,8.812095620669425e-05,8.70540679898113e-05,8.415528282057494e-05,8.688413799973205e-05,0.00019262493879068643,0.0002310636336915195,0.00018872274085879326,0.0002142703888239339,0.00018130344687961042,0.00022549576533492655,0.0001962888054549694,0.00016614532796666026,0.00015623014769516885,0.00015014843665994704,0.00014792541333008558,0.00014802292571403086,0.00014782382640987635,0.000155967878527008,0.0001611122424947098,0.00017427375132683665,0.0001545574195915833,0.00015370249457191676,0.00011874472693307325,7.880273915361613e-05,9.2556860181503e-05,0.00014322060451377183,0.00016002463235054165,0.00014086843293625861,0.00010630219185259193,9.330522880190983e-05,0.0001186137305921875,8.37485131341964e-05,7.564216502942145e-05,6.514810229418799e-05,0.00013388089428190142,0.00012541317846626043,0.00010714180825743824,0.0001121474415413104,9.76952287601307e-05,0.00011879285739269108,0.00015214341692626476,0.00010630107135511935,9.035545372171327e-05,0.00010009987454395741,0.00011179592547705397,9.496713028056547e-05,0.00011284524225629866,9.657350165070966e-05,9.316078649135306e-05,0.00010273405496263877,9.872288501355797e-05,8.872785838320851e-05,8.296591113321483e-05,8.11627323855646e-05,8.961743878899142e-05,8.431260357610881e-05,8.872095349943265e-05,0.00010078354534925893,8.197282295441255e-05,7.423343049595132e-05,9.267790301237255e-05,9.984417556552216e-05,8.391874143853784e-05,9.080138988792896e-05,7.506527617806569e-05,7.619132520630956e-05,6.709942681482062e-05,5.755386155215092e-05,5.669849633704871e-05,4.8169971705647185e-05,4.5982073061168194e-05,9.916696581058204e-05,8.339276246260852e-05,5.861327372258529e-05,7.095355249475688e-05,7.247082248795778e-05,7.052981527522206e-05,6.388207839336246e-05,
mae,0.1365983635187149,0.03951701521873474,0.025129958987236023,0.02458367869257927,0.020653435960412025,0.023044753819704056,0.019297095015645027,0.016847476363182068,0.01889631524682045,0.01709285005927086,0.017909914255142212,0.010640437714755535,0.009829900227487087,0.009391939267516136,0.009123014286160469,0.008711681701242924,0.008483251556754112,0.008088430389761925,0.008013445883989334,0.007756073493510485,0.0076007056050002575,0.007407278288155794,0.007273378316313028,0.00723345996811986,0.007108582183718681,0.007140329107642174,0.010440180078148842,0.012284115888178349,0.010985318571329117,0.011664038524031639,0.010576745495200157,0.011884696781635284,0.011037401854991913,0.010191405192017555,0.009794295765459538,0.009401529096066952,0.00959755852818489,0.009679368697106838,0.009686547331511974,0.00987335667014122,0.00988329853862524,0.010556262917816639,0.009707937017083168,0.009854842908680439,0.008776954375207424,0.006751797161996365,0.007261525373905897,0.009507567621767521,0.009929376654326916,0.009399320930242538,0.008462410420179367,0.0077333105728030205,0.00859122909605503,0.007001012563705444,0.006652136333286762,0.006230782251805067,0.00921507366001606,0.008871245197951794,0.008015715517103672,0.00832555815577507,0.007678183261305094,0.008547739125788212,0.009780650958418846,0.008048271760344505,0.007237541489303112,0.007942198775708675,0.008501588366925716,0.007461302913725376,0.008505194447934628,0.0078010861761868,0.007414311170578003,0.008084569126367569,0.007951551117002964,0.007545379921793938,0.007270798087120056,0.0070207249373197556,0.0075927539728581905,0.007240957580506802,0.0073494152165949345,0.008012148551642895,0.007137754466384649,0.00672940444201231,0.007656544912606478,0.007794534787535667,0.007191197481006384,0.007414583116769791,0.006801494862884283,0.007164056412875652,0.006649973336607218,0.0059685176238417625,0.005648414604365826,0.005277333781123161,0.005177712999284267,0.007914823479950428,0.007107256446033716,0.0058065433986485004,0.006472441367805004,0.006515811197459698,0.006719301920384169,0.0065801627933979034,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 43075     
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 43076     
=================================================================
Total params: 86,151
Trainable params: 86,151
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 2048)              10240     
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 16)                32784     
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 51        
=================================================================
Total params: 43,075
Trainable params: 43,075
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 16)                64        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 2048)              34816     
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 8196      
=================================================================
Total params: 43,076
Trainable params: 43,076
Non-trainable params: 0
_________________________________________________________________
