2021-06-26
loss,0.3249552547931671,0.19327311217784882,0.07134924083948135,0.04140327498316765,0.03409825637936592,0.04095759987831116,0.035404711961746216,0.0307940524071455,0.02603774145245552,0.02337929792702198,0.022727157920598984,0.021579300984740257,0.020663633942604065,0.019716383889317513,0.01930798962712288,0.018919015303254128,0.018399210646748543,0.01801113970577717,0.017660066485404968,0.01739906147122383,0.017124023288488388,0.016789661720395088,0.016649315133690834,0.016355108469724655,0.01605612225830555,0.015997562557458878,0.015742111951112747,0.015452554449439049,0.02211059257388115,0.026173509657382965,0.023739203810691833,0.022021224722266197,0.023497916758060455,0.02626670151948929,0.02299785614013672,0.021190157160162926,0.0200948528945446,0.019169453531503677,0.018514297902584076,0.0178961381316185,0.017313525080680847,0.02670018933713436,0.027449030429124832,0.023450160399079323,0.020663293078541756,0.020919281989336014,0.019694697111845016,0.018752019852399826,0.023986849933862686,0.02389717847108841,0.021921293810009956,0.0208017248660326,0.019775409251451492,0.01882067508995533,0.018126429989933968,0.01753845438361168,0.017044585198163986,0.016483239829540253,0.015754161402583122,0.015249546617269516,0.015223783440887928,0.015261584892868996,0.017753388732671738,0.020956726744771004,0.019042180851101875,0.01758081465959549,0.01639658957719803,0.016958054155111313,0.02202758379280567,0.019813217222690582,0.018617883324623108,0.0177678894251585,0.017078924924135208,0.016328750178217888,0.01586318202316761,0.01528056338429451,0.014997083693742752,0.014353709295392036,0.013409138657152653,0.012766098603606224,0.012258528731763363,0.011225373484194279,0.015353286638855934,0.016016561537981033,0.015151343308389187,0.014603594318032265,0.014221983030438423,0.013690151274204254,0.013273699209094048,0.012823917903006077,0.012261233292520046,0.01221525389701128,0.02004425786435604,0.017856692895293236,0.016337919980287552,0.01548745110630989,0.014661495573818684,0.014664432033896446,0.02098684012889862,0.018451470881700516,
mse,0.037060655653476715,0.013739369809627533,0.001834732131101191,0.000592311262153089,0.00038524853880517185,0.0005212369724176824,0.00038148098974488676,0.0002886275469791144,0.00021053417003713548,0.00017655327974352986,0.00016012322157621384,0.00014177896082401276,0.000130282060126774,0.00012034283281536773,0.0001153612247435376,0.00010742668382590637,0.00010194014612352476,9.668729035183787e-05,9.285661508329213e-05,8.97256177267991e-05,8.697026351001114e-05,8.366216934518889e-05,8.11460631666705e-05,7.88285760791041e-05,7.547979475930333e-05,7.450690463883802e-05,7.203912537079304e-05,7.122659008018672e-05,0.00015018538397271186,0.00019340771541465074,0.0001615333167137578,0.00013821928587276489,0.00015900142898317426,0.0001872071879915893,0.00014628445205744356,0.00012506436905823648,0.0001132958204834722,0.00010521445074118674,9.735607454786077e-05,9.1291825810913e-05,8.598561544204131e-05,0.0002043160202447325,0.0002088325854856521,0.0001582951663294807,0.00012625387171283364,0.00012216380855534226,0.00010910519631579518,0.00010107251000590622,0.0001659935514908284,0.00016001959738787264,0.00013634987408295274,0.00012263019743841141,0.00011163600720465183,0.00010172340989811346,9.518946171738207e-05,8.891455945558846e-05,8.37557963677682e-05,7.751596422167495e-05,6.961329199839383e-05,6.538167508551851e-05,6.923871842445806e-05,6.825960008427501e-05,9.391893399879336e-05,0.00012341195542830974,0.00010075380851048976,8.744685328565538e-05,7.694435043958947e-05,8.452103793388233e-05,0.00013640598626807332,0.00010768735955934972,9.654240420786664e-05,8.823566167848185e-05,8.24551098048687e-05,7.524964166805148e-05,7.099013600964099e-05,6.61199155729264e-05,6.417227268684655e-05,5.964896990917623e-05,5.286484883981757e-05,4.820508911507204e-05,4.434265065356158e-05,3.843662852887064e-05,6.686220149276778e-05,7.036814349703491e-05,6.368015601765364e-05,5.986067480989732e-05,5.7206518249586225e-05,5.304336082190275e-05,5.037660230300389e-05,4.77911344205495e-05,4.4329295633360744e-05,4.4773612899007276e-05,0.00010925497190328315,8.785439422354102e-05,7.573737093480304e-05,6.800547998864204e-05,6.166569073684514e-05,6.306388240773231e-05,0.00012320610403548926,9.349460015073419e-05,
mae,0.13763412833213806,0.08150725066661835,0.02929621748626232,0.017607588320970535,0.014478827826678753,0.017561333253979683,0.015359887853264809,0.013104567304253578,0.011055302806198597,0.009851583279669285,0.00961469579488039,0.009223958477377892,0.008814146742224693,0.008412917144596577,0.008216271176934242,0.008045095950365067,0.007763888221234083,0.0075643728487193584,0.0074718184769153595,0.00734848203137517,0.007316939998418093,0.007116132881492376,0.007086825557053089,0.00698104640468955,0.006845575757324696,0.0067680696956813335,0.0066763972863554955,0.006608199328184128,0.00944663118571043,0.01146684866398573,0.0104029830545187,0.009584331884980202,0.010201440192759037,0.011039072647690773,0.009603834711015224,0.008845326490700245,0.0084095923230052,0.007965127937495708,0.007565566338598728,0.007233100477606058,0.007142761256545782,0.011245720088481903,0.010946196503937244,0.009826678782701492,0.009007623419165611,0.009129777550697327,0.008587164804339409,0.008326888084411621,0.009975986555218697,0.010003548115491867,0.00928460992872715,0.00907795038074255,0.009077701717615128,0.008605879731476307,0.00825448241084814,0.00790141150355339,0.007563502062112093,0.007106191013008356,0.0063782501965761185,0.006183570716530085,0.006431616377085447,0.006517153233289719,0.007564263883978128,0.008726304396986961,0.007758405990898609,0.007263255305588245,0.00709318183362484,0.0072710528038442135,0.009400459937751293,0.008426308631896973,0.007863378152251244,0.0074460674077272415,0.007214031182229519,0.0070268213748931885,0.006903256755322218,0.00660877488553524,0.006326386239379644,0.005966005381196737,0.0056251054629683495,0.0054632569663226604,0.005235233809798956,0.004770195111632347,0.006510463543236256,0.006626885384321213,0.0062417155131697655,0.0060728685930371284,0.0060079218819737434,0.005863061640411615,0.005717804655432701,0.005581734701991081,0.005280403885990381,0.0051866453140974045,0.008687851019203663,0.007847853004932404,0.007118940353393555,0.006714729126542807,0.006327065639197826,0.006211165338754654,0.008971953764557838,0.008129142224788666,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 9027      
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 9028      
=================================================================
Total params: 18,055
Trainable params: 18,055
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 64)                320       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 128)               8320      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 387       
=================================================================
Total params: 9,027
Trainable params: 9,027
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 128)               512       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 64)                8256      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 260       
=================================================================
Total params: 9,028
Trainable params: 9,028
Non-trainable params: 0
_________________________________________________________________
