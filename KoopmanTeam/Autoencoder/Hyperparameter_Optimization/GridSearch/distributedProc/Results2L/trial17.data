2021-06-26
loss,0.4605361223220825,0.2276124358177185,0.13828179240226746,0.06253568828105927,0.0449632853269577,0.035443298518657684,0.030674047768115997,0.027912378311157227,0.02597678266465664,0.0243171826004982,0.02336263470351696,0.0224155243486166,0.021442679688334465,0.0208152886480093,0.020318660885095596,0.019808709621429443,0.019164491444826126,0.018924826756119728,0.018367992714047432,0.017938978970050812,0.017727619037032127,0.017340010032057762,0.017105786129832268,0.016882916912436485,0.016588162630796432,0.016284791752696037,0.01615305058658123,0.01591535657644272,0.015712110325694084,0.015554795041680336,0.01538036111742258,0.015198307111859322,0.015070842579007149,0.01495667826384306,0.014792426489293575,0.014736302196979523,0.014505840837955475,0.01449002604931593,0.014334932900965214,0.014113995246589184,0.014188267290592194,0.0140228271484375,0.013954713009297848,0.013863257132470608,0.013744287192821503,0.013664006255567074,0.013622710481286049,0.013550412841141224,0.013444479554891586,0.013450664468109608,0.013323567807674408,0.013261067681014538,0.013246859423816204,0.013111434876918793,0.013024318031966686,0.013003284111618996,0.012948690913617611,0.012949791736900806,0.01284322701394558,0.012918761000037193,0.012729057110846043,0.012701798230409622,0.012631038203835487,0.012626256793737411,0.012517512775957584,0.012498251162469387,0.012552435509860516,0.012422892265021801,0.012318287044763565,0.012445487082004547,0.012417752295732498,0.01230120100080967,0.012290353886783123,0.012219331227242947,0.012196090072393417,0.01219379436224699,0.012094485573470592,0.012071427889168262,0.012036469765007496,0.012087570503354073,0.011988426558673382,0.011978921480476856,0.011954392306506634,0.01196103822439909,0.011951926164329052,0.011826561763882637,0.011770547367632389,0.011774024926126003,0.01181415468454361,0.01173302624374628,0.011769771575927734,0.011639812961220741,0.01169622503221035,0.011694877408444881,0.01151579525321722,0.01162330899387598,0.011624735780060291,0.011494277976453304,0.011585189960896969,0.011503411456942558,
mse,0.07776174694299698,0.018173012882471085,0.007733145263046026,0.0016154018230736256,0.0008280485053546727,0.000549559248611331,0.00040514997090213,0.0003182009677402675,0.00026254772092215717,0.00022318452829495072,0.0002016910002566874,0.00018223891675006598,0.00016534089809283614,0.000154012770508416,0.0001452067372156307,0.00013569844304583967,0.00012817623792216182,0.0001229722547577694,0.00011554094089660794,0.0001104028124245815,0.00010626731091178954,0.00010086067777592689,9.783267887542024e-05,9.467361815040931e-05,9.041485463967547e-05,8.795761823421344e-05,8.451725443592295e-05,8.208116196328774e-05,7.960520451888442e-05,7.831751281628385e-05,7.581847603432834e-05,7.335568079724908e-05,7.229868060676381e-05,7.077480404404923e-05,6.851615034975111e-05,6.818175461376086e-05,6.596854655072093e-05,6.516991561511531e-05,6.362805288517848e-05,6.108695379225537e-05,6.218568159965798e-05,6.0520615079440176e-05,5.959940972388722e-05,5.868560037924908e-05,5.752237484557554e-05,5.6792770919855684e-05,5.6197830417659134e-05,5.582120138569735e-05,5.441352186608128e-05,5.4577187256654724e-05,5.34997416252736e-05,5.277504533296451e-05,5.244868225418031e-05,5.152392986929044e-05,5.044325735070743e-05,5.037602022639476e-05,4.9691054300637916e-05,4.9945301725529134e-05,4.882479697698727e-05,4.949294452671893e-05,4.7839857870712876e-05,4.7529749281238765e-05,4.714785245596431e-05,4.693578011938371e-05,4.5962333388160914e-05,4.5909757318440825e-05,4.609258030541241e-05,4.517559500527568e-05,4.4475447793956846e-05,4.548547440208495e-05,4.521178561844863e-05,4.424362487043254e-05,4.403443381306715e-05,4.359878585091792e-05,4.3524152715690434e-05,4.339333463576622e-05,4.253382940078154e-05,4.232350693200715e-05,4.2049527110066265e-05,4.2410098103573546e-05,4.174165951553732e-05,4.166727376286872e-05,4.151242319494486e-05,4.1417846659896895e-05,4.1639468690846115e-05,4.051261566928588e-05,3.997593012172729e-05,4.020734195364639e-05,4.0371716750087216e-05,3.960489630117081e-05,3.9901529817143455e-05,3.891325832228176e-05,3.955666034016758e-05,3.9395315980073065e-05,3.83476035494823e-05,3.880753865814768e-05,3.880814983858727e-05,3.799162004725076e-05,3.8564990973100066e-05,3.8183956348802894e-05,
mae,0.19176740944385529,0.09282352775335312,0.05643393471837044,0.025759022682905197,0.018814021721482277,0.014972344972193241,0.012992063537240028,0.011808378621935844,0.01100890152156353,0.010349683463573456,0.009916996583342552,0.00952441617846489,0.009091963991522789,0.008854524232447147,0.008633557707071304,0.008343985304236412,0.0081394724547863,0.007986698299646378,0.007746364921331406,0.007547704968601465,0.007487589027732611,0.0072969007305800915,0.007293448317795992,0.0070435404777526855,0.007003416307270527,0.006794380489736795,0.006799874361604452,0.0067432476207613945,0.006632125936448574,0.006456205155700445,0.006453503854572773,0.00641148816794157,0.00632336363196373,0.006274915765970945,0.00625249557197094,0.006167311687022448,0.006095102988183498,0.006080152932554483,0.005941115785390139,0.005974707659333944,0.005844181403517723,0.005912747234106064,0.005878472235053778,0.005735325627028942,0.005751166492700577,0.005837026052176952,0.005734552163630724,0.005631698295474052,0.005659630987793207,0.005652701947838068,0.005628309212625027,0.005619711242616177,0.005510842893272638,0.005484710447490215,0.005536118056625128,0.005459981970489025,0.005500052124261856,0.005427780095487833,0.005347132682800293,0.00536518357694149,0.005401505157351494,0.005349361803382635,0.005318245850503445,0.005244757980108261,0.005294973962008953,0.0052575767040252686,0.005268456414341927,0.005281382240355015,0.005165679380297661,0.005220093764364719,0.005183962173759937,0.005227180663496256,0.005149070173501968,0.0051391408778727055,0.005141767673194408,0.005145668983459473,0.005071284715086222,0.005114182364195585,0.005072284955531359,0.005092893727123737,0.005053061991930008,0.005030789878219366,0.005041256081312895,0.004960707854479551,0.004991598892956972,0.004985464736819267,0.0049289679154753685,0.004927998408675194,0.004969585686922073,0.004936030134558678,0.004953064955770969,0.004970400594174862,0.004929028917104006,0.004920207429677248,0.004872424993664026,0.004919875413179398,0.004886091686785221,0.004884525667876005,0.004870215430855751,0.004803749267011881,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 2499      
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 2500      
=================================================================
Total params: 4,999
Trainable params: 4,999
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 64)                320       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 32)                2080      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 99        
=================================================================
Total params: 2,499
Trainable params: 2,499
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 32)                128       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 64)                2112      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 260       
=================================================================
Total params: 2,500
Trainable params: 2,500
Non-trainable params: 0
_________________________________________________________________
