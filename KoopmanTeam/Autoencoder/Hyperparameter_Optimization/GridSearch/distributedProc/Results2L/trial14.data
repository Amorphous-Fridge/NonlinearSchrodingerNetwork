2021-06-26
loss,0.2258145660161972,0.06651808321475983,0.037765953689813614,0.027227625250816345,0.024455513805150986,0.022637629881501198,0.021426433697342873,0.02793077751994133,0.03383005037903786,0.031066076830029488,0.030741650611162186,0.025078648701310158,0.022373585030436516,0.029152676463127136,0.028186345472931862,0.027837835252285004,0.02307545579969883,0.021371209993958473,0.021985886618494987,0.025912605226039886,0.02470104955136776,0.02510814741253853,0.02411477640271187,0.021982088685035706,0.0214565247297287,0.023323802277445793,0.020343391224741936,0.020335901528596878,0.020267250016331673,0.022731037810444832,0.021038087084889412,0.019268788397312164,0.017304623499512672,0.01977895013988018,0.019330700859427452,0.019500283524394035,0.019863326102495193,0.020178839564323425,0.020012693479657173,0.018270451575517654,0.017189083620905876,0.01632295921444893,0.01751754805445671,0.016841284930706024,0.018722916021943092,0.020314719527959824,0.01734963059425354,0.015556705184280872,0.01444698590785265,0.013840431347489357,0.01296455878764391,0.01356883067637682,0.01551892515271902,0.018370920792222023,0.01824631728231907,0.017381103709340096,0.016094136983156204,0.015183615498244762,0.01768440380692482,0.01692914590239525,0.016066867858171463,0.015284964814782143,0.014517681673169136,0.013734843581914902,0.013197401538491249,0.012617450207471848,0.013206885196268559,0.015613770112395287,0.015631822869181633,0.01530428882688284,0.014557922258973122,0.013250049203634262,0.015680139884352684,0.014674420468509197,0.013844596222043037,0.01334763690829277,0.012772820889949799,0.012350899167358875,0.012017780914902687,0.014251278713345528,0.015351341105997562,0.013988659717142582,0.016346191987395287,0.015203147195279598,0.013871490024030209,0.012762770988047123,0.012183967046439648,0.0117355827242136,0.011273416690528393,0.01114626508206129,0.013435211032629013,0.013740522786974907,0.01304425485432148,0.013809198513627052,0.013327758759260178,0.012093747965991497,0.01156412810087204,0.013403668068349361,0.015689166262745857,0.014823650941252708,
mse,0.022992348298430443,0.0013534952886402607,0.0004461383796297014,0.00022681667178403586,0.00017849892901722342,0.0001535995106678456,0.00013698931434191763,0.00024251578724943101,0.00032471425947733223,0.0002855005441233516,0.0002676231088116765,0.00018661649664863944,0.00015294289914891124,0.000244463182752952,0.00022247000015340745,0.00022124963288661093,0.00015570237883366644,0.00014141474093776196,0.0001494620810262859,0.00019003610941581428,0.0001702547597233206,0.00017770026170182973,0.00016046795644797385,0.00013802993635181338,0.00013437475718092173,0.0001498063065810129,0.00011949752661166713,0.00011982892465312034,0.00011696532601490617,0.00014673593977931887,0.0001261151337530464,0.00010959994688164443,9.35179486987181e-05,0.00011467472359072417,0.00010796682181535289,0.00011116981477243826,0.0001103304821299389,0.000114574606413953,0.00010885873052757233,9.392384527018294e-05,8.416669152211398e-05,7.625294529134408e-05,9.601545752957463e-05,8.520104893250391e-05,0.0001010437190416269,0.00011306226224405691,8.897890074877068e-05,7.368168735411018e-05,6.435918476199731e-05,5.892203262192197e-05,5.217172292759642e-05,5.714717190130614e-05,7.384319906122983e-05,9.558775491314009e-05,9.441391739528626e-05,8.511858322890475e-05,7.378845475614071e-05,6.86366474837996e-05,8.848083962220699e-05,8.037800580495968e-05,7.391565304715186e-05,6.681694503640756e-05,6.1027458286844194e-05,5.591440640273504e-05,5.156113184057176e-05,4.728975545731373e-05,5.303474608808756e-05,7.142970571294427e-05,6.756761285942048e-05,6.803486758144572e-05,6.156034214654937e-05,5.41283079655841e-05,6.864956230856478e-05,6.126264634076506e-05,5.5182972573675215e-05,5.167319977772422e-05,4.786764839082025e-05,4.5087250327924266e-05,4.5778411731589586e-05,5.96116587985307e-05,6.46658445475623e-05,5.653938205796294e-05,7.431173435179517e-05,6.410344212781638e-05,5.444523776532151e-05,4.653108771890402e-05,4.3185460526729e-05,4.003229696536437e-05,3.753650526050478e-05,3.794079020735808e-05,5.2431427320698276e-05,5.278254684526473e-05,4.8665489885024726e-05,5.53440440853592e-05,5.080018308945e-05,4.360979801276699e-05,4.0942089981399477e-05,5.268114182399586e-05,6.934255361557007e-05,6.385580491041765e-05,
mae,0.09609424322843552,0.02830142341554165,0.016081487759947777,0.011610392481088638,0.010450300760567188,0.009632059372961521,0.009093224070966244,0.011961922980844975,0.014382671564817429,0.013193152844905853,0.01292006392031908,0.01060201320797205,0.009553857147693634,0.012446098029613495,0.012019088491797447,0.01167306024581194,0.0099746473133564,0.009200976230204105,0.009334784932434559,0.010887485928833485,0.010357019491493702,0.01055033691227436,0.010231797583401203,0.009286553598940372,0.009051022119820118,0.009864441119134426,0.0085855508223176,0.008745694532990456,0.008614973165094852,0.009794635698199272,0.00905570201575756,0.008144892752170563,0.007514436263591051,0.008344766683876514,0.008144926279783249,0.008298052474856377,0.008406106382608414,0.008576023392379284,0.008341715671122074,0.0076072788797318935,0.007292351685464382,0.006951027549803257,0.007447624579071999,0.007200842723250389,0.007979067042469978,0.008735702373087406,0.007347405422478914,0.00663512060418725,0.006179605145007372,0.005910704843699932,0.005536010954529047,0.005665577482432127,0.006601540371775627,0.007530478294938803,0.007751852739602327,0.0071080452762544155,0.0065132672898471355,0.00635835574939847,0.00748728821054101,0.007099323906004429,0.006679154466837645,0.006414179690182209,0.006079843267798424,0.0058418381959199905,0.005640340503305197,0.005410945974290371,0.0055703092366456985,0.006578526925295591,0.006724054925143719,0.006531643681228161,0.006197011563926935,0.0056508504785597324,0.006598851177841425,0.006142066791653633,0.005807993933558464,0.005632658489048481,0.005416951607912779,0.005191297736018896,0.005074393004179001,0.006005650851875544,0.006224578246474266,0.0057147229090332985,0.007133718580007553,0.0067098806612193584,0.005925917997956276,0.005231871735304594,0.005011075641959906,0.0048929317854344845,0.004743190482258797,0.0047439332120120525,0.005667279940098524,0.005901854485273361,0.005561577156186104,0.005958643741905689,0.005711408331990242,0.0050997380167245865,0.0048686545342206955,0.005698909983038902,0.006668537389487028,0.006310964003205299,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 37027     
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 37028     
=================================================================
Total params: 74,055
Trainable params: 74,055
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 32)                160       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 1024)              33792     
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 3075      
=================================================================
Total params: 37,027
Trainable params: 37,027
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 1024)              4096      
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 32)                32800     
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 132       
=================================================================
Total params: 37,028
Trainable params: 37,028
Non-trainable params: 0
_________________________________________________________________
