2021-06-26
loss,0.3081318438053131,0.07412607967853546,0.04991663619875908,0.04197147861123085,0.06352374702692032,0.04876457527279854,0.03794390708208084,0.04353386536240578,0.03671080991625786,0.03749636188149452,0.03154492750763893,0.033890482038259506,0.03535410389304161,0.0348614826798439,0.02963295951485634,0.03042501024901867,0.025592641904950142,0.023667575791478157,0.03089321404695511,0.029046209529042244,0.02802688628435135,0.02838078700006008,0.029419027268886566,0.026965726166963577,0.026704704388976097,0.025629675015807152,0.025953490287065506,0.02765151672065258,0.02486942708492279,0.02273033931851387,0.021642379462718964,0.018935201689600945,0.02413974516093731,0.021902238950133324,0.024892600253224373,0.023984596133232117,0.022825516760349274,0.02188233844935894,0.021260550245642662,0.02293052151799202,0.02242407016456127,0.02170589379966259,0.018110621720552444,0.01922454498708248,0.020029384642839432,0.022656025364995003,0.022917337715625763,0.020160259678959846,0.018597319722175598,0.01710396260023117,0.01751551777124405,0.01800212450325489,0.019595390185713768,0.021469363942742348,0.01879514940083027,0.01657961495220661,0.015384615398943424,0.018816182389855385,0.01985999383032322,0.018681323155760765,0.017700107768177986,0.01853122189640999,0.017882291227579117,0.015839936211705208,0.01710333675146103,0.017769712954759598,0.017742589116096497,0.017463525757193565,0.015592198818922043,0.01431177370250225,0.014168351888656616,0.014355569146573544,0.015990078449249268,0.01746356673538685,0.016057182103395462,0.015186123549938202,0.015706300735473633,0.01717694103717804,0.017115429043769836,0.017228685319423676,0.016413625329732895,0.015623454935848713,0.015358869917690754,0.015903489664196968,0.01609821990132332,0.014312375336885452,0.012573645450174809,0.014842677861452103,0.01571539416909218,0.01521968562155962,0.015054860152304173,0.014398552477359772,0.013281170278787613,0.014206575229763985,0.015751276165246964,0.015699904412031174,0.015075696632266045,0.014244550839066505,0.013484453782439232,0.014216831885278225,
mse,0.04027368128299713,0.0017792928265407681,0.0007393748965114355,0.0005170952645130455,0.0012180489720776677,0.0006903070025146008,0.0004225462325848639,0.0005560663994401693,0.0003906401980202645,0.0004038855549879372,0.0002925757726188749,0.00033193352282978594,0.0003593126020859927,0.00034543481888249516,0.00025731520145200193,0.00026379176415503025,0.0001949150610016659,0.00017073923663701862,0.0002751601568888873,0.00023630379291716963,0.00022866517247166485,0.0002286063099745661,0.0002470242907293141,0.00020669160585384816,0.00020245475752744824,0.00019359328143764287,0.0001987861905945465,0.00021570622629951686,0.0001778584555722773,0.00015219443594105542,0.00013958167983219028,0.00011225257912883535,0.00016856803267728537,0.00014266087964642793,0.0001761242892825976,0.00016045692609623075,0.00014902939437888563,0.0001343586773145944,0.0001311527012148872,0.00014791672583669424,0.00014004985860083252,0.00013422328629530966,0.00010239522816846147,0.00011248250666540116,0.00011612683738349006,0.000145864236401394,0.00014905034913681448,0.00011687702499330044,0.00010252648644382134,8.788509148871526e-05,9.250264702131972e-05,9.769829921424389e-05,0.00010933963494608179,0.00012929148215334862,0.0001055545435519889,8.340027125086635e-05,7.132047903724015e-05,0.00010412674600956962,0.00010781661694636568,9.616418537916616e-05,9.092198888538405e-05,9.805040463106707e-05,9.052691166289151e-05,7.517343328800052e-05,8.647728100186214e-05,8.909927419153973e-05,8.817292109597474e-05,8.726258965907618e-05,7.114293839549646e-05,6.303128611762077e-05,6.31623697699979e-05,6.251039303606376e-05,7.595671922899783e-05,8.733419963391498e-05,7.412426202790812e-05,6.727690197294578e-05,7.403591007459909e-05,8.32474252092652e-05,8.361526852240786e-05,8.421165694016963e-05,7.515826291637495e-05,7.122602983145043e-05,6.830671918578446e-05,7.270144851645455e-05,7.165129500208423e-05,6.083242624299601e-05,4.906144386040978e-05,6.614050653297454e-05,6.95976268616505e-05,6.686699634883553e-05,6.496746209450066e-05,5.9666290326276794e-05,5.2817380492342636e-05,6.193921581143513e-05,6.912725802976638e-05,6.861885049147531e-05,6.352311902446672e-05,5.847280044690706e-05,5.294202856021002e-05,5.914267967455089e-05,
mae,0.13209925591945648,0.03158777579665184,0.02111480012536049,0.017853189259767532,0.027184946462512016,0.02034556306898594,0.0163030456751585,0.018766315653920174,0.01564878039062023,0.015682511031627655,0.01330924965441227,0.01433157455176115,0.014867911115288734,0.014859838411211967,0.012544970959424973,0.012926207855343819,0.010885592550039291,0.010032537393271923,0.01303362287580967,0.012553547509014606,0.011997970752418041,0.011937991715967655,0.012146259658038616,0.011407728306949139,0.011381477117538452,0.010855503380298615,0.010986674576997757,0.011665629222989082,0.010571508668363094,0.009531326591968536,0.009207867085933685,0.008074191398918629,0.0102397371083498,0.009437973611056805,0.01051697600632906,0.010145001113414764,0.009765819646418095,0.00924292765557766,0.008877310901880264,0.00968410074710846,0.009451746940612793,0.0091722896322608,0.007695531938225031,0.008114968426525593,0.008524194359779358,0.009579502046108246,0.0094881197437644,0.008623826317489147,0.00787302665412426,0.007202748209238052,0.0074043418280780315,0.007583344355225563,0.008238722570240498,0.008900362998247147,0.007857424207031727,0.007105337921530008,0.006582919508218765,0.008007681928575039,0.008369509130716324,0.00801234133541584,0.007394378073513508,0.007818841375410557,0.0074806516058743,0.0067190080881118774,0.007323639467358589,0.007562199607491493,0.0074460553005337715,0.007453439757227898,0.006654082331806421,0.006116590928286314,0.006056040525436401,0.006100011989474297,0.0067955320701003075,0.0073004793375730515,0.006694580428302288,0.006391037721186876,0.006666306406259537,0.00729694589972496,0.007208309601992369,0.00734698073938489,0.007048762869089842,0.006682585924863815,0.006517264060676098,0.006777292583137751,0.006843064446002245,0.0059443325735628605,0.005355709698051214,0.006314415484666824,0.0066678463481366634,0.006428995635360479,0.006403835490345955,0.006140370387583971,0.0056290533393621445,0.006033723242580891,0.006610834039747715,0.006620202213525772,0.006410819943994284,0.006046731024980545,0.005786031484603882,0.006060490384697914,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 67843     
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 67844     
=================================================================
Total params: 135,687
Trainable params: 135,687
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 256)               1280      
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 256)               65792     
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 771       
=================================================================
Total params: 67,843
Trainable params: 67,843
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 256)               1024      
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 256)               65792     
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 1028      
=================================================================
Total params: 67,844
Trainable params: 67,844
Non-trainable params: 0
_________________________________________________________________
