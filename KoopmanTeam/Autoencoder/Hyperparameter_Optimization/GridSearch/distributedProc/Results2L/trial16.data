2021-06-26
loss,0.5451361536979675,0.18803642690181732,0.09838772565126419,0.06440342217683792,0.05332866683602333,0.045954618602991104,0.03971683606505394,0.03505614772439003,0.03088446334004402,0.028385410085320473,0.025874018669128418,0.024164605885744095,0.02285432256758213,0.02201051637530327,0.021275775507092476,0.02093403972685337,0.020271556451916695,0.019792459905147552,0.019524728879332542,0.019000409170985222,0.018684139475226402,0.018190551549196243,0.01794307306408882,0.017801795154809952,0.017311766743659973,0.016985328868031502,0.01681571826338768,0.016516482457518578,0.01633250154554844,0.016172261908650398,0.016030501574277878,0.01576986350119114,0.015604288317263126,0.015545475296676159,0.015306051820516586,0.015153241343796253,0.015035551972687244,0.01485747005790472,0.014873206615447998,0.014442196115851402,0.014636967331171036,0.014464354142546654,0.014294973574578762,0.014183152467012405,0.013990181498229504,0.014011833816766739,0.013875192031264305,0.013700536452233791,0.01374165341258049,0.013751175254583359,0.013557987287640572,0.013488917611539364,0.013407084159553051,0.01329209003597498,0.013305786065757275,0.013169100508093834,0.012994199991226196,0.013023904524743557,0.012900893576443195,0.012914538383483887,0.01277619693428278,0.012779613956809044,0.01278324332088232,0.012589653953909874,0.01257559284567833,0.012394490651786327,0.012520701624453068,0.012317530810832977,0.012286240234971046,0.012310179881751537,0.01218259334564209,0.012249571271240711,0.012148728594183922,0.012009033933281898,0.01205915305763483,0.012033252976834774,0.011921925470232964,0.01189664751291275,0.011853215284645557,0.011804021894931793,0.011779891327023506,0.011698330752551556,0.011688708327710629,0.011622361838817596,0.011559107340872288,0.011609860695898533,0.011501293629407883,0.011529610492289066,0.011465187184512615,0.01138116791844368,0.011375628411769867,0.011361397802829742,0.011205273680388927,0.011323255486786366,0.011156614869832993,0.011229320429265499,0.01127668283879757,0.011188392527401447,0.011193854734301567,0.011125044897198677,
mse,0.13412082195281982,0.012481778860092163,0.003525041975080967,0.0014822473749518394,0.0010500490898266435,0.0007957946509122849,0.0006050108931958675,0.00046728391316719353,0.0003629520069807768,0.00030115823028609157,0.00025326135801151395,0.00021969851513858885,0.00019506276294123381,0.0001772256218828261,0.00016445106302853674,0.00015678216004744172,0.00014609821664635092,0.0001376889122184366,0.00013453605060931295,0.0001256294926861301,0.00012124908971600235,0.0001140649983426556,0.00011063045531045645,0.00010761935118352994,0.00010166247375309467,9.758899250300601e-05,9.630596468923613e-05,9.258606587536633e-05,8.982213330455124e-05,8.826459088595584e-05,8.694158168509603e-05,8.338364568771794e-05,8.101595449261367e-05,8.059453102760017e-05,7.81417838879861e-05,7.65097574912943e-05,7.469961565220729e-05,7.285265746759251e-05,7.340431329794228e-05,6.903856410644948e-05,7.070280844345689e-05,6.927933281986043e-05,6.744080019416288e-05,6.615104211959988e-05,6.339244282571599e-05,6.450483488151804e-05,6.309892341960222e-05,6.1011407524347305e-05,6.187649705680087e-05,6.132675480330363e-05,5.942896314081736e-05,5.8909579820465297e-05,5.8383553550811484e-05,5.732953650294803e-05,5.718141619581729e-05,5.565625542658381e-05,5.4207932407734916e-05,5.44446193089243e-05,5.3120376833248883e-05,5.347528349375352e-05,5.2244256949052215e-05,5.195007543079555e-05,5.187223359826021e-05,5.0305236072745174e-05,5.0406233640387654e-05,4.8952209908748046e-05,5.000778764951974e-05,4.797472138307057e-05,4.77906214655377e-05,4.762607204611413e-05,4.705628816736862e-05,4.741212978842668e-05,4.655990414903499e-05,4.558145519695245e-05,4.596520739141852e-05,4.5386554120341316e-05,4.442581121111289e-05,4.431683919392526e-05,4.4033444282831624e-05,4.361110404715873e-05,4.2878160456893966e-05,4.2623527406249195e-05,4.2701409256551415e-05,4.1910414438461885e-05,4.140468445257284e-05,4.1917031921911985e-05,4.132391404709779e-05,4.121528036193922e-05,4.0846996853360906e-05,4.021977656520903e-05,4.011185956187546e-05,3.9788297726772726e-05,3.848843334708363e-05,3.9489259506808594e-05,3.8347585359588265e-05,3.844979437417351e-05,3.880274744005874e-05,3.835121970041655e-05,3.8552934711333364e-05,3.776006633415818e-05,
mae,0.2301217019557953,0.0825999453663826,0.043141767382621765,0.027417751029133797,0.02260430157184601,0.019494257867336273,0.0168475192040205,0.01494774129241705,0.013160130940377712,0.012093487195670605,0.011015675030648708,0.010323338210582733,0.009724435396492481,0.009334225207567215,0.009033997543156147,0.00889267586171627,0.008606739342212677,0.008395804092288017,0.008288510143756866,0.008093009702861309,0.007955525070428848,0.007730383425951004,0.007619936019182205,0.007542184554040432,0.0073471772484481335,0.007201415952295065,0.007146519608795643,0.007030514068901539,0.006953869946300983,0.006861756555736065,0.006759691052138805,0.006649404298514128,0.006626182235777378,0.00661369739100337,0.006513318046927452,0.006436557043343782,0.006364598870277405,0.006315247621387243,0.00630000839009881,0.006144964601844549,0.006233884021639824,0.006108809728175402,0.006091077346354723,0.006014934275299311,0.005930893588811159,0.005961105227470398,0.0058310069143772125,0.005818473175168037,0.005836538504809141,0.005822255741804838,0.005767225753515959,0.005670204758644104,0.00569915771484375,0.005670941900461912,0.005633784458041191,0.0055261398665606976,0.005495926830917597,0.005508808884769678,0.005522879306226969,0.005510976538062096,0.005397692788392305,0.00542865414172411,0.005416469648480415,0.00532383332028985,0.005375607404857874,0.005279298406094313,0.005331545602530241,0.005249405279755592,0.005144755356013775,0.0052093761041760445,0.005169681739062071,0.005183688364923,0.00513884611427784,0.005124339368194342,0.005125196650624275,0.005080902948975563,0.005100904498249292,0.005063673481345177,0.005056858528405428,0.005007247906178236,0.00499323382973671,0.004947878886014223,0.005003220401704311,0.004981229081749916,0.0048604849725961685,0.0049148197285830975,0.004914109595119953,0.004882247652858496,0.004890024196356535,0.0048639425076544285,0.004808806348592043,0.00486338185146451,0.004754179622977972,0.0047346060164272785,0.004764788318425417,0.004765520337969065,0.004793546628206968,0.004737477749586105,0.004738634917885065,0.004720924887806177,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 1411      
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 1412      
=================================================================
Total params: 2,823
Trainable params: 2,823
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 64)                320       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 16)                1040      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 51        
=================================================================
Total params: 1,411
Trainable params: 1,411
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 16)                64        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 64)                1088      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 260       
=================================================================
Total params: 1,412
Trainable params: 1,412
Non-trainable params: 0
_________________________________________________________________
