2021-06-26
loss,0.27567511796951294,0.05999113619327545,0.033559270203113556,0.03077106736600399,0.0345069095492363,0.03723504766821861,0.031561288982629776,0.029440341517329216,0.031471479684114456,0.025442512705922127,0.022129405289888382,0.020466597750782967,0.018951591104269028,0.01940358802676201,0.027288060635328293,0.02319151721894741,0.01971329003572464,0.01824268139898777,0.017367864027619362,0.026609191671013832,0.024178430438041687,0.021015115082263947,0.021162722259759903,0.024645505473017693,0.02118699997663498,0.01946117728948593,0.01783043146133423,0.0198174100369215,0.01986183226108551,0.02234555222094059,0.020092206075787544,0.018461646512150764,0.017115700989961624,0.016141816973686218,0.01498357206583023,0.013907351531088352,0.01519591361284256,0.01921425200998783,0.01739526353776455,0.018611149862408638,0.017097994685173035,0.015915347263216972,0.015158594585955143,0.017957085743546486,0.019937345758080482,0.017655856907367706,0.016410809010267258,0.014410402625799179,0.016290755942463875,0.01754673756659031,0.016766726970672607,0.015836799517273903,0.018244892358779907,0.017834125086665154,0.01635590009391308,0.015616782009601593,0.014946323819458485,0.014372987672686577,0.013800464570522308,0.01328863576054573,0.0128709701821208,0.012434832751750946,0.011955756694078445,0.01270234864205122,0.012108041904866695,0.01173047348856926,0.014763466082513332,0.01591430976986885,0.015705905854701996,0.014493311755359173,0.01367124356329441,0.013008832931518555,0.012334444560110569,0.011604120954871178,0.014918262138962746,0.016433771699666977,0.014942928217351437,0.014061624184250832,0.013395985588431358,0.012776225805282593,0.012278187088668346,0.011946776881814003,0.011685793288052082,0.011142483912408352,0.01100293267518282,0.012077036313712597,0.011962417513132095,0.017478318884968758,0.015686651691794395,0.014142491854727268,0.013269846327602863,0.01277694571763277,0.01236308179795742,0.011973129585385323,0.01146672759205103,0.010628039948642254,0.010396791622042656,0.012510688044130802,0.01189009565860033,0.011880163103342056,
mse,0.03109910897910595,0.001314498600549996,0.00038855132879689336,0.00030128657817840576,0.0003609642735682428,0.00039811208262108266,0.0002945854212157428,0.00025210657622665167,0.00028854337870143354,0.00018703966634348035,0.00014535266382154077,0.0001241694699274376,0.00010683057917049155,0.00011532059579622,0.00021579666645266116,0.00015785825962666422,0.00011648821964627132,0.0001083116585505195,9.583528299117461e-05,0.00020808438421227038,0.00016428423987235874,0.00012857817637268454,0.00013008904352318496,0.00017126450256910175,0.0001304628822254017,0.00011002077371813357,9.391883213538677e-05,0.00011576656106626615,0.00011283039202680811,0.0001433155994163826,0.00011553279182408005,9.847006731433794e-05,8.508007158525288e-05,7.723858288954943e-05,6.894054968142882e-05,5.983253868180327e-05,7.078019552864134e-05,0.00010404238128103316,8.865080599207431e-05,9.648784907767549e-05,8.283934585051611e-05,7.271388312801719e-05,6.663790554739535e-05,9.246823174180463e-05,0.00011027180880773813,9.017188131110743e-05,8.010208694031462e-05,6.440987635869533e-05,8.038440137170255e-05,8.881052053766325e-05,8.288588287541643e-05,7.461797213181853e-05,9.155394218396395e-05,8.812969463178888e-05,7.582096441183239e-05,6.90867964294739e-05,6.395569653250277e-05,5.933101419941522e-05,5.576599869527854e-05,5.252407936495729e-05,4.902294313069433e-05,4.558148793876171e-05,4.284494571038522e-05,5.240313112153672e-05,4.759483999805525e-05,4.380692553240806e-05,6.457568815676495e-05,7.115471817087382e-05,6.925666093593463e-05,5.9534031606744975e-05,5.360365685191937e-05,4.917916885460727e-05,4.5685694203712046e-05,4.12407644034829e-05,6.378869875334203e-05,7.440846820827574e-05,6.346165901049972e-05,5.6576427596155554e-05,5.229248199611902e-05,4.77139838039875e-05,4.482734584598802e-05,4.2283896618755534e-05,4.009984331787564e-05,3.6248973628971726e-05,3.561317134881392e-05,4.441751298145391e-05,4.488174818106927e-05,8.420035010203719e-05,6.952646799618378e-05,5.645168130286038e-05,4.9623544327914715e-05,4.66708697786089e-05,4.379925667308271e-05,4.162567711318843e-05,3.8768888771301135e-05,3.4309065085835755e-05,3.3097818231908605e-05,4.564442133414559e-05,4.3001313315471634e-05,4.4271953811403364e-05,
mae,0.1190536692738533,0.02608262002468109,0.014496564865112305,0.013101908378303051,0.01464880257844925,0.01574777066707611,0.013269648887217045,0.012387740425765514,0.013289705850183964,0.010720343329012394,0.009160846471786499,0.008568779565393925,0.008026011288166046,0.00823185220360756,0.01174487080425024,0.009939158335328102,0.00833352841436863,0.007768706884235144,0.007409075275063515,0.01161614153534174,0.010176708921790123,0.008918148465454578,0.00898626446723938,0.010235153138637543,0.009046612307429314,0.008394881151616573,0.0075544025748968124,0.00839474517852068,0.008463949896395206,0.009471826255321503,0.008426320739090443,0.00789127591997385,0.0073860944248735905,0.006957238540053368,0.006298282649368048,0.005807237233966589,0.006459016352891922,0.008229798637330532,0.007368134800344706,0.007865997962653637,0.007299469783902168,0.006947079207748175,0.006561377551406622,0.007554583717137575,0.008415335789322853,0.007210839539766312,0.006689069326967001,0.006136696320027113,0.006931604351848364,0.007111439015716314,0.006855635438114405,0.006625727284699678,0.007699003908783197,0.0075398157350718975,0.007142592687159777,0.006824331823736429,0.006499136798083782,0.0061910259537398815,0.005901343654841185,0.0057011498138308525,0.00555775361135602,0.005378632340580225,0.005154654383659363,0.005375249776989222,0.0051406146958470345,0.004953877069056034,0.006163371726870537,0.00676195602864027,0.006642795633524656,0.0062026772648096085,0.005874912720173597,0.00556580163538456,0.005233740899711847,0.00490214629098773,0.006297433283179998,0.0068292440846562386,0.0061596292071044445,0.005881930701434612,0.00570363225415349,0.005450617056339979,0.005241112317889929,0.005098016932606697,0.004954921081662178,0.0046698241494596004,0.004653592128306627,0.005162969697266817,0.005058483686298132,0.007237257435917854,0.006449778564274311,0.0061456747353076935,0.005826273001730442,0.005613604560494423,0.005457213614135981,0.005269965622574091,0.004929181654006243,0.0045661600306630135,0.004430378787219524,0.005335209425538778,0.005004453472793102,0.0049980334006249905,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 18595     
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 18596     
=================================================================
Total params: 37,191
Trainable params: 37,191
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 32)                160       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 512)               16896     
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 1539      
=================================================================
Total params: 18,595
Trainable params: 18,595
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 512)               2048      
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 32)                16416     
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 132       
=================================================================
Total params: 18,596
Trainable params: 18,596
Non-trainable params: 0
_________________________________________________________________
