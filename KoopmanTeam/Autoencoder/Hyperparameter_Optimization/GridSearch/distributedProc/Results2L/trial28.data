2021-06-26
loss,0.30988219380378723,0.11665156483650208,0.05526113882660866,0.05276119336485863,0.03773268684744835,0.037416502833366394,0.03638279438018799,0.03327269107103348,0.028839511796832085,0.027076825499534607,0.03288663923740387,0.028475409373641014,0.025844762101769447,0.02664772793650627,0.031639307737350464,0.027960309758782387,0.02544957958161831,0.025461824610829353,0.023089203983545303,0.029020605608820915,0.028247427195310593,0.02406783029437065,0.023840472102165222,0.028339222073554993,0.021692022681236267,0.019076623022556305,0.017659520730376244,0.023728497326374054,0.0248511154204607,0.0183112695813179,0.02666407823562622,0.024096446111798286,0.023802408948540688,0.021260196343064308,0.01891176775097847,0.021409237757325172,0.023918749764561653,0.023659853264689445,0.018415920436382294,0.0181147288531065,0.02334415726363659,0.020847808569669724,0.022687971591949463,0.01894422248005867,0.016508737578988075,0.016271010041236877,0.021602578461170197,0.02116677351295948,0.018884949386119843,0.018393803387880325,0.019836142659187317,0.01953149400651455,0.018108509480953217,0.017073702067136765,0.015206354670226574,0.013390772044658661,0.019053228199481964,0.019206911325454712,0.017245132476091385,0.016147492453455925,0.015099452808499336,0.014180522412061691,0.013789845630526543,0.017787078395485878,0.01989697851240635,0.01767588220536709,0.017488926649093628,0.02026478201150894,0.018667133525013924,0.017221946269273758,0.016056867316365242,0.014267316088080406,0.013067723251879215,0.01243404671549797,0.013725876808166504,0.017096305266022682,0.015816323459148407,0.017273902893066406,0.016775501891970634,0.015482909977436066,0.014814592897891998,0.015236075036227703,0.015737542882561684,0.014714530669152737,0.0161784365773201,0.017054682597517967,0.016706574708223343,0.015763981267809868,0.015089357271790504,0.01446531806141138,0.013962287455797195,0.013282423838973045,0.012509715743362904,0.0124817481264472,0.013869188725948334,0.015468168072402477,0.014779746532440186,0.013548234477639198,0.012526142410933971,0.012839503586292267,
mse,0.03600205108523369,0.005227599758654833,0.0009877353440970182,0.0008640255546197295,0.00042278069304302335,0.0004108638968318701,0.00038727675564587116,0.0003143184876535088,0.00024250229762401432,0.00021532805112656206,0.0003061743045691401,0.0002405561099294573,0.00020171920186839998,0.0002036722144111991,0.00028855158598162234,0.00023751839762553573,0.00018813041970133781,0.00018185410590376705,0.00015514681581407785,0.0002488229365553707,0.00023240875452756882,0.00016716083337087184,0.00016969507851172239,0.00022164327674545348,0.0001441940839868039,0.00011346499377395958,9.443843009648845e-05,0.00016217718075495213,0.00017435598419979215,0.00010365512571297586,0.00019762055308092386,0.00016845106438267976,0.0001605477009434253,0.0001315001427428797,0.00010591762838885188,0.00013261438289191574,0.00015905097825452685,0.00015641507343389094,0.00010276563261868432,9.959809540305287e-05,0.00015090852684807032,0.0001239083503605798,0.0001410620752722025,0.00010582908726064488,8.47156552481465e-05,8.019056258490309e-05,0.0001336139248451218,0.00012280832743272185,0.00010423331696074456,9.766053699422628e-05,0.00010879623732762411,0.00010632829798851162,9.400545968674123e-05,8.528280886821449e-05,7.26361686247401e-05,5.505468288902193e-05,0.00010256368841510266,0.0001004521909635514,8.355083991773427e-05,7.44869394111447e-05,6.701116217300296e-05,5.9725371102103963e-05,5.752518700319342e-05,9.132984996540472e-05,0.00010772887617349625,8.866834104992449e-05,9.042638703249395e-05,0.00011364549573045224,9.932682587532327e-05,8.674369018990546e-05,7.598470256198198e-05,6.114670395618305e-05,5.183101529837586e-05,4.731333319796249e-05,5.8301768149249256e-05,8.161173172993585e-05,7.232584903249517e-05,8.256777800852433e-05,7.711645594099537e-05,6.931970710866153e-05,6.60471705486998e-05,6.761595432180911e-05,6.807078170822933e-05,6.212313746800646e-05,7.339967123698443e-05,8.078701648628339e-05,7.65179138397798e-05,7.017477037152275e-05,6.473116809502244e-05,6.008769923937507e-05,5.643376789521426e-05,5.0787395593943074e-05,4.649030597647652e-05,4.878412073594518e-05,5.739777043345384e-05,6.650500290561467e-05,6.197464244905859e-05,5.1246526709292084e-05,4.624255598173477e-05,4.9220427172258496e-05,
mae,0.12768062949180603,0.05197710916399956,0.023817487061023712,0.022803977131843567,0.015959106385707855,0.01583881676197052,0.015344767831265926,0.014152909629046917,0.01229141466319561,0.011486947536468506,0.013831695541739464,0.01219224650412798,0.011057245545089245,0.01137353852391243,0.013574080541729927,0.011818983592092991,0.010877626948058605,0.011057143099606037,0.00975330825895071,0.012508239597082138,0.012039215303957462,0.010268906131386757,0.009991869330406189,0.012127859517931938,0.00915449671447277,0.008163257502019405,0.007443706505000591,0.010102355852723122,0.01050140056759119,0.0077687217853963375,0.011444815434515476,0.010169374756515026,0.010107593610882759,0.008966125547885895,0.007980789057910442,0.008981477469205856,0.010272916406393051,0.010259377770125866,0.007862572558224201,0.007703667972236872,0.009978153742849827,0.008983499370515347,0.009619190357625484,0.007988466881215572,0.006995692383497953,0.006903693545609713,0.009002038277685642,0.008884672075510025,0.007960100658237934,0.007805558852851391,0.008423859253525734,0.008397535420954227,0.007818845100700855,0.007300478871911764,0.00651485426351428,0.005721032619476318,0.008059647865593433,0.008113496005535126,0.007372571621090174,0.006800621747970581,0.006305524613708258,0.005938793998211622,0.005917849484831095,0.007617260329425335,0.008593729697167873,0.007629002910107374,0.007369017228484154,0.00843765214085579,0.0077878995798528194,0.007263381499797106,0.006882531568408012,0.006039225962013006,0.005545697174966335,0.005302050616592169,0.005908197723329067,0.007283467799425125,0.00662669911980629,0.007343777921050787,0.007118218578398228,0.00655327970162034,0.006211912725120783,0.006500771269202232,0.006820484530180693,0.0061899409629404545,0.006583830341696739,0.0073570203967392445,0.007172412239015102,0.006766824517399073,0.0064959400333464146,0.00619308277964592,0.005907826591283083,0.005675566848367453,0.005350872874259949,0.005314674228429794,0.005873886402696371,0.006562846712768078,0.006306908093392849,0.005893700290471315,0.005405687261372805,0.005497227422893047,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 34435     
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 34436     
=================================================================
Total params: 68,871
Trainable params: 68,871
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 128)               640       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 256)               33024     
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 771       
=================================================================
Total params: 34,435
Trainable params: 34,435
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 256)               1024      
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 128)               32896     
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 516       
=================================================================
Total params: 34,436
Trainable params: 34,436
Non-trainable params: 0
_________________________________________________________________
