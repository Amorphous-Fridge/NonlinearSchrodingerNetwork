2021-06-26
loss,0.5654087066650391,0.14030876755714417,0.067143015563488,0.047102198004722595,0.03837849944829941,0.03443881869316101,0.03155262768268585,0.029623467475175858,0.027927933260798454,0.026845533400774002,0.025603050366044044,0.024810753762722015,0.024271506816148758,0.02368759550154209,0.023068763315677643,0.02250986360013485,0.022171758115291595,0.021628592163324356,0.02147061750292778,0.021041443571448326,0.0207085981965065,0.020316513255238533,0.020214350894093513,0.0199997927993536,0.01962103322148323,0.019380582496523857,0.01933181658387184,0.019141249358654022,0.018818579614162445,0.018685942515730858,0.01858251355588436,0.018387991935014725,0.01826312206685543,0.01807842217385769,0.017936956137418747,0.017688071355223656,0.017772221937775612,0.017541470006108284,0.01727721281349659,0.017280900850892067,0.01713591068983078,0.017048407346010208,0.016929693520069122,0.016803253442049026,0.016788100823760033,0.01664075255393982,0.016496798023581505,0.016485650092363358,0.016361527144908905,0.01622331701219082,0.016118887811899185,0.016081633046269417,0.01601581461727619,0.015955494716763496,0.015813099220395088,0.015716711059212685,0.01566954329609871,0.015739303082227707,0.015516001731157303,0.015485817566514015,0.01538083702325821,0.01529699843376875,0.01521204598248005,0.01511325966566801,0.015135424211621284,0.015127891674637794,0.015056141652166843,0.0149548863992095,0.014959701336920261,0.01484981644898653,0.014672892168164253,0.014741315506398678,0.01477779634296894,0.014615955762565136,0.014656148850917816,0.014553943648934364,0.014508799649775028,0.014510862529277802,0.014434264041483402,0.01432364247739315,0.014286530204117298,0.014313402585685253,0.014229584485292435,0.014225013554096222,0.01421357411891222,0.01415841095149517,0.014062932692468166,0.014094547368586063,0.014036877080798149,0.013903073966503143,0.013920935802161694,0.013839849270880222,0.013809701427817345,0.01372614223510027,0.0137354526668787,0.013665965758264065,0.013608835637569427,0.01371926162391901,0.01368868350982666,0.013602595776319504,
mse,0.15460868179798126,0.006988690700381994,0.0015695984475314617,0.0007845431100577116,0.0005111950449645519,0.0004017243627458811,0.0003331113839522004,0.0002880064712371677,0.00025447740335948765,0.0002317958715138957,0.00021002233552280813,0.0001961386005859822,0.00018642637587618083,0.0001763088075676933,0.00016661881818436086,0.0001587714214110747,0.00015282710955943912,0.00014489520981442183,0.00014234418631531298,0.0001364943163935095,0.00013202620903030038,0.00012691861775238067,0.0001246984611498192,0.00012208116822876036,0.00011698019079631194,0.00011387348786229268,0.00011304294457659125,0.00011096912203356624,0.00010651005140971392,0.00010522865341044962,0.00010392528201919049,0.0001016139576677233,9.986005898099393e-05,9.770521864993498e-05,9.640481584938243e-05,9.375415538670495e-05,9.446139301871881e-05,9.165194205706939e-05,8.894688653526828e-05,8.874196646502241e-05,8.741443161852658e-05,8.609178621554747e-05,8.494222856825218e-05,8.342359797097743e-05,8.326696115545928e-05,8.154997340170667e-05,8.085639274213463e-05,7.984097464941442e-05,7.853926945244893e-05,7.738397107459605e-05,7.626413571415469e-05,7.602509140269831e-05,7.533881580457091e-05,7.445007213391364e-05,7.323222962440923e-05,7.260341226356104e-05,7.172883488237858e-05,7.232947973534465e-05,7.054160232655704e-05,7.00589080224745e-05,6.914546247571707e-05,6.847999611636624e-05,6.817741086706519e-05,6.702252721879631e-05,6.698937795590609e-05,6.648404814768583e-05,6.585949449799955e-05,6.505186320282519e-05,6.497892900370061e-05,6.427466723835096e-05,6.325433059828356e-05,6.336069054668769e-05,6.343371933326125e-05,6.218424823600799e-05,6.220852810656652e-05,6.144311919342726e-05,6.108660454628989e-05,6.105696957092732e-05,6.028894495102577e-05,5.966256867395714e-05,5.923421122133732e-05,5.92479482293129e-05,5.8572652051225305e-05,5.8597834140527993e-05,5.833701288793236e-05,5.787805639556609e-05,5.7257242588093504e-05,5.743479778175242e-05,5.6998367654159665e-05,5.609437721432187e-05,5.6021377531578764e-05,5.545790190808475e-05,5.5212505685631186e-05,5.4477863159263507e-05,5.449833406601101e-05,5.433348997030407e-05,5.389069701777771e-05,5.4153031669557095e-05,5.390275327954441e-05,5.3284904424799606e-05,
mae,0.23499903082847595,0.05894109234213829,0.028627462685108185,0.020014602690935135,0.01633215881884098,0.014698084443807602,0.013467849232256413,0.012664461508393288,0.011949203908443451,0.01149426307529211,0.010970660485327244,0.010644822381436825,0.010420642793178558,0.010179419070482254,0.009888729080557823,0.00967977475374937,0.009513248689472675,0.009297559969127178,0.009223301894962788,0.009015236981213093,0.008906139992177486,0.008717019110918045,0.008684614673256874,0.008600796572864056,0.008429476991295815,0.008319679647684097,0.008313644677400589,0.0082367779687047,0.00807138066738844,0.008010022342205048,0.00796955730766058,0.00791013240814209,0.007849905639886856,0.007746458053588867,0.007705283351242542,0.007573714945465326,0.0076324534602463245,0.007520902901887894,0.0074119772762060165,0.007406932767480612,0.007362850941717625,0.007318473886698484,0.007244037929922342,0.007224998902529478,0.0072381566278636456,0.007128534838557243,0.0070864069275557995,0.0070748599246144295,0.007013898342847824,0.006987841799855232,0.006916011683642864,0.006898047868162394,0.0068780058063566685,0.006854127626866102,0.006769919767975807,0.006752554327249527,0.006716690491884947,0.006751030683517456,0.006663615815341473,0.006618257146328688,0.006552567705512047,0.006529604084789753,0.006518600042909384,0.006463513243943453,0.0064772083424031734,0.006461312994360924,0.006505640223622322,0.0064225816167891026,0.006423907354474068,0.0063676293939352036,0.006267262157052755,0.0063120340928435326,0.006325064226984978,0.0062781330198049545,0.006266832817345858,0.006227274425327778,0.006232059560716152,0.006219998002052307,0.006182983051985502,0.0061170794069767,0.006137065123766661,0.006097086705267429,0.006080097984522581,0.006084749009460211,0.0060973805375397205,0.00605736905708909,0.006011997349560261,0.006002057809382677,0.00598879111930728,0.005954216234385967,0.00596782099455595,0.005967306904494762,0.005944077391177416,0.005888744723051786,0.0058394670486450195,0.00587861193343997,0.005803351290524006,0.005879390984773636,0.0058685047551989555,0.00580727681517601,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 1963      
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 1964      
=================================================================
Total params: 3,927
Trainable params: 3,927
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 8)                 40        
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 32)                288       
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 32)                1056      
_________________________________________________________________
encoding_layer_4 (Dense)     (None, 16)                528       
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 51        
=================================================================
Total params: 1,963
Trainable params: 1,963
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 16)                64        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 32)                544       
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 32)                1056      
_________________________________________________________________
decoding_layer_4 (Dense)     (None, 8)                 264       
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 36        
=================================================================
Total params: 1,964
Trainable params: 1,964
Non-trainable params: 0
_________________________________________________________________
