2021-06-26
loss,1.8827100992202759,0.4904458224773407,0.4550135135650635,0.44705620408058167,0.45020294189453125,0.45496276021003723,0.4496740400791168,0.4479457437992096,0.4518093168735504,0.4493872821331024,0.450565904378891,0.45108848810195923,0.45222681760787964,0.4488891363143921,0.44990435242652893,0.44967055320739746,0.44885262846946716,0.4497385025024414,0.4480268061161041,0.4474649429321289,0.44636353850364685,0.4462907314300537,0.44777995347976685,0.44688498973846436,0.4465649425983429,0.4487650692462921,0.44671058654785156,0.446259081363678,0.44625815749168396,0.44604960083961487,0.4460281729698181,0.4466644525527954,0.4467660188674927,0.44739657640457153,0.4460563659667969,0.44521549344062805,0.4460187256336212,0.4454406797885895,0.44542554020881653,0.446065753698349,0.4465675354003906,0.44641193747520447,0.4460172653198242,0.4451177716255188,0.44554242491722107,0.44663116335868835,0.44516316056251526,0.44531893730163574,0.44546982645988464,0.4454638659954071,0.4449107050895691,0.4450532793998718,0.44621241092681885,0.44560858607292175,0.4445962905883789,0.44584155082702637,0.44644850492477417,0.44461390376091003,0.4450145661830902,0.4452764093875885,0.4456556737422943,0.4446340501308441,0.44555583596229553,0.4449803829193115,0.4455503523349762,0.4447559118270874,0.4447718858718872,0.44450995326042175,0.44491612911224365,0.4450557231903076,0.4445699453353882,0.44472214579582214,0.44444411993026733,0.4446401596069336,0.4456233084201813,0.44551894068717957,0.4447486400604248,0.444841206073761,0.4448527991771698,0.4456113576889038,0.8485516905784607,0.4475601613521576,0.44440189003944397,0.44525155425071716,0.44506171345710754,0.4449882209300995,0.44404298067092896,0.4448292553424835,0.4448557198047638,0.4449453055858612,0.4448092579841614,0.44507381319999695,0.445048987865448,0.4445337653160095,0.4451034665107727,0.44429919123649597,0.4444410800933838,0.4445568919181824,0.44444119930267334,0.4450387954711914,
mse,1.3858542442321777,0.06832996010780334,0.05785539746284485,0.05571256950497627,0.05638404190540314,0.05748914182186127,0.05631326511502266,0.055904146283864975,0.05683043599128723,0.05628325417637825,0.0564611479640007,0.056680548936128616,0.057107605040073395,0.05610751733183861,0.056393906474113464,0.0563959963619709,0.05617064610123634,0.05644060671329498,0.05596787855029106,0.05582441762089729,0.05552852526307106,0.055490847676992416,0.05589368939399719,0.055590372532606125,0.05561770498752594,0.05612337216734886,0.05561692267656326,0.055496059358119965,0.055546145886182785,0.05546078830957413,0.055458251386880875,0.05558517575263977,0.05560367554426193,0.055749669671058655,0.055455103516578674,0.05525532364845276,0.05547694116830826,0.05529211834073067,0.05530570074915886,0.05545419827103615,0.05553564429283142,0.055545296519994736,0.05542527511715889,0.05521867796778679,0.05533552169799805,0.055566683411598206,0.05520837381482124,0.055293843150138855,0.05528343841433525,0.05530938506126404,0.0551723875105381,0.05518011748790741,0.05547182634472847,0.05529512092471123,0.05511702224612236,0.05539032816886902,0.055527325719594955,0.05509283021092415,0.05518776923418045,0.05524374544620514,0.055333711206912994,0.05508461594581604,0.055300235748291016,0.055187638849020004,0.05529598891735077,0.0551404133439064,0.05513565614819527,0.05506158247590065,0.05514176934957504,0.05516987666487694,0.05506132170557976,0.0551324225962162,0.0550379604101181,0.055126987397670746,0.05530364066362381,0.05531183257699013,0.05511130765080452,0.0551445297896862,0.05514788255095482,0.05531398952007294,0.24826988577842712,0.055822111666202545,0.05504033714532852,0.05521903932094574,0.05517882853746414,0.05518088862299919,0.05497840791940689,0.055163491517305374,0.05512947589159012,0.05517397075891495,0.05513683706521988,0.055191412568092346,0.05515924468636513,0.05509300157427788,0.05520749092102051,0.055007461458444595,0.0550355538725853,0.05509006604552269,0.05506356060504913,0.05519070103764534,
mae,0.8197755217552185,0.2131074070930481,0.20106342434883118,0.1980644166469574,0.19907039403915405,0.20064182579517365,0.198911651968956,0.1982535570859909,0.1996278166770935,0.1988404244184494,0.1991998553276062,0.1994084119796753,0.200046107172966,0.19868989288806915,0.19918808341026306,0.1990683376789093,0.19888372719287872,0.199060320854187,0.1984110027551651,0.1981673687696457,0.19789230823516846,0.1977895349264145,0.19843824207782745,0.1981329470872879,0.19791412353515625,0.19873298704624176,0.19789943099021912,0.19784510135650635,0.19776605069637299,0.19762568175792694,0.19774745404720306,0.19800274074077606,0.19800853729248047,0.1983547955751419,0.19781255722045898,0.19733475148677826,0.19764089584350586,0.19758565723896027,0.19755229353904724,0.19777153432369232,0.19791358709335327,0.19795483350753784,0.19768713414669037,0.1973617672920227,0.19762633740901947,0.19812121987342834,0.19741404056549072,0.197464257478714,0.19760286808013916,0.1976141780614853,0.19737455248832703,0.1973801851272583,0.19789758324623108,0.19761979579925537,0.19712209701538086,0.19770944118499756,0.19798921048641205,0.1971815675497055,0.19737306237220764,0.19748346507549286,0.19765597581863403,0.19723671674728394,0.1976238489151001,0.19737589359283447,0.19757531583309174,0.19730156660079956,0.19729219377040863,0.19715195894241333,0.19738103449344635,0.19739709794521332,0.1971711963415146,0.19723400473594666,0.19709956645965576,0.19722123444080353,0.19767844676971436,0.19759178161621094,0.19724410772323608,0.197289377450943,0.19733886420726776,0.19748897850513458,0.3597637414932251,0.19824820756912231,0.19712939858436584,0.1975206434726715,0.19742953777313232,0.1973837912082672,0.1969376504421234,0.1973024159669876,0.1973690539598465,0.1973084658384323,0.19735045731067657,0.19748236238956451,0.19741760194301605,0.19716492295265198,0.19744443893432617,0.1970820277929306,0.19713549315929413,0.19720155000686646,0.19715876877307892,0.1973687708377838,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 362307    
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 362308    
=================================================================
Total params: 724,615
Trainable params: 724,615
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 64)                320       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 512)               33280     
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 512)               262656    
_________________________________________________________________
encoding_layer_4 (Dense)     (None, 128)               65664     
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 387       
=================================================================
Total params: 362,307
Trainable params: 362,307
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 128)               512       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 512)               66048     
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 512)               262656    
_________________________________________________________________
decoding_layer_4 (Dense)     (None, 64)                32832     
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 260       
=================================================================
Total params: 362,308
Trainable params: 362,308
Non-trainable params: 0
_________________________________________________________________
