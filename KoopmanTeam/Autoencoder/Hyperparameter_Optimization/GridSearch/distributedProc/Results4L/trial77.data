2021-06-26
loss,1.5691313743591309,0.5038708448410034,0.4498313069343567,0.45024776458740234,0.4511635899543762,0.4523569345474243,0.4487902820110321,0.4507192373275757,0.45426785945892334,0.44724801182746887,0.4459814727306366,0.44761744141578674,0.4481850862503052,0.4470946788787842,0.4468718469142914,0.4486580789089203,0.44882088899612427,0.4484400451183319,0.44895556569099426,0.4464409649372101,0.44647854566574097,0.44765281677246094,0.4474882185459137,0.44763457775115967,0.4461880624294281,0.44759601354599,0.44529467821121216,0.44683772325515747,0.4468119144439697,0.44687825441360474,0.4459507465362549,0.44686663150787354,0.44685372710227966,0.44562050700187683,0.4457469582557678,0.4459119141101837,0.4460670053958893,0.4450784921646118,0.44525396823883057,0.44566676020622253,0.44505855441093445,0.44526389241218567,0.4450443685054779,0.44508156180381775,0.4453457295894623,0.4455699026584625,0.44632062315940857,0.4455096423625946,0.4454672634601593,0.4455603361129761,0.4452363848686218,0.4456896483898163,0.44542303681373596,0.44514405727386475,0.4449935257434845,0.4447536766529083,0.44508248567581177,0.44498685002326965,0.44546905159950256,0.4453643560409546,0.4449239671230316,0.4445846676826477,0.44429928064346313,0.4451156258583069,0.4443647861480713,0.4447067677974701,0.4447135925292969,0.44477397203445435,0.445409893989563,0.4453487694263458,0.44574570655822754,0.4442446827888489,0.4445750415325165,0.44498929381370544,0.4449882507324219,0.4453203082084656,0.446440190076828,0.44518211483955383,0.44476935267448425,0.44477954506874084,0.445343941450119,0.44609054923057556,0.4450298249721527,0.44532424211502075,0.4447031319141388,0.4447140395641327,0.4445827603340149,0.44456300139427185,0.4454648196697235,0.44545429944992065,0.44489291310310364,0.445427805185318,1.513052225112915,4.495563507080078,4.424158096313477,4.424313545227051,4.424128532409668,4.424067497253418,4.424190521240234,4.424088478088379,
mse,1.0135623216629028,0.0719524696469307,0.056322552263736725,0.05640903115272522,0.05671601742506027,0.05698314681649208,0.05612486973404884,0.056538697332143784,0.05745834857225418,0.05574546381831169,0.055487629026174545,0.055832695215940475,0.055942751467227936,0.05570968613028526,0.05568798631429672,0.05604706332087517,0.056135259568691254,0.05606717988848686,0.05619816854596138,0.05556395649909973,0.055570852011442184,0.055846743285655975,0.055838827043771744,0.055858563631772995,0.05547311156988144,0.05585850402712822,0.055264830589294434,0.055628061294555664,0.05563843250274658,0.05566643178462982,0.05539236590266228,0.05568724125623703,0.05564673990011215,0.05534270405769348,0.055383600294589996,0.055384472012519836,0.055474910885095596,0.05518915504217148,0.055248893797397614,0.05534976348280907,0.05516991764307022,0.05524444952607155,0.05520950257778168,0.055207494646310806,0.05524788796901703,0.05532757192850113,0.055484820157289505,0.05532506853342056,0.0553261898458004,0.055310286581516266,0.05525146424770355,0.05536290630698204,0.05530077964067459,0.05524744838476181,0.055197544395923615,0.05509477108716965,0.0551828108727932,0.05519267916679382,0.05529028922319412,0.05524880811572075,0.055159807205200195,0.05506040155887604,0.05503416061401367,0.055201902985572815,0.05502047389745712,0.05512780323624611,0.055090274661779404,0.055120471864938736,0.05525854229927063,0.05525921285152435,0.055346764624118805,0.05500868707895279,0.055061545222997665,0.055140696465969086,0.05517297238111496,0.05526341497898102,0.05553419515490532,0.05523807182908058,0.05513007566332817,0.05515247955918312,0.055290788412094116,0.0554598905146122,0.055222030729055405,0.055247046053409576,0.055113907903432846,0.05510910972952843,0.05509629100561142,0.05504435673356056,0.05528313294053078,0.05525655671954155,0.05515226349234581,0.05527350306510925,1.240384817123413,5.441781520843506,4.894186019897461,4.894536972045898,4.894122123718262,4.893993854522705,4.894258499145508,4.894038200378418,
mae,0.7015827298164368,0.21869361400604248,0.1989583522081375,0.19903789460659027,0.19939763844013214,0.19996201992034912,0.19866621494293213,0.19933830201625824,0.20043808221817017,0.19811880588531494,0.19765914976596832,0.19835299253463745,0.19852134585380554,0.198047935962677,0.19798675179481506,0.19860391318798065,0.198756605386734,0.19868062436580658,0.19885027408599854,0.19784966111183167,0.19784028828144073,0.19838359951972961,0.19829027354717255,0.19838592410087585,0.19773530960083008,0.19834628701210022,0.19749432802200317,0.1980617493391037,0.19804346561431885,0.19796979427337646,0.19783712923526764,0.19806301593780518,0.1980365812778473,0.19750840961933136,0.19763782620429993,0.19771505892276764,0.19776968657970428,0.1973402351140976,0.19749028980731964,0.19751006364822388,0.1973433643579483,0.19749735295772552,0.19738851487636566,0.19738450646400452,0.19752293825149536,0.1975298374891281,0.19793245196342468,0.19755662977695465,0.1975870132446289,0.19758403301239014,0.1974191814661026,0.19768252968788147,0.19758573174476624,0.1974286139011383,0.19732923805713654,0.1972307413816452,0.19748422503471375,0.19734206795692444,0.19758455455303192,0.1975594311952591,0.19733700156211853,0.19720622897148132,0.19707034528255463,0.19740375876426697,0.19711826741695404,0.1973547339439392,0.1972784847021103,0.19729158282279968,0.1974390596151352,0.19748908281326294,0.19774574041366577,0.1970430314540863,0.1972249448299408,0.19736215472221375,0.19735205173492432,0.1974734663963318,0.19794000685214996,0.1974480003118515,0.19732913374900818,0.19731059670448303,0.197603240609169,0.19782760739326477,0.1973951756954193,0.19761468470096588,0.19728456437587738,0.1972731649875641,0.19720321893692017,0.19719864428043365,0.1975790411233902,0.19763381779193878,0.19735409319400787,0.19753451645374298,0.6653558611869812,2.1727418899536133,2.1998472213745117,2.199946641921997,2.1998291015625,2.1997926235198975,2.1998682022094727,2.19980525970459,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 395395    
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 395396    
=================================================================
Total params: 790,791
Trainable params: 790,791
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 128)               640       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 512)               66048     
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 512)               262656    
_________________________________________________________________
encoding_layer_4 (Dense)     (None, 128)               65664     
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 387       
=================================================================
Total params: 395,395
Trainable params: 395,395
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 128)               512       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 512)               66048     
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 512)               262656    
_________________________________________________________________
decoding_layer_4 (Dense)     (None, 128)               65664     
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 516       
=================================================================
Total params: 395,396
Trainable params: 395,396
Non-trainable params: 0
_________________________________________________________________
