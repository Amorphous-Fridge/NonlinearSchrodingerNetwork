2021-06-26
loss,0.5593843460083008,0.10248379409313202,0.0704193040728569,0.0806347206234932,0.058160871267318726,0.060875363647937775,0.05106614902615547,0.04833553358912468,0.04284808784723282,0.043198294937610626,0.03580781817436218,0.04231277108192444,0.04194718226790428,0.0397154800593853,0.034235455095767975,0.03088628128170967,0.03364891931414604,0.03460665047168732,0.03258630633354187,0.03255438432097435,0.031132657080888748,0.0288435909897089,0.02765728160738945,0.02654171548783779,0.027015961706638336,0.028123559430241585,0.026578130200505257,0.027075419202446938,0.024859284982085228,0.02159884013235569,0.020660309121012688,0.02187306433916092,0.02577831968665123,0.023138312622904778,0.022668523713946342,0.01973774842917919,0.020399687811732292,0.023117678239941597,0.02026979625225067,0.019779736176133156,0.020267657935619354,0.020322728902101517,0.018878113478422165,0.019039390608668327,0.018723858520388603,0.019900916144251823,0.018540741875767708,0.01930399425327778,0.01802004873752594,0.017658794298768044,0.019195793196558952,0.01924605667591095,0.018474156036973,0.017544908449053764,0.017112942412495613,0.01852743700146675,0.017892155796289444,0.019015992060303688,0.017664439976215363,0.017650211229920387,0.01577596366405487,0.016294319182634354,0.01653645560145378,0.01701292209327221,0.01630074344575405,0.017329132184386253,0.016807835549116135,0.016514455899596214,0.015693075954914093,0.015746820718050003,0.016483092680573463,0.01508951187133789,0.01491172518581152,0.015920022502541542,0.016068655997514725,0.015274528414011002,0.015245342627167702,0.01585131138563156,0.014029765501618385,0.014518480747938156,0.013436325825750828,0.014515537768602371,0.014049413613975048,0.015985898673534393,0.014311072416603565,0.014732398092746735,0.0151676582172513,0.014201117679476738,0.012057621031999588,0.013510449789464474,0.01394248753786087,0.014072487130761147,0.014780384488403797,0.015415739268064499,0.014449064619839191,0.014199825935065746,0.013123227283358574,0.014927179552614689,0.014345938339829445,0.014891236089169979,
mse,0.19276423752307892,0.003393395571038127,0.0014730044640600681,0.0019947881810367107,0.001016794703900814,0.0011105951853096485,0.0007654443616047502,0.0006959747988730669,0.0005432292236946523,0.0005534134688787162,0.0003844652383122593,0.000526592368260026,0.000532530655618757,0.00046625841059722006,0.00035025441320613027,0.0002950450871139765,0.0003462368331383914,0.00035713642137125134,0.00031328716431744397,0.00031340718851424754,0.00029468649881891906,0.0002519959525670856,0.0002265219809487462,0.00020740901527460665,0.00022204936249181628,0.00023460642842110246,0.00021000223932787776,0.0002113114605890587,0.00018034195818472654,0.00014176177501212806,0.00013195592327974737,0.0001435148878954351,0.00019105292449239641,0.0001552264584461227,0.000152435532072559,0.0001168822927866131,0.0001247533509740606,0.0001515284093329683,0.00011955713125644252,0.00011660029122140259,0.00012102824985049665,0.00012164184590801597,0.00010399942402727902,0.00010607454896671697,0.00010291142098139971,0.00011404308315832168,0.00010140699305338785,0.00010766982450149953,9.903219324769452e-05,9.243856038665399e-05,0.0001026000827550888,0.00010534136526985094,9.892981324810535e-05,9.075835259864107e-05,8.764842641539872e-05,9.889098873827606e-05,9.248132118955255e-05,0.00010203802230535075,9.107522055273876e-05,8.90344672370702e-05,7.525764522142708e-05,7.85159136285074e-05,8.127315231831744e-05,8.326376701006666e-05,7.75845255702734e-05,8.717509626876563e-05,7.957420166349038e-05,7.770016236463562e-05,7.209434261312708e-05,7.217725942609832e-05,8.057113154791296e-05,6.672610470559448e-05,6.537157605635002e-05,7.07660656189546e-05,7.628549064975232e-05,6.751751789124683e-05,6.839397246949375e-05,7.257554534589872e-05,5.920324110775255e-05,6.185141683090478e-05,5.4632590035907924e-05,6.2295985117089e-05,5.876984869246371e-05,7.362571341218427e-05,6.0755024605896324e-05,6.289729935815558e-05,6.685522384941578e-05,5.9331396187189966e-05,4.438285395735875e-05,5.473878627526574e-05,5.842391692567617e-05,5.852471440448426e-05,6.272072641877457e-05,6.863489397801459e-05,6.205387762747705e-05,5.8868827181868255e-05,5.1543378503993154e-05,6.33672607364133e-05,5.9805228374898434e-05,6.38447017990984e-05,
mae,0.2407694011926651,0.04346694052219391,0.030061712488532066,0.03433850780129433,0.024653790518641472,0.02600131556391716,0.021645747125148773,0.02052120678126812,0.0181427039206028,0.018463538959622383,0.015127255581319332,0.017877627164125443,0.017859507352113724,0.01695662923157215,0.014581996016204357,0.013261259533464909,0.014439709484577179,0.014689607545733452,0.013494379818439484,0.01376662403345108,0.013348989188671112,0.011944844387471676,0.011728090234100819,0.01133666280657053,0.011460279114544392,0.011857186444103718,0.011124585755169392,0.011509841307997704,0.010511292144656181,0.009081234224140644,0.008848045021295547,0.009223690256476402,0.010815837420523167,0.009744488634169102,0.009529419243335724,0.008376686833798885,0.00868647638708353,0.009697552770376205,0.008534376509487629,0.008403902873396873,0.008627297356724739,0.00855365488678217,0.008039266802370548,0.007941544987261295,0.00791789498180151,0.008381571620702744,0.007861089892685413,0.008012405596673489,0.007543736603111029,0.007377839181572199,0.008086394518613815,0.008108476176857948,0.007832336239516735,0.007208749186247587,0.007155323401093483,0.007881847210228443,0.007508085574954748,0.008035164326429367,0.007495912257581949,0.007458462845534086,0.006688660942018032,0.006914996542036533,0.006953827105462551,0.007113778032362461,0.006910756230354309,0.007313773036003113,0.007120115682482719,0.006942129693925381,0.006593571975827217,0.006622156128287315,0.006856911815702915,0.006338236853480339,0.006308107171207666,0.006671682465821505,0.0068101114593446255,0.00641829427331686,0.006398841738700867,0.006705429404973984,0.005844193045049906,0.006035270635038614,0.005735473241657019,0.006184143479913473,0.006001555360853672,0.006805242504924536,0.006147383246570826,0.0062662893906235695,0.006359166465699673,0.006036174483597279,0.005056123714894056,0.005737725645303726,0.005843411665409803,0.005903258919715881,0.006195199675858021,0.006551225669682026,0.006105309352278709,0.006073445547372103,0.005516279488801956,0.0062836394645273685,0.006043464411050081,0.0063054501079022884,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 149155    
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 149156    
=================================================================
Total params: 298,311
Trainable params: 298,311
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 32)                160       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 512)               16896     
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 256)               131328    
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 771       
=================================================================
Total params: 149,155
Trainable params: 149,155
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 256)               1024      
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 512)               131584    
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 32)                16416     
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 132       
=================================================================
Total params: 149,156
Trainable params: 149,156
Non-trainable params: 0
_________________________________________________________________
