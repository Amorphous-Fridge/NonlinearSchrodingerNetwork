2021-06-26
loss,0.316617876291275,0.11775817722082138,0.06141360104084015,0.044318147003650665,0.03709733113646507,0.03294909745454788,0.031085235998034477,0.029179269447922707,0.027950942516326904,0.02667158655822277,0.025743527337908745,0.024775749072432518,0.023975439369678497,0.02321087382733822,0.022703934460878372,0.02212175354361534,0.02159867063164711,0.021265389397740364,0.020744474604725838,0.02042686752974987,0.01994430460035801,0.01961713284254074,0.01934920996427536,0.01893463358283043,0.018592415377497673,0.01830592192709446,0.018200740218162537,0.017810838297009468,0.01769961789250374,0.017484592273831367,0.017147785052657127,0.016936305910348892,0.016789589077234268,0.016580315306782722,0.016436267644166946,0.016339972615242004,0.016089411452412605,0.015931183472275734,0.01592346467077732,0.01564893126487732,0.015575959347188473,0.015402509830892086,0.01535082794725895,0.015223179012537003,0.015129449777305126,0.014949127100408077,0.014914152212440968,0.014744745567440987,0.014782807789742947,0.014558778144419193,0.014489417895674706,0.01443687453866005,0.014250464737415314,0.014223538339138031,0.0142204649746418,0.01406736858189106,0.013877064920961857,0.013939647935330868,0.013897690922021866,0.013698739930987358,0.013801571913063526,0.013574902899563313,0.013555661775171757,0.013419314287602901,0.013437793590128422,0.013344794511795044,0.01327357068657875,0.013209010474383831,0.013170081190764904,0.01312161609530449,0.013066012412309647,0.01290395762771368,0.012869837693870068,0.01284767035394907,0.0127891656011343,0.012789367698132992,0.012709243223071098,0.012649793177843094,0.01261539850383997,0.0125403618440032,0.012499545700848103,0.01243295893073082,0.012477499432861805,0.012396999634802341,0.012312421575188637,0.012295199558138847,0.01225623581558466,0.012212865054607391,0.011976080946624279,0.012166143395006657,0.012092089280486107,0.012020478025078773,0.01202971301972866,0.011925880797207355,0.01188605185598135,0.011924346908926964,0.011824412271380424,0.011815514415502548,0.011764171533286572,0.011665334925055504,
mse,0.035062018781900406,0.004895859397947788,0.0012530663516372442,0.0006257598870433867,0.00042361736996099353,0.0003375489905010909,0.0002933077921625227,0.00026040634838864207,0.00023487235011998564,0.00021300424123182893,0.00019807783246506006,0.00018318342335987836,0.000171214051079005,0.0001601107360329479,0.00015312952746171504,0.00014472169277723879,0.0001375883148284629,0.00013295511598698795,0.00012622910435311496,0.0001225314917974174,0.00011695174180204049,0.00011245863424846902,0.00010940866195596755,0.00010440548794576898,0.00010055417806142941,9.775834769243374e-05,9.602501086192206e-05,9.207690163748339e-05,9.077009599423036e-05,8.84321125340648e-05,8.4954452177044e-05,8.284755313070491e-05,8.203999459510669e-05,7.948090933496132e-05,7.813041156623513e-05,7.697161345276982e-05,7.520120561821386e-05,7.310305954888463e-05,7.323202589759603e-05,7.085673860274255e-05,7.008787360973656e-05,6.818971451139078e-05,6.766995647922158e-05,6.641100480919704e-05,6.550028774654493e-05,6.43857492832467e-05,6.392904469976202e-05,6.266253330977634e-05,6.296426727203652e-05,6.148364627733827e-05,6.009343996993266e-05,5.9473761211847886e-05,5.835711635882035e-05,5.806309127365239e-05,5.7871933677233756e-05,5.678594607161358e-05,5.51245357200969e-05,5.606071135844104e-05,5.565675382968038e-05,5.362338924896903e-05,5.437698564492166e-05,5.278048411128111e-05,5.2394163503777236e-05,5.15440187882632e-05,5.1644863560795784e-05,5.101388887851499e-05,5.058133683633059e-05,5.0091770390281454e-05,4.963711035088636e-05,4.9082631448982283e-05,4.8670983233023435e-05,4.75722954433877e-05,4.754580731969327e-05,4.7273551899706945e-05,4.655996963265352e-05,4.660696868086234e-05,4.625063229468651e-05,4.5934637455502525e-05,4.526237535174005e-05,4.503705713432282e-05,4.4601416448131204e-05,4.446940511115827e-05,4.4836069719167426e-05,4.399566387291998e-05,4.32730266766157e-05,4.309513678890653e-05,4.285369868739508e-05,4.2452978959772736e-05,4.172443368588574e-05,4.2260653572157025e-05,4.1738141590030864e-05,4.146658829995431e-05,4.138977237744257e-05,4.071852163178846e-05,4.0815975808072835e-05,4.043700755573809e-05,3.997602470917627e-05,4.006183735327795e-05,3.9324990211753175e-05,3.883624231093563e-05,
mae,0.13659973442554474,0.05055955424904823,0.02606176771223545,0.018842335790395737,0.015821555629372597,0.01406029611825943,0.013293956406414509,0.012493913061916828,0.011961578391492367,0.011423344723880291,0.01101683173328638,0.01061740517616272,0.010264433920383453,0.009969402104616165,0.009721131063997746,0.009487692266702652,0.009227392263710499,0.009156479500234127,0.00893393810838461,0.008731087669730186,0.008595752529799938,0.008397952653467655,0.008316203020513058,0.008182733319699764,0.00791365746408701,0.007932206615805626,0.007808221038430929,0.007640263997018337,0.007603743579238653,0.007521714549511671,0.007310766726732254,0.007284773513674736,0.00723276985809207,0.007121719885617495,0.007037490140646696,0.007059188559651375,0.006899779662489891,0.006865695584565401,0.00683200778439641,0.006749329157173634,0.006668487098067999,0.006617647595703602,0.006600331515073776,0.006516939029097557,0.006480406504124403,0.006396241951733828,0.006435905117541552,0.006316536571830511,0.006360536441206932,0.006263307295739651,0.006158271338790655,0.006162503734230995,0.006091471761465073,0.006106128916144371,0.006110151298344135,0.006021080072969198,0.005875083617866039,0.005995042156428099,0.005917245522141457,0.00577573711052537,0.005976320710033178,0.005808764137327671,0.00585986627265811,0.0057183909229934216,0.005723206326365471,0.005699133966118097,0.005710747092962265,0.005626029334962368,0.005663907155394554,0.0056513333693146706,0.005587018560618162,0.005499569699168205,0.005489315837621689,0.0055375127121806145,0.00545443594455719,0.00549495592713356,0.005487531423568726,0.005400593392550945,0.005372372921556234,0.005304390098899603,0.005376079585403204,0.005361074116080999,0.005363472271710634,0.005300490185618401,0.005229098256677389,0.005278082564473152,0.0052257138304412365,0.00521836569532752,0.005025404039770365,0.0051958137191832066,0.005176105536520481,0.005164284724742174,0.005222391337156296,0.005065111443400383,0.005086202174425125,0.0051048519089818,0.005086423363536596,0.005026593338698149,0.0050361668691039085,0.004982961807399988,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 6507      
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 6508      
=================================================================
Total params: 13,015
Trainable params: 13,015
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 8)                 40        
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 256)               2304      
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 16)                4112      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 51        
=================================================================
Total params: 6,507
Trainable params: 6,507
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 16)                64        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 256)               4352      
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 8)                 2056      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 36        
=================================================================
Total params: 6,508
Trainable params: 6,508
Non-trainable params: 0
_________________________________________________________________
