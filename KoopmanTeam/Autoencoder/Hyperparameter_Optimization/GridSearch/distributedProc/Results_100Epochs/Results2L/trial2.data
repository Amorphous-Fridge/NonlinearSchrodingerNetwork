2021-06-26
loss,0.3469253182411194,0.22632762789726257,0.22235926985740662,0.21693776547908783,0.14288009703159332,0.06030721589922905,0.03847726434469223,0.030828561633825302,0.027028586715459824,0.024405667558312416,0.022714965045452118,0.021023035049438477,0.020222783088684082,0.019063245505094528,0.01862647570669651,0.017992954701185226,0.017369400709867477,0.016895147040486336,0.016563648357987404,0.016243426129221916,0.01577164977788925,0.015568421222269535,0.01521527674049139,0.014960203319787979,0.014804990030825138,0.014488010667264462,0.014375295490026474,0.01414571888744831,0.013957484625279903,0.013723249547183514,0.013604863546788692,0.013445916585624218,0.013205260038375854,0.013085599057376385,0.013016682118177414,0.012972904369235039,0.012760570272803307,0.01270146295428276,0.012563910335302353,0.012469145469367504,0.012360337190330029,0.012179647572338581,0.012100450694561005,0.01210640650242567,0.011960293166339397,0.011878806166350842,0.011825751513242722,0.01179918646812439,0.011660115793347359,0.011614309623837471,0.011487302370369434,0.011441363021731377,0.011393913999199867,0.011293534189462662,0.011188250035047531,0.01121581718325615,0.011113529093563557,0.011068804189562798,0.010917078703641891,0.010943856090307236,0.01086648739874363,0.01083581056445837,0.010778088122606277,0.010713593102991581,0.01062795426696539,0.010632222518324852,0.010578184388577938,0.010513778775930405,0.010391485877335072,0.010405726730823517,0.010308879427611828,0.010373259894549847,0.010290640406310558,0.010286782868206501,0.010224353522062302,0.010201368480920792,0.01008899137377739,0.010069038718938828,0.010048027150332928,0.01001511886715889,0.00993450079113245,0.00997255276888609,0.009922483935952187,0.009887725114822388,0.009843623265624046,0.009799834340810776,0.009772216901183128,0.009751462377607822,0.009721188805997372,0.009672004729509354,0.00962958112359047,0.009636839851737022,0.009589326567947865,0.00952154491096735,0.009514986537396908,0.00953737087547779,0.009475929662585258,0.009479077532887459,0.009417968802154064,0.009404861368238926,
mse,0.043099433183670044,0.018545305356383324,0.01813134178519249,0.017326198518276215,0.008017487823963165,0.0013372312532737851,0.0005686702788807452,0.00036496060783974826,0.00027660332852974534,0.00022524582163896412,0.00019062557839788496,0.0001643576833885163,0.00014972082863096148,0.0001339124864898622,0.00012556405272334814,0.0001163413908216171,0.00010946673137368634,0.00010309631034033373,9.80942786554806e-05,9.328420128440484e-05,8.806743426248431e-05,8.488979801768437e-05,8.161705773090944e-05,7.85800875746645e-05,7.646878657396883e-05,7.300377183128148e-05,7.0927148044575e-05,6.8657987867482e-05,6.696482159895822e-05,6.425438186852261e-05,6.280347588472068e-05,6.154608126962557e-05,5.886991857551038e-05,5.772558506578207e-05,5.690339094144292e-05,5.640148083330132e-05,5.430533565231599e-05,5.389628859120421e-05,5.277779928292148e-05,5.173290628590621e-05,5.0447451940272003e-05,4.9045447667595e-05,4.854389408137649e-05,4.795296263182536e-05,4.70387858513277e-05,4.6426779590547085e-05,4.578080915962346e-05,4.537360291578807e-05,4.439568147063255e-05,4.376234574010596e-05,4.263643131707795e-05,4.249681660439819e-05,4.2098286940017715e-05,4.121017991565168e-05,4.0582846850156784e-05,4.056159741594456e-05,3.997180101578124e-05,3.9394428313244134e-05,3.862989251501858e-05,3.839109922409989e-05,3.7825298932148144e-05,3.744662171811797e-05,3.7209916627034545e-05,3.6697623727377504e-05,3.602198194130324e-05,3.590943379094824e-05,3.5879747883882374e-05,3.513119736453518e-05,3.453233148320578e-05,3.438994463067502e-05,3.380400448804721e-05,3.381761416676454e-05,3.3689906558720395e-05,3.366401506355032e-05,3.2905616535572335e-05,3.2867428672034293e-05,3.226991611882113e-05,3.200475111952983e-05,3.1768864573678e-05,3.1547559046885e-05,3.089673555223271e-05,3.133469363092445e-05,3.112137346761301e-05,3.062122414121404e-05,3.0232751669245772e-05,3.002838639076799e-05,2.982789919769857e-05,2.981289435410872e-05,2.9614477170980535e-05,2.9147917302907445e-05,2.8999982532695867e-05,2.892437441914808e-05,2.8638200092245825e-05,2.81762913800776e-05,2.826955824275501e-05,2.8190288503537886e-05,2.791260340018198e-05,2.7914657039218582e-05,2.7595526262302883e-05,2.7549969672691077e-05,
mae,0.14667800068855286,0.09569693356752396,0.09376443177461624,0.09262436628341675,0.06215604767203331,0.025789940729737282,0.016566423699259758,0.013176863081753254,0.011511867865920067,0.010388058610260487,0.009677065536379814,0.008958266116678715,0.00862205121666193,0.008130165748298168,0.007950241677463055,0.0076705217361450195,0.007407397497445345,0.007211001589894295,0.007065074983984232,0.006941637024283409,0.00672163488343358,0.006635764613747597,0.006507399026304483,0.0063855648040771484,0.006329561583697796,0.006181582808494568,0.006117795594036579,0.006038665305823088,0.00594883318990469,0.005856786388903856,0.0058089811354875565,0.0057377382181584835,0.005623755510896444,0.0055859764106571674,0.005552633199840784,0.005542814265936613,0.00546517688781023,0.005409281700849533,0.005353385582566261,0.005308454856276512,0.005263507831841707,0.005189220421016216,0.005151976831257343,0.0051568676717579365,0.005099555477499962,0.005053065251559019,0.005046642851084471,0.005022823810577393,0.004969865549355745,0.004949910566210747,0.004883982706815004,0.004862823989242315,0.004860953893512487,0.0048131560906767845,0.004763276316225529,0.004782567732036114,0.004735835362225771,0.004707681946456432,0.0046475776471197605,0.0046518053859472275,0.004636069759726524,0.004605113063007593,0.004589330404996872,0.00455337343737483,0.004521982278674841,0.004517050925642252,0.004494057036936283,0.0044756378047168255,0.004406171850860119,0.00442773150280118,0.0043783485889434814,0.004416108597069979,0.0043855574913322926,0.004386714193969965,0.004336608108133078,0.004332652781158686,0.004295608028769493,0.004279832821339369,0.004263835959136486,0.004253131337463856,0.004220790229737759,0.004244693089276552,0.00422268733382225,0.004202407319098711,0.004182274919003248,0.004172731190919876,0.004156118258833885,0.004140806384384632,0.004123575519770384,0.004116659518331289,0.00409572571516037,0.004085300490260124,0.004061880521476269,0.004031165037304163,0.004064913373440504,0.004045538138598204,0.004016396589577198,0.004036441911011934,0.004004867747426033,0.003995473962277174,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 1363      
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 1364      
=================================================================
Total params: 2,727
Trainable params: 2,727
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 16)                80        
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 64)                1088      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 195       
=================================================================
Total params: 1,363
Trainable params: 1,363
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 64)                256       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 16)                1040      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 68        
=================================================================
Total params: 1,364
Trainable params: 1,364
Non-trainable params: 0
_________________________________________________________________
