2021-06-26
loss,0.3258211016654968,0.10549585521221161,0.07457368075847626,0.05644319951534271,0.054736074060201645,0.04532312974333763,0.0373491533100605,0.043220486491918564,0.03992919996380806,0.03524020314216614,0.030934028327465057,0.0332915261387825,0.031057193875312805,0.03061242401599884,0.03026619367301464,0.029205715283751488,0.035786181688308716,0.028933411464095116,0.02610454149544239,0.026076024398207664,0.024866890162229538,0.027836892753839493,0.0223937276750803,0.023588115349411964,0.024200476706027985,0.02651801146566868,0.02722734957933426,0.024336500093340874,0.022211579605937004,0.022992029786109924,0.023855457082390785,0.0234833974391222,0.023557495325803757,0.021259620785713196,0.02145313285291195,0.02275802567601204,0.02275134064257145,0.021851008757948875,0.020443057641386986,0.02054373174905777,0.019269146025180817,0.019010163843631744,0.01884015090763569,0.020546915009617805,0.019664309918880463,0.017703812569379807,0.019454987719655037,0.01975317671895027,0.018407847732305527,0.014742604456841946,0.015451911836862564,0.016874954104423523,0.01570204086601734,0.0188479945063591,0.0201357863843441,0.01851472072303295,0.015107888728380203,0.014186927117407322,0.016391674056649208,0.012818816117942333,0.018107326701283455,0.019700640812516212,0.019100913777947426,0.018162593245506287,0.017407093197107315,0.017021460458636284,0.015808377414941788,0.01493349950760603,0.013872457668185234,0.015128647908568382,0.013586794957518578,0.014235773123800755,0.01817002147436142,0.018254602327942848,0.01696838065981865,0.018991250544786453,0.016186900436878204,0.01501130498945713,0.016430355608463287,0.016947485506534576,0.017043599858880043,0.015727199614048004,0.01633618026971817,0.014924820512533188,0.0156087102368474,0.01656893827021122,0.015156612731516361,0.01450114045292139,0.015376104041934013,0.013430382125079632,0.012431254610419273,0.014734573662281036,0.015938472002744675,0.015764135867357254,0.01524637546390295,0.01356794685125351,0.014134038239717484,0.015028106048703194,0.015958625823259354,0.015030315145850182,
mse,0.0370546318590641,0.0038821143098175526,0.0016551490407437086,0.0009482016903348267,0.0008565953467041254,0.0006036339909769595,0.00042586418567225337,0.0005407495191320777,0.00046070144162513316,0.00036415434442460537,0.0002836293715517968,0.00033068165066652,0.0002840390952769667,0.00027745834086090326,0.00026504741981625557,0.0002510490012355149,0.0003812501672655344,0.00024279521312564611,0.0002036962832789868,0.0001991730387089774,0.00018378182721789926,0.00022313004592433572,0.00015172516577877104,0.00016792339738458395,0.000172610001754947,0.00020259551820345223,0.00021068727073725313,0.0001707969349808991,0.00014539087715093046,0.00015331107715610415,0.00016598058573435992,0.00016088983102235943,0.0001547390565974638,0.00013408952509053051,0.00013703401782549918,0.00014677102444693446,0.00014676192949991673,0.0001347732322756201,0.00012162020721007138,0.0001263169979210943,0.00011163837916683406,0.00010691108764149249,0.0001062738592736423,0.00012048494681948796,0.00011216697021154687,9.302335092797875e-05,0.00010819693125085905,0.00010950328578474,9.766502626007423e-05,6.920730083948001e-05,7.627228478668258e-05,8.580428402638063e-05,7.616108632646501e-05,0.00010264003503834829,0.00011390072904760018,9.702912211650982e-05,7.111504964996129e-05,6.19267811998725e-05,8.418049401370808e-05,5.082322604721412e-05,9.82736237347126e-05,0.00010825853678397834,0.00010166280844714493,9.269396105082706e-05,8.845797128742561e-05,8.203580364352092e-05,7.223728607641533e-05,6.556943844771013e-05,5.7227196521125734e-05,7.073138112900779e-05,5.690259058610536e-05,6.187694816617295e-05,9.349339961772785e-05,9.503465844318271e-05,8.390199218410999e-05,0.00010050557466456667,7.629027822986245e-05,6.813115760451183e-05,7.658258982701227e-05,8.282760973088443e-05,8.428728324361145e-05,7.147908036131412e-05,7.615135109517723e-05,6.583975482499227e-05,7.063881639624014e-05,7.933411689009517e-05,6.636595207965001e-05,6.173583096824586e-05,6.697090429952368e-05,5.387025885283947e-05,4.68840153189376e-05,6.611236312892288e-05,7.223734428407624e-05,7.074510358506814e-05,6.5496118622832e-05,5.525042797671631e-05,5.902353586861864e-05,6.518576992675662e-05,7.278405246324837e-05,6.490868690889329e-05,
mae,0.1403311789035797,0.04617171362042427,0.031718965619802475,0.0239303819835186,0.023119840770959854,0.01936701126396656,0.01600790210068226,0.018269842490553856,0.017099415883421898,0.014818635769188404,0.013063409365713596,0.014249717816710472,0.01311758067458868,0.013043725863099098,0.012995298951864243,0.012385312467813492,0.015168152749538422,0.012324311770498753,0.011223124340176582,0.011096659116446972,0.01070666778832674,0.012036943808197975,0.009483170695602894,0.010072777047753334,0.010292879305779934,0.01132019143551588,0.011537804268300533,0.010286906734108925,0.009509582072496414,0.009698443114757538,0.01020717341452837,0.010047856718301773,0.01008159201592207,0.009152762591838837,0.009080005809664726,0.009664464741945267,0.009712746366858482,0.00931333377957344,0.008614418096840382,0.008840941824018955,0.00822353083640337,0.008096558041870594,0.007949471473693848,0.008648006245493889,0.008263004012405872,0.0075614298693835735,0.008287893608212471,0.008367005735635757,0.007798613514751196,0.006359450984746218,0.006522298324853182,0.0072300671599805355,0.006710869260132313,0.007997275330126286,0.00863935612142086,0.007854623720049858,0.006354964803904295,0.006025465205311775,0.006969737820327282,0.005474037490785122,0.00777062913402915,0.008301068097352982,0.008103382773697376,0.007706340402364731,0.007426789496093988,0.007215971127152443,0.006768298335373402,0.006317243445664644,0.0058341301046311855,0.006341726053506136,0.005807595793157816,0.006124282255768776,0.007763892877846956,0.00790440570563078,0.007233685348182917,0.007994742132723331,0.006922869011759758,0.006372338626533747,0.006962425075471401,0.007235309109091759,0.007314037065953016,0.006690656766295433,0.006882658693939447,0.006372703239321709,0.006701160687953234,0.007066002115607262,0.006371743511408567,0.006137969437986612,0.006531220860779285,0.005700673442333937,0.005244177766144276,0.006294026970863342,0.006753632333129644,0.006756462622433901,0.006537593435496092,0.005747582763433456,0.006035351660102606,0.006299634464085102,0.00677454611286521,0.006365630775690079,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 75907     
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 75908     
=================================================================
Total params: 151,815
Trainable params: 151,815
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 2048)              10240     
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 32)                65568     
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 99        
=================================================================
Total params: 75,907
Trainable params: 75,907
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 32)                128       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 2048)              67584     
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 8196      
=================================================================
Total params: 75,908
Trainable params: 75,908
Non-trainable params: 0
_________________________________________________________________
