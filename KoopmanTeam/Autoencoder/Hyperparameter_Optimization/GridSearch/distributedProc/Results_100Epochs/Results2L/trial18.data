2021-06-26
loss,0.2963944971561432,0.11767364293336868,0.05232791602611542,0.03906051069498062,0.04052791744470596,0.03728017210960388,0.032337430864572525,0.030332699418067932,0.027447981759905815,0.02511020004749298,0.02377219684422016,0.02294652909040451,0.022053932771086693,0.02136371098458767,0.020991064608097076,0.0204851683229208,0.02005165070295334,0.01965782232582569,0.01930508203804493,0.019052324816584587,0.0187396127730608,0.01861286722123623,0.018447715789079666,0.018105648458003998,0.0180240198969841,0.017709851264953613,0.017689304426312447,0.017397712916135788,0.017221106216311455,0.017022764310240746,0.016804341226816177,0.016576305031776428,0.016615524888038635,0.016433918848633766,0.016339683905243874,0.016178827732801437,0.01602134481072426,0.01623343490064144,0.016061989590525627,0.01606663130223751,0.01576489955186844,0.015584251843392849,0.015497313812375069,0.01533234678208828,0.015217193402349949,0.014919011853635311,0.014937899075448513,0.014910557307302952,0.014698812738060951,0.014717408455908298,0.014590632170438766,0.014484828338027,0.014376367442309856,0.014311449602246284,0.014125395566225052,0.014140193350613117,0.014035538770258427,0.014011514373123646,0.013832666911184788,0.013843699358403683,0.015163657255470753,0.022247696295380592,0.01903914101421833,0.017392657697200775,0.016520600765943527,0.020203476771712303,0.021598894149065018,0.01995782181620598,0.01900986023247242,0.01827629655599594,0.017802126705646515,0.01730692945420742,0.017184924334287643,0.01610856130719185,0.015370327048003674,0.019675077870488167,0.023201361298561096,0.020724695175886154,0.01941504329442978,0.023495782166719437,0.027665331959724426,0.026346523314714432,0.02479724958539009,0.023763086646795273,0.02293510176241398,0.02218182571232319,0.021580297499895096,0.021045751869678497,0.02058563195168972,0.020209908485412598,0.019779155030846596,0.019386090338230133,0.01911771111190319,0.018758827820420265,0.018437709659337997,0.018189512193202972,0.017952390015125275,0.017728645354509354,0.017503835260868073,0.017280304804444313,
mse,0.03250248730182648,0.005550824571400881,0.001003812300041318,0.0005630658124573529,0.0005518773687072098,0.00044301364687271416,0.0003423668385948986,0.00029875259497202933,0.0002457243390381336,0.000210364640224725,0.00018711945449467748,0.0001725249894661829,0.0001603923155926168,0.0001492213923484087,0.0001433079451089725,0.00013618874072562903,0.0001295736146857962,0.00012485049956012517,0.00011963563156314194,0.00011576844553928822,0.00011172328231623396,0.00010981617379002273,0.00010685739835025743,0.00010259815462632105,0.00010161880345549434,9.945886267814785e-05,9.732742182677612e-05,9.341221448266879e-05,9.098343434743583e-05,8.864641131367534e-05,8.658877777634189e-05,8.61059597809799e-05,8.47000555950217e-05,8.277804590761662e-05,8.071916090557352e-05,7.943453965708613e-05,7.852325506974012e-05,8.144829189404845e-05,8.101329149212688e-05,7.727833144599572e-05,7.365585770457983e-05,7.22049517207779e-05,7.105075201252475e-05,6.930888048373163e-05,6.855947140138596e-05,6.616840983042493e-05,6.580099579878151e-05,6.557058804901317e-05,6.383887375704944e-05,6.365049921441823e-05,6.256911001401022e-05,6.160011980682611e-05,6.059487350285053e-05,6.0199199651833624e-05,5.852775939274579e-05,5.862833131686784e-05,5.7803772506304085e-05,5.7397894124733284e-05,5.567101834458299e-05,5.5851429351605475e-05,7.455565355485305e-05,0.0001418079627910629,0.00010625839058775455,9.041107114171609e-05,8.122246799757704e-05,0.00011797007755376399,0.00012680004874709994,0.00011024305422324687,0.00010061151988338679,9.414018131792545e-05,8.908049494493753e-05,8.495977817801759e-05,8.45725298859179e-05,7.531771552748978e-05,7.32875632820651e-05,0.00011479920794954523,0.00014478364028036594,0.00011701232870109379,0.00010420901526231319,0.00016386658535338938,0.00021821611153427511,0.00018828720203600824,0.00016844726633280516,0.00015479925787076354,0.0001439789921278134,0.00013455293083097786,0.00012796171358786523,0.00012190471170470119,0.00011716574226738885,0.0001130909877247177,0.00010884750372497365,0.00010483622463652864,0.00010205581202171743,9.834155207499862e-05,9.622922516427934e-05,9.344048157799989e-05,9.091477113543078e-05,8.906266884878278e-05,8.699719182914123e-05,8.468695159535855e-05,
mae,0.12372691184282303,0.04957512393593788,0.022353071719408035,0.016642548143863678,0.01722804643213749,0.01594751514494419,0.013872653245925903,0.012936298735439777,0.011639770120382309,0.010670688934624195,0.010087783448398113,0.009750843979418278,0.009377609007060528,0.009093162603676319,0.008933519944548607,0.00872112438082695,0.008536431938409805,0.008407891727983952,0.008240733295679092,0.00819107424467802,0.008007297292351723,0.007936779409646988,0.007875697687268257,0.007685218472033739,0.007649078965187073,0.007591689005494118,0.007529005873948336,0.007468957453966141,0.007356726098805666,0.007217916194349527,0.00716029666364193,0.007071913685649633,0.007111652288585901,0.006990687921643257,0.006973794661462307,0.006896057166159153,0.006818804889917374,0.006956510711461306,0.006843667011708021,0.006845859345048666,0.006734092719852924,0.006682658102363348,0.006638248451054096,0.006512755062431097,0.006467156577855349,0.006376237142831087,0.0063575697131454945,0.006346000824123621,0.006275404244661331,0.006196218077093363,0.006160097196698189,0.006143486127257347,0.00612145010381937,0.006057376973330975,0.006026112474501133,0.005980339832603931,0.006040233187377453,0.00591260427609086,0.005914937239140272,0.005856748670339584,0.006403056904673576,0.009477589279413223,0.008485765196383,0.007745126262307167,0.007388606201857328,0.008785263635218143,0.009467348456382751,0.008777167648077011,0.00836559571325779,0.00804612971842289,0.007824374362826347,0.007554379291832447,0.007338813506066799,0.006972931791096926,0.006547786761075258,0.008317163214087486,0.01003089640289545,0.009002906270325184,0.008496695198118687,0.010040849447250366,0.011693138629198074,0.011367879807949066,0.01071983017027378,0.01034129410982132,0.009944274090230465,0.00959011446684599,0.009369079023599625,0.009213880635797977,0.009078197181224823,0.008953732438385487,0.008762264624238014,0.008574745617806911,0.008453886955976486,0.008289950899779797,0.008133020251989365,0.008022356778383255,0.0079036895185709,0.007802352774888277,0.007692578248679638,0.007574969902634621,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 4675      
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 4676      
=================================================================
Total params: 9,351
Trainable params: 9,351
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 64)                320       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 64)                4160      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 195       
=================================================================
Total params: 4,675
Trainable params: 4,675
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 64)                256       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 64)                4160      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 260       
=================================================================
Total params: 4,676
Trainable params: 4,676
Non-trainable params: 0
_________________________________________________________________
