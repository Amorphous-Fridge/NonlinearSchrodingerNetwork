2021-06-26
loss,0.32743099331855774,0.18878741562366486,0.08049093931913376,0.04617379233241081,0.03408641740679741,0.028012914583086967,0.02540653944015503,0.023666467517614365,0.022340435534715652,0.021345682442188263,0.02050819806754589,0.019846994429826736,0.019296158105134964,0.018738454207777977,0.018370943143963814,0.01789127290248871,0.017387736588716507,0.017069585621356964,0.016889743506908417,0.016497327014803886,0.016273651272058487,0.015987442806363106,0.015740379691123962,0.015491579659283161,0.01540292240679264,0.01520218700170517,0.014983686618506908,0.014784049242734909,0.01461891457438469,0.014450631104409695,0.01427666749805212,0.01423992495983839,0.014087116345763206,0.014013306237757206,0.013871929608285427,0.01372840628027916,0.013679741881787777,0.013595658354461193,0.013459010981023312,0.013317763805389404,0.013319642283022404,0.0132282804697752,0.013103543780744076,0.013128332793712616,0.012948687188327312,0.01289241574704647,0.01277543418109417,0.012812487781047821,0.012706946581602097,0.012548979371786118,0.01252415869385004,0.012477818876504898,0.012437809258699417,0.012031854130327702,0.019065067172050476,0.01813618093729019,0.016332941129803658,0.015290290117263794,0.014618396759033203,0.01439209096133709,0.013618145138025284,0.01290474459528923,0.012822618708014488,0.012583133764564991,0.012446608394384384,0.012259026989340782,0.012200603261590004,0.01209671888500452,0.011959750205278397,0.011982543393969536,0.011847903952002525,0.011729499325156212,0.011708002537488937,0.011619586497545242,0.011600349098443985,0.011509312316775322,0.011473380029201508,0.011427496559917927,0.011342057958245277,0.011300130747258663,0.011350198648869991,0.011246058158576488,0.011180352419614792,0.011204567737877369,0.011151880025863647,0.0111087616533041,0.01104983314871788,0.01109025813639164,0.010991143994033337,0.010937293991446495,0.010948827490210533,0.010842339135706425,0.010890116915106773,0.010805536061525345,0.010807942599058151,0.010747655294835567,0.01070305984467268,0.010701468214392662,0.010696173645555973,0.010585992597043514,
mse,0.039448581635951996,0.012950917705893517,0.00229463423602283,0.0007528002024628222,0.00039953429950401187,0.00026496549253351986,0.00021259160712361336,0.00018202424689661711,0.00016071993741206825,0.0001448862167308107,0.00013288272020872682,0.00012383642024360597,0.00011650164378806949,0.00010832260159077123,0.00010349479271098971,9.807916649151593e-05,9.205984679283574e-05,8.843890100251883e-05,8.603981405030936e-05,8.234449342126027e-05,7.956987974466756e-05,7.673972140764818e-05,7.400927279377356e-05,7.177663064794615e-05,7.090703002177179e-05,6.905649934196845e-05,6.659500650130212e-05,6.468645733548328e-05,6.318648956948891e-05,6.167504034237936e-05,6.0069392930017784e-05,5.9403300838312134e-05,5.817607598146424e-05,5.7471384934615344e-05,5.619712464977056e-05,5.540805068449117e-05,5.467200026032515e-05,5.3732026572106406e-05,5.274158320389688e-05,5.178683568374254e-05,5.1257771701784804e-05,5.0533049943624064e-05,4.9955771828535944e-05,4.970046575181186e-05,4.868740506935865e-05,4.825581709155813e-05,4.716142575489357e-05,4.7042045480338857e-05,4.620123945642263e-05,4.5554013922810555e-05,4.5254022552398965e-05,4.4747619540430605e-05,4.5404307456919923e-05,4.446818638825789e-05,0.00011299551260890439,9.590666013536975e-05,7.875735173001885e-05,6.921248859725893e-05,6.29583082627505e-05,6.092592957429588e-05,5.2903644245816395e-05,4.6847933845128864e-05,4.625633664545603e-05,4.456008173292503e-05,4.37561102444306e-05,4.265939787728712e-05,4.246931348461658e-05,4.1511488234391436e-05,4.072894444107078e-05,4.0789869672153145e-05,3.9928989281179383e-05,3.920622111763805e-05,3.8944141124375165e-05,3.861411823891103e-05,3.858987838611938e-05,3.820928031927906e-05,3.7606565456371754e-05,3.70770285371691e-05,3.666234260890633e-05,3.690711309900507e-05,3.667447526822798e-05,3.592891152948141e-05,3.575995287974365e-05,3.5911158192902803e-05,3.5472341551212594e-05,3.5230426874477416e-05,3.481247404124588e-05,3.506761640892364e-05,3.430386641412042e-05,3.3838976378319785e-05,3.418832420720719e-05,3.365618613315746e-05,3.367883255123161e-05,3.306723738205619e-05,3.306388680357486e-05,3.2869604183360934e-05,3.2486816053278744e-05,3.251068847021088e-05,3.2550349715165794e-05,3.1844152545090765e-05,
mae,0.13530988991260529,0.07716309279203415,0.03414016216993332,0.01998783089220524,0.014651138335466385,0.011971411295235157,0.010836993344128132,0.010120229795575142,0.009541105479001999,0.00912235863506794,0.00876107718795538,0.008497077971696854,0.008288759738206863,0.007986124604940414,0.007909136824309826,0.0076670292764902115,0.007438150234520435,0.007279777433723211,0.007245379034429789,0.007047334685921669,0.006977219134569168,0.006880502216517925,0.006724626291543245,0.006594313774257898,0.00656013423576951,0.006537707522511482,0.006473305635154247,0.00627214414998889,0.006292128004133701,0.006176943890750408,0.006019705906510353,0.0060850512236356735,0.006022361107170582,0.006073886528611183,0.005962089169770479,0.005805847235023975,0.005814479198306799,0.00587362889200449,0.005744566209614277,0.005680607166141272,0.00574311800301075,0.005686113145202398,0.005543604027479887,0.0056029087863862514,0.005583988968282938,0.0054915291257202625,0.005471325013786554,0.005486806854605675,0.00556320371106267,0.005300353746861219,0.005309876054525375,0.00539133558049798,0.005295608192682266,0.005135297775268555,0.008679468184709549,0.008514496497809887,0.007574092131108046,0.007024856749922037,0.00633881613612175,0.00645235413685441,0.0057930247858166695,0.005537096876651049,0.005507597699761391,0.005428369157016277,0.0053515248000621796,0.005217905156314373,0.005199316889047623,0.005146551877260208,0.00506961764767766,0.005141510162502527,0.005129009950906038,0.004999171476811171,0.00504034711048007,0.004968464840203524,0.004918435122817755,0.004917825572192669,0.004895729944109917,0.004882870241999626,0.004912758246064186,0.0047810799442231655,0.004837525077164173,0.004819879308342934,0.004771053325384855,0.00479182880371809,0.004760356154292822,0.004748202860355377,0.004637518431991339,0.0047392467968165874,0.004686024971306324,0.00461087841540575,0.004696272313594818,0.004684279207140207,0.004635749850422144,0.004627137444913387,0.00461598439142108,0.004549018107354641,0.004573543556034565,0.0046183574013412,0.004457233939319849,0.004432063549757004,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 4867      
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 4868      
=================================================================
Total params: 9,735
Trainable params: 9,735
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 128)               640       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 32)                4128      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 99        
=================================================================
Total params: 4,867
Trainable params: 4,867
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 32)                128       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 128)               4224      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 516       
=================================================================
Total params: 4,868
Trainable params: 4,868
Non-trainable params: 0
_________________________________________________________________
