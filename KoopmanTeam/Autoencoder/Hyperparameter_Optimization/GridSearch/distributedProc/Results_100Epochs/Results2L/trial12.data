2021-06-26
loss,0.3094843029975891,0.08607683330774307,0.044219717383384705,0.03910405933856964,0.03331784904003143,0.0296027772128582,0.02603362873196602,0.02205781638622284,0.020421862602233887,0.01946786791086197,0.01845971867442131,0.017730645835399628,0.017046784982085228,0.016474120318889618,0.016033925116062164,0.015496828593313694,0.015188387595117092,0.014800400473177433,0.014542579650878906,0.014284058474004269,0.013967994600534439,0.013735470362007618,0.01356652844697237,0.013372372835874557,0.013131452724337578,0.012914909981191158,0.012861371040344238,0.012645197100937366,0.012562954798340797,0.012522666715085506,0.012302041985094547,0.012175541371107101,0.012067326344549656,0.012006646022200584,0.011792820878326893,0.011671199463307858,0.01167298574000597,0.011744188144803047,0.011499213054776192,0.011422420851886272,0.01130923256278038,0.01130262017250061,0.011167450807988644,0.010931364260613918,0.011108250357210636,0.010965908877551556,0.01089515071362257,0.010834750719368458,0.01077311486005783,0.010697734542191029,0.010616814717650414,0.010587487369775772,0.010564104653894901,0.010411071591079235,0.010414578951895237,0.010386329144239426,0.010304762050509453,0.010590015910565853,0.010344075970351696,0.01024160161614418,0.010134386830031872,0.010093217715620995,0.01005741860717535,0.010073916986584663,0.009994085878133774,0.009933331981301308,0.009816359728574753,0.009817657060921192,0.009733641520142555,0.009706610813736916,0.009720750153064728,0.009648245759308338,0.009564405307173729,0.009510337375104427,0.00956495851278305,0.009483370929956436,0.00942900124937296,0.009337053634226322,0.009355776011943817,0.009430058300495148,0.00928311888128519,0.009250112809240818,0.009262749925255775,0.009238619357347488,0.009126427583396435,0.009163610637187958,0.00907969195395708,0.00910966843366623,0.00904605258256197,0.008973301388323307,0.008960851468145847,0.009002611972391605,0.008918034844100475,0.00889789778739214,0.008823366835713387,0.011859354563057423,0.016030097380280495,0.015204660594463348,0.014339297078549862,0.014941977337002754,
mse,0.034620024263858795,0.002870232565328479,0.0007108075660653412,0.0005198310827836394,0.0003539519675541669,0.0002743866643868387,0.00021763810946140438,0.00015839989646337926,0.00013565659173764288,0.00012118821177864447,0.00010904463852057233,9.972890984499827e-05,9.201321518048644e-05,8.549461927032098e-05,8.02061622380279e-05,7.487247057724744e-05,7.184580317698419e-05,6.76781201036647e-05,6.537456647492945e-05,6.247012061066926e-05,5.9368623624322936e-05,5.7662557082949206e-05,5.59983127459418e-05,5.433300611912273e-05,5.206380956224166e-05,5.010539825889282e-05,4.969893052475527e-05,4.8178517317865044e-05,4.7482088120887056e-05,4.725293183582835e-05,4.5205950300442055e-05,4.432187051861547e-05,4.352998803369701e-05,4.27403392677661e-05,4.1548439185135067e-05,4.08330924983602e-05,4.125603300053626e-05,4.18367053498514e-05,3.9224065403686836e-05,3.8473539461847395e-05,3.8342157495208085e-05,3.782205385505222e-05,3.738072337000631e-05,3.573984940885566e-05,3.629058483056724e-05,3.586097591323778e-05,3.508521695039235e-05,3.4901815524790436e-05,3.478485086816363e-05,3.388473123777658e-05,3.33248135575559e-05,3.3411568438168615e-05,3.3048982004402205e-05,3.233554889447987e-05,3.2465399272041395e-05,3.1685198337072507e-05,3.1734172807773575e-05,3.347744859638624e-05,3.145811933791265e-05,3.072524123126641e-05,3.0028177206986584e-05,3.026016929652542e-05,3.0158513254718855e-05,3.0122413591016084e-05,2.9477920179488137e-05,2.8669142920989543e-05,2.850816963473335e-05,2.8366745027597062e-05,2.748181577771902e-05,2.7986377972410992e-05,2.77879062196007e-05,2.7183043130207807e-05,2.7233321816311218e-05,2.6506835638429038e-05,2.673482958925888e-05,2.63741058006417e-05,2.603963002911769e-05,2.5937244572560303e-05,2.5992316295742057e-05,2.6077534130308777e-05,2.515871346986387e-05,2.4931938241934404e-05,2.533054976083804e-05,2.515727828722447e-05,2.429556116112508e-05,2.487436904630158e-05,2.387690983596258e-05,2.4448379917885177e-05,2.3961516490089707e-05,2.4011671484913677e-05,2.3584920199937187e-05,2.4229548216680996e-05,2.3584174414281733e-05,2.3464403057005256e-05,2.313673758180812e-05,4.540352165349759e-05,7.072321022860706e-05,6.353764183586463e-05,5.918687384109944e-05,6.36931144981645e-05,
mae,0.1336798518896103,0.03614979609847069,0.01902398280799389,0.016514696180820465,0.01421583816409111,0.012732894159853458,0.011239388026297092,0.009407558478415012,0.008717535994946957,0.008333105593919754,0.00793371256440878,0.007585553918033838,0.00729253189638257,0.007064741104841232,0.006878158543258905,0.006627976894378662,0.0065104104578495026,0.006379634607583284,0.006179641466587782,0.006124461069703102,0.005926953162997961,0.005917308386415243,0.005816491786390543,0.005717359948903322,0.005699791945517063,0.005517491605132818,0.005471224896609783,0.005405130330473185,0.005378435365855694,0.005355950444936752,0.005269371904432774,0.005159303545951843,0.005141922738403082,0.005161866080015898,0.005065468605607748,0.00500884372740984,0.004935877863317728,0.005070777609944344,0.0049264137633144855,0.004892204888164997,0.004857590887695551,0.004878128878772259,0.004809812176972628,0.004654014017432928,0.004676849115639925,0.004647476598620415,0.004608824383467436,0.004584321286529303,0.004620850086212158,0.004421791527420282,0.004494301974773407,0.004499770700931549,0.004510296043008566,0.0044106896966695786,0.004431227687746286,0.004395471420139074,0.004355223383754492,0.004472631029784679,0.00436797458678484,0.004375070333480835,0.004342264030128717,0.0042663756757974625,0.004326561465859413,0.004288575612008572,0.00427449494600296,0.0042325877584517,0.004216903354972601,0.004228917416185141,0.004081294406205416,0.004119874443858862,0.004135456867516041,0.004102871287614107,0.004045478999614716,0.004047876223921776,0.00401639798656106,0.004041377920657396,0.004071289207786322,0.004008650314062834,0.0039561716839671135,0.00401334511116147,0.003943119663745165,0.003889442188665271,0.003969432320445776,0.003950408659875393,0.003861862700432539,0.003926136530935764,0.0038817585445940495,0.0038708425126969814,0.003883226541802287,0.0038340792525559664,0.0038170067127794027,0.003814530558884144,0.0037948740646243095,0.00377958663739264,0.0037469607777893543,0.0050549679435789585,0.006734490394592285,0.006160215940326452,0.005756544880568981,0.006271314807236195,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 9379      
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 9380      
=================================================================
Total params: 18,759
Trainable params: 18,759
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 32)                160       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 256)               8448      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 771       
=================================================================
Total params: 9,379
Trainable params: 9,379
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 256)               1024      
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 32)                8224      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 132       
=================================================================
Total params: 9,380
Trainable params: 9,380
Non-trainable params: 0
_________________________________________________________________
