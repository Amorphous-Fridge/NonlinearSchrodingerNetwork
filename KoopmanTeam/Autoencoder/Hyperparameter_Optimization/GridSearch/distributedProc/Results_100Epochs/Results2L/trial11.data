2021-06-26
loss,0.3627098798751831,0.2219865918159485,0.14592722058296204,0.07320120930671692,0.04621599242091179,0.03581036254763603,0.031236931681632996,0.028747422620654106,0.026367202401161194,0.024892399087548256,0.02434545010328293,0.023404905572533607,0.022529372945427895,0.022140679880976677,0.02158169075846672,0.020810406655073166,0.0205412358045578,0.019863275811076164,0.01949983276426792,0.01882060058414936,0.01867849752306938,0.018256884068250656,0.017901897430419922,0.01757916994392872,0.01743135042488575,0.01716252975165844,0.01681210845708847,0.016681931912899017,0.016453007236123085,0.016289755702018738,0.015986496582627296,0.015858985483646393,0.0157540962100029,0.015516401268541813,0.0154667804017663,0.015235069207847118,0.015079844743013382,0.014861354604363441,0.014998132362961769,0.014724493958055973,0.014658847823739052,0.01450328342616558,0.014370829798281193,0.014321045018732548,0.014222873374819756,0.014156600460410118,0.014039278030395508,0.013972865417599678,0.013830546289682388,0.013776566833257675,0.013645538128912449,0.013683722354471684,0.013544538989663124,0.013499514199793339,0.013388506136834621,0.013277309946715832,0.013247218914330006,0.0131680928170681,0.013151482678949833,0.013031954877078533,0.013018721714615822,0.012935562059283257,0.01289596688002348,0.012842132709920406,0.012754004448652267,0.01269098836928606,0.01263201143592596,0.012572142295539379,0.012667014263570309,0.01248367503285408,0.012437190860509872,0.012479182332754135,0.012386773712933064,0.012306246906518936,0.012232687324285507,0.012275958433747292,0.012164006009697914,0.012201751582324505,0.012136795558035374,0.012067493982613087,0.011960539035499096,0.012026931159198284,0.011958014219999313,0.014319753274321556,0.019474027678370476,0.019235752522945404,0.018930688500404358,0.015641959384083748,0.02008994296193123,0.01879327930510044,0.017971573397517204,0.017308132722973824,0.016787512227892876,0.016279734671115875,0.01590314693748951,0.015529733151197433,0.015277158468961716,0.014929232187569141,0.014680440537631512,0.016813619062304497,
mse,0.04615604504942894,0.017746977508068085,0.008414044044911861,0.0018126021604984999,0.000772224273532629,0.00047165126306936145,0.0003458521969150752,0.00027962069725617766,0.00023260632588062435,0.00020975996449124068,0.0001939092908287421,0.0001745218032738194,0.00016647086886223406,0.00015754046034999192,0.00014440261293202639,0.0001351887476630509,0.00013169563317205757,0.00012380453699734062,0.00011695315333781764,0.00010861379996640608,0.00010654053767211735,0.00010149175068363547,9.702405805001035e-05,9.274619515053928e-05,9.129269164986908e-05,8.789852290647104e-05,8.433582115685567e-05,8.327687828568742e-05,8.046273433137685e-05,7.821628241799772e-05,7.578278746223077e-05,7.378985901596025e-05,7.284923049155623e-05,7.0432826760225e-05,6.986478547332808e-05,6.798366666771472e-05,6.618281622650102e-05,6.397324614226818e-05,6.514063716167584e-05,6.251074955798686e-05,6.226087134564295e-05,6.0786282119806856e-05,5.945842349319719e-05,5.8965510106645525e-05,5.839843288413249e-05,5.753974983235821e-05,5.6403940106974915e-05,5.587588020716794e-05,5.463345587486401e-05,5.4249427194008604e-05,5.318757030181587e-05,5.3500123613048345e-05,5.2139806939521804e-05,5.1522987632779405e-05,5.088731268187985e-05,4.979806180926971e-05,4.993927359464578e-05,4.892089782515541e-05,4.885790986008942e-05,4.807134246220812e-05,4.7623001592000946e-05,4.719754360849038e-05,4.6960489271441475e-05,4.666205131798051e-05,4.572466423269361e-05,4.539674046100117e-05,4.482177973841317e-05,4.431486740941182e-05,4.519468348007649e-05,4.3504711356945336e-05,4.333296965342015e-05,4.381277904030867e-05,4.293928941478953e-05,4.220293340040371e-05,4.2103776650037616e-05,4.219591573928483e-05,4.1457089537288994e-05,4.173284105490893e-05,4.1091836465056986e-05,4.059006096213125e-05,3.974866194766946e-05,4.0441987948725e-05,4.004076254204847e-05,6.945767381694168e-05,0.00011240733147133142,0.00010777670104289427,0.0001076355401892215,7.674553489778191e-05,0.00011117845861008391,9.587180829839781e-05,8.748226537136361e-05,8.136991527862847e-05,7.701290451223031e-05,7.371674291789532e-05,7.025172817520797e-05,6.731582834618166e-05,6.493725959444419e-05,6.213578308233991e-05,6.069131995900534e-05,8.610363875050098e-05,
mae,0.1546643078327179,0.09261862933635712,0.06211069971323013,0.031128229573369026,0.019859669730067253,0.015370063483715057,0.013338056392967701,0.012257899157702923,0.011238929815590382,0.01058023888617754,0.010411269962787628,0.009986263699829578,0.009676152840256691,0.009457911364734173,0.009201282635331154,0.008907584473490715,0.008744173683226109,0.008462537080049515,0.008305970579385757,0.00799047201871872,0.008051193319261074,0.007863237522542477,0.007575657218694687,0.007582721300423145,0.007403618190437555,0.007291025947779417,0.007177447900176048,0.007242539431899786,0.007075282279402018,0.006985170766711235,0.006781591102480888,0.006788200233131647,0.006701209116727114,0.00670822337269783,0.006615949794650078,0.006592273712158203,0.006464146543294191,0.006214920897036791,0.0064292713068425655,0.006297823041677475,0.006326558534055948,0.006103979889303446,0.006183620076626539,0.006101122125983238,0.006140387151390314,0.006060676649212837,0.006013695616275072,0.005984972231090069,0.005934315733611584,0.005757412873208523,0.005790521390736103,0.005909159779548645,0.005719613283872604,0.005754916463047266,0.0057340567000210285,0.005635716486722231,0.005591752473264933,0.005685442592948675,0.0056874738074839115,0.0055563850328326225,0.0055674402974545956,0.005508625879883766,0.005454737693071365,0.0055083464831113815,0.005341025069355965,0.005448824260383844,0.005495446268469095,0.005353762302547693,0.005397271364927292,0.005338671617209911,0.005337735638022423,0.00527088763192296,0.005291101522743702,0.005263874307274818,0.005206899717450142,0.005287122912704945,0.005221230443567038,0.00524903554469347,0.005130037199705839,0.005249958485364914,0.005090954713523388,0.005171575117856264,0.004989241249859333,0.006183628458529711,0.008366638794541359,0.008325903676450253,0.008071106858551502,0.006467731203883886,0.008125496096909046,0.007634188048541546,0.00732844416052103,0.0070840949192643166,0.006902002729475498,0.006742043886333704,0.006642999593168497,0.0065026963129639626,0.006413603201508522,0.0062713283114135265,0.006127874832600355,0.007090350612998009,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 4771      
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 4772      
=================================================================
Total params: 9,543
Trainable params: 9,543
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 32)                160       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 128)               4224      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 387       
=================================================================
Total params: 4,771
Trainable params: 4,771
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 128)               512       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 32)                4128      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 132       
=================================================================
Total params: 4,772
Trainable params: 4,772
Non-trainable params: 0
_________________________________________________________________
