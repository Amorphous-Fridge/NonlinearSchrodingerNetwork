2021-06-26
loss,0.24932661652565002,0.06460341811180115,0.045590098947286606,0.04677300900220871,0.041981782764196396,0.041183266788721085,0.03730856254696846,0.03465523570775986,0.03271025791764259,0.040452491492033005,0.032447896897792816,0.034925058484077454,0.031148144975304604,0.03134286031126976,0.03138839080929756,0.033393606543540955,0.0301080159842968,0.028925297781825066,0.028983091935515404,0.025568939745426178,0.027120349928736687,0.027787253260612488,0.027960537001490593,0.024285033345222473,0.022736189886927605,0.023808009922504425,0.020885571837425232,0.026116924360394478,0.023456454277038574,0.024151869118213654,0.02189398929476738,0.01973423734307289,0.01801077276468277,0.01775268465280533,0.021328909322619438,0.0212326031178236,0.02207646332681179,0.021062195301055908,0.019686246290802956,0.020391220226883888,0.020265085622668266,0.019356951117515564,0.02041674219071865,0.02053125947713852,0.018909716978669167,0.017158158123493195,0.01869930513203144,0.018848611041903496,0.019549252465367317,0.018422728404402733,0.016765009611845016,0.015859419479966164,0.016849663108587265,0.015203498303890228,0.014130502939224243,0.0159792248159647,0.01919812522828579,0.017103303223848343,0.018351895734667778,0.01730060763657093,0.015873124822974205,0.014927045442163944,0.015373818576335907,0.014843489043414593,0.013892234303057194,0.014438707381486893,0.013262114487588406,0.013968423940241337,0.016654208302497864,0.015854032710194588,0.01721847616136074,0.01645994931459427,0.01648339256644249,0.017255766317248344,0.017099080607295036,0.015857748687267303,0.014813500456511974,0.014058246277272701,0.013003170490264893,0.012385631911456585,0.011717722751200199,0.011759605258703232,0.011026214808225632,0.014614828862249851,0.016307560727000237,0.014705355279147625,0.014793625101447105,0.014444989152252674,0.013461722061038017,0.012828776612877846,0.013169118203222752,0.015154533088207245,0.014804340898990631,0.015437272377312183,0.014575352892279625,0.01352976355701685,0.013008075766265392,0.012603094801306725,0.01189885288476944,0.01185703556984663,
mse,0.028004294261336327,0.0013469156110659242,0.0006222890224307775,0.0006430033827200532,0.0005154978716745973,0.0004851799749303609,0.00043198507046326995,0.0003636676410678774,0.00031210677116177976,0.0004658725520130247,0.0003105301584582776,0.00034981605131179094,0.00029425459797494113,0.0002874433994293213,0.00027941231383010745,0.0003173516015522182,0.00025818843278102577,0.00023945396242197603,0.00024148021475411952,0.00019290138152427971,0.0002115420502377674,0.00021757443028036505,0.00022067103418521583,0.00017554586520418525,0.0001562914694659412,0.00016283188597299159,0.0001334135449724272,0.00018895292305387557,0.00015563330089207739,0.0001623442949494347,0.00013505156675819308,0.00011551073112059385,0.00010034145816462114,9.946452337317169e-05,0.00013837849837727845,0.0001299900031881407,0.00013752133236266673,0.0001285881589865312,0.00011693159467540681,0.00011698152957251295,0.00011742248898372054,0.0001093802202376537,0.00011723270290531218,0.00011864786938531324,9.993241837946698e-05,8.874687046045437e-05,0.00010362489410908893,0.00010009951802203432,0.00010854013089556247,9.592865535523742e-05,8.132967195706442e-05,7.769496005494148e-05,8.388263086089864e-05,7.138049113564193e-05,6.103527266532183e-05,7.920455391285941e-05,0.00010141513484995812,8.01926726126112e-05,9.512675023870543e-05,8.392858580918983e-05,7.21435499144718e-05,6.49587600491941e-05,7.108975114533678e-05,6.692162423860282e-05,6.063475666451268e-05,6.63094615447335e-05,5.5784705182304606e-05,5.9777470596600324e-05,7.738947897450998e-05,7.231185009004548e-05,8.223410986829549e-05,7.587332947878167e-05,7.60219045332633e-05,8.431635069428012e-05,8.46927214297466e-05,7.298275158973411e-05,6.241994560696185e-05,5.6300439609913155e-05,5.050688923802227e-05,4.6103272325126454e-05,4.130108209210448e-05,4.24116660724394e-05,3.767715679714456e-05,6.301552639342844e-05,7.364818156929687e-05,6.107354420237243e-05,6.266764103202149e-05,5.824227991979569e-05,5.2248909923946485e-05,4.7552966861985624e-05,5.212122414377518e-05,6.476294947788119e-05,6.145359657239169e-05,6.77703574183397e-05,6.020807632012293e-05,5.300542034092359e-05,4.93641164212022e-05,4.609964162227698e-05,4.24256177211646e-05,4.2796553316293284e-05,
mae,0.10948678851127625,0.02772495523095131,0.019403725862503052,0.019827943295240402,0.017867598682641983,0.017713923007249832,0.015623897314071655,0.014554185792803764,0.013982457108795643,0.017150098457932472,0.013937353156507015,0.014953109435737133,0.01315416768193245,0.013271388597786427,0.01325838640332222,0.014151806943118572,0.012884646654129028,0.012410363182425499,0.01223880983889103,0.010743413120508194,0.01149201299995184,0.011757075786590576,0.011880197562277317,0.010082616470754147,0.00974646583199501,0.010146076790988445,0.008917021565139294,0.011174909770488739,0.009983527474105358,0.010335547849535942,0.009294813498854637,0.008431104011833668,0.007623371668159962,0.007414753548800945,0.009095363318920135,0.008850532583892345,0.009449731558561325,0.008943954482674599,0.00832225102931261,0.008792300708591938,0.008552180603146553,0.008218219503760338,0.008582470007240772,0.008658431470394135,0.00785791501402855,0.007329647429287434,0.00794825330376625,0.008025809191167355,0.00840843003243208,0.007858424447476864,0.0071508679538965225,0.006776232738047838,0.007170695811510086,0.006577808875590563,0.0061179823242127895,0.00682271970435977,0.008123781532049179,0.007189533207565546,0.007610305678099394,0.006987209897488356,0.00661177234724164,0.006370453163981438,0.006582528818398714,0.006375506985932589,0.005944845732301474,0.0060958112590014935,0.005513629876077175,0.005890075117349625,0.007035454735159874,0.006691647693514824,0.007394895423203707,0.006953885778784752,0.006936224177479744,0.007327563129365444,0.007341613993048668,0.006555101368576288,0.006140218582004309,0.0058765532448887825,0.005450639873743057,0.005234085489064455,0.004923117347061634,0.004962841980159283,0.00468704616650939,0.006145731080323458,0.00675333384424448,0.006248186342418194,0.006229078862816095,0.006134113296866417,0.005681933369487524,0.0053877620957791805,0.0056007541716098785,0.0063708568923175335,0.006276822183281183,0.006543968338519335,0.00603568134829402,0.005594704300165176,0.005372004117816687,0.005280562676489353,0.005064358003437519,0.005029839929193258,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 69955     
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 69956     
=================================================================
Total params: 139,911
Trainable params: 139,911
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 64)                320       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 1024)              66560     
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 3075      
=================================================================
Total params: 69,955
Trainable params: 69,955
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 1024)              4096      
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 64)                65600     
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 260       
=================================================================
Total params: 69,956
Trainable params: 69,956
Non-trainable params: 0
_________________________________________________________________
