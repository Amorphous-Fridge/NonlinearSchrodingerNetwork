2021-06-26
loss,0.2748468816280365,0.06939169764518738,0.05693066120147705,0.038558974862098694,0.04012270271778107,0.03268948942422867,0.04025406762957573,0.04463272914290428,0.03542479872703552,0.028263237327337265,0.029493214562535286,0.031943898648023605,0.02905757911503315,0.03603389486670494,0.03142927587032318,0.03348005563020706,0.028986088931560516,0.025679178535938263,0.030061153694987297,0.029025211930274963,0.025722656399011612,0.02385471761226654,0.02333252504467964,0.022669866681098938,0.022786462679505348,0.027852166444063187,0.025811705738306046,0.025271447375416756,0.024925103411078453,0.024968063458800316,0.025333581492304802,0.023791339248418808,0.021660326048731804,0.02048201486468315,0.01916586607694626,0.019201897084712982,0.021878957748413086,0.0224324781447649,0.020502226427197456,0.021668067201972008,0.02135087177157402,0.02112163044512272,0.019818086177110672,0.018276656046509743,0.01817435584962368,0.01996009051799774,0.018584322184324265,0.019504783675074577,0.017600219696760178,0.018619336187839508,0.02054220251739025,0.019510680809617043,0.018140556290745735,0.017040330916643143,0.016079168766736984,0.014938389882445335,0.013061778619885445,0.018244102597236633,0.019303685054183006,0.017270462587475777,0.015819866210222244,0.017067721113562584,0.018435629084706306,0.0175766721367836,0.018971100449562073,0.01907101646065712,0.017410757020115852,0.016117194667458534,0.014689897187054157,0.01634645275771618,0.016414720565080643,0.015237618237733841,0.014218700118362904,0.013525263406336308,0.012495268136262894,0.014165564440190792,0.015757353976368904,0.014685072004795074,0.013586515560746193,0.012954243458807468,0.01200628187507391,0.014152774587273598,0.019258689135313034,0.01674727164208889,0.015658779069781303,0.01472470536828041,0.014098389074206352,0.01346671860665083,0.012567240744829178,0.011196392588317394,0.013989884406328201,0.014407459646463394,0.01734052784740925,0.016575397923588753,0.015151143074035645,0.014023076742887497,0.013391495682299137,0.012871718034148216,0.012282880954444408,0.011816744692623615,
mse,0.036777038127183914,0.0015013562515377998,0.0009894206887111068,0.000451194413471967,0.0004751996020786464,0.00031932417186908424,0.00047496994375251234,0.0005633427063003182,0.000359529658453539,0.0002506971650291234,0.00026299868477508426,0.0003074136620853096,0.00025480808108113706,0.0003675307962112129,0.00028175805346108973,0.0003192204749211669,0.0002450361498631537,0.0002054353099083528,0.0002566348703112453,0.00024385824508499354,0.00019495336164254695,0.00017135996313299984,0.0001650504709687084,0.00015809957403689623,0.00015680823707953095,0.00022161786910146475,0.00018951350648421794,0.00018213836301583797,0.00017980538541451097,0.00017916249635163695,0.00017909880261868238,0.00016060439520515501,0.00013557443162426353,0.0001215247975778766,0.00010738060518633574,0.00011094140791101381,0.00014389949501492083,0.00014429815928451717,0.00012241063814144582,0.00013672288332600147,0.00012757563672494143,0.00012946926290169358,0.0001126994684454985,9.800386033020914e-05,9.810122719500214e-05,0.00011589292262215167,0.00010342399764340371,0.000108084968815092,9.272433089790866e-05,0.00010251951607642695,0.00011894631461473182,0.00010589497105684131,9.420606511412188e-05,8.560679270885885e-05,7.519395876443014e-05,6.9242283643689e-05,5.365976903703995e-05,9.993361163651571e-05,0.00010459410259500146,8.524248551111668e-05,7.75112712290138e-05,8.954041550168768e-05,9.469431824982166e-05,9.012970986077562e-05,0.00010317737178411335,0.00010519058560021222,8.858526416588575e-05,7.644309516763315e-05,6.667724665021524e-05,7.760450534988195e-05,7.829783135093749e-05,6.844881136203185e-05,6.074256089050323e-05,5.685635915142484e-05,5.071550185675733e-05,6.354170909617096e-05,7.160579116316512e-05,6.255925836740062e-05,5.523203071788885e-05,4.9381771532353014e-05,4.6134344302117825e-05,6.449600914493203e-05,0.00010890547855524346,8.020105451578274e-05,7.047435065032914e-05,6.390458293026313e-05,5.900406904402189e-05,5.455179052660242e-05,4.785109922522679e-05,4.0335718949791044e-05,5.9992700698785484e-05,6.297254731180146e-05,8.592903759563342e-05,7.933562301332131e-05,6.814810330979526e-05,5.7665587519295514e-05,5.29255012224894e-05,4.863114736508578e-05,4.412149064592086e-05,4.133623588131741e-05,
mae,0.11864807456731796,0.029674386605620384,0.02420748397707939,0.016420776024460793,0.017038287594914436,0.013822399079799652,0.017108429223299026,0.01922965608537197,0.015057341195642948,0.012069405056536198,0.012587151490151882,0.013805383816361427,0.0123168108984828,0.015430731698870659,0.013261066749691963,0.014377852901816368,0.012263826094567776,0.010903661139309406,0.01284623984247446,0.012363621965050697,0.010949492454528809,0.010192487388849258,0.009943152777850628,0.009588777087628841,0.009588385932147503,0.011826076544821262,0.011016891337931156,0.01080174371600151,0.01063899789005518,0.01072264090180397,0.010680101811885834,0.010139111429452896,0.009219315834343433,0.0085428012534976,0.008013895712792873,0.00810936652123928,0.009192331694066525,0.009499852545559406,0.008570021949708462,0.009211511351168156,0.008996261283755302,0.009098227135837078,0.008543869480490685,0.007900171913206577,0.00773217249661684,0.008431753143668175,0.007888371124863625,0.008270514197647572,0.007465221453458071,0.007908117957413197,0.008658829145133495,0.008255459368228912,0.007739621214568615,0.007305013947188854,0.0068807899951934814,0.006356366910040379,0.005555876065045595,0.0076961396262049675,0.008209045976400375,0.007393205538392067,0.006787188816815615,0.007324354723095894,0.007790024857968092,0.007453327998518944,0.008090291172266006,0.008202781900763512,0.007395298685878515,0.006790978368371725,0.006171004846692085,0.006943277548998594,0.0071512372232973576,0.0067202127538621426,0.006155495531857014,0.0057338448241353035,0.005314705427736044,0.0060095470398664474,0.00654150266200304,0.006174447946250439,0.005614751018583775,0.005315318237990141,0.005088482052087784,0.0059949662536382675,0.00847378745675087,0.007092710118740797,0.006620096042752266,0.0063385264948010445,0.006055563688278198,0.00562183978036046,0.0052155256271362305,0.004740530624985695,0.005934296175837517,0.006152482237666845,0.0074414340779185295,0.0071845934726297855,0.006543030962347984,0.005909381899982691,0.005626238416880369,0.005461341701447964,0.005276625044643879,0.005076546687632799,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 73891     
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 73892     
=================================================================
Total params: 147,783
Trainable params: 147,783
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 32)                160       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 2048)              67584     
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 6147      
=================================================================
Total params: 73,891
Trainable params: 73,891
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 2048)              8192      
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 32)                65568     
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 132       
=================================================================
Total params: 73,892
Trainable params: 73,892
Non-trainable params: 0
_________________________________________________________________
