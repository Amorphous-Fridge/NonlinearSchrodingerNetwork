2021-06-26
loss,0.32536202669143677,0.20621788501739502,0.10394560545682907,0.06046732887625694,0.06185727193951607,0.050083283334970474,0.04657194763422012,0.05158126354217529,0.041210003197193146,0.049629129469394684,0.04644561931490898,0.0406084880232811,0.03850175812840462,0.03242255374789238,0.02974129468202591,0.028671031817793846,0.0345907062292099,0.03602702170610428,0.02933512255549431,0.027416711673140526,0.02404114231467247,0.022208314388990402,0.029349690303206444,0.026262575760483742,0.025930918753147125,0.023889875039458275,0.027739480137825012,0.026608893647789955,0.023248756304383278,0.021651700139045715,0.020819110795855522,0.020086107775568962,0.01961379684507847,0.021533742547035217,0.026381881907582283,0.023722827434539795,0.021391618996858597,0.01995224319398403,0.019231069833040237,0.01886577717959881,0.016866004094481468,0.016846489161252975,0.018052920699119568,0.02022351510822773,0.020306263118982315,0.021662697196006775,0.021968664601445198,0.019423365592956543,0.019894592463970184,0.021201493218541145,0.020253213122487068,0.020130276679992676,0.020213698968291283,0.019072990864515305,0.017717063426971436,0.016140419989824295,0.015659362077713013,0.01921808160841465,0.018555372953414917,0.017209287732839584,0.015031502582132816,0.014066371135413647,0.01395357958972454,0.013566731475293636,0.016049858182668686,0.01886575296521187,0.019580479711294174,0.02031606435775757,0.01969376765191555,0.015741776674985886,0.012997056357562542,0.01586323417723179,0.01932985708117485,0.018369901925325394,0.01671743206679821,0.014819357544183731,0.01565762236714363,0.0167837031185627,0.017403559759259224,0.016339069232344627,0.015095651149749756,0.014091758988797665,0.013345095328986645,0.012001651339232922,0.011995765380561352,0.014345049858093262,0.013779073022305965,0.013829818926751614,0.018207386136054993,0.01611379347741604,0.014749126508831978,0.015934161841869354,0.015183743089437485,0.01762140356004238,0.016757214441895485,0.015578444115817547,0.014785980805754662,0.013682943768799305,0.012919178232550621,0.012292105704545975,
mse,0.03922923654317856,0.015761787071824074,0.003512076335027814,0.001145362388342619,0.0012026629410684109,0.0007478565094061196,0.0006538435118272901,0.0007794495904818177,0.0004939830396324396,0.0007083106902427971,0.0006136965821497142,0.0004657207755371928,0.00041512984898872674,0.00030159318703226745,0.0002542083675507456,0.00024777010548859835,0.00033344156690873206,0.0003610474232118577,0.00025142685626633465,0.00021392367489170283,0.0001681549911154434,0.00014949167962186038,0.00024130847305059433,0.00019933964358642697,0.00018763760454021394,0.00016675023653078824,0.00021643332729581743,0.00020393649174366146,0.00015366298612207174,0.00013569944712799042,0.0001267634506803006,0.00011899502715095878,0.00011541042476892471,0.00013633276103064418,0.00019355988479219377,0.00015877107216510922,0.00013131355808582157,0.00011627849016804248,0.00010599276720313355,0.00010123977699549869,8.452529436908662e-05,8.698738383827731e-05,9.538422455079854e-05,0.00011849401198560372,0.00012202741345390677,0.00013320386642590165,0.0001408025127602741,0.00011148936027893797,0.0001120684391935356,0.0001250869972864166,0.00011336224997648969,0.000113420064735692,0.00011671949323499575,0.00010317027772543952,9.093840344576165e-05,7.751721568638459e-05,7.41558033041656e-05,0.00010650311014614999,9.881694131763652e-05,8.484611316816881e-05,6.784883589716628e-05,6.0198908613529056e-05,5.863662954652682e-05,5.667546429322101e-05,7.825349166523665e-05,0.00010033986472990364,0.00010530337021918967,0.00011570188507903367,0.00010837068839464337,7.777580321999267e-05,5.471131589729339e-05,7.437590829795226e-05,0.00010313332313671708,9.292487084167078e-05,7.953534804983065e-05,6.68957582092844e-05,7.239614933496341e-05,8.135688403854147e-05,8.345884270966053e-05,7.492839358747005e-05,6.677705096080899e-05,6.008600757922977e-05,5.5590629926882684e-05,4.3894906411878765e-05,4.3752905185101554e-05,6.302657129708678e-05,5.8874957176158205e-05,5.8510042435955256e-05,8.924737630877644e-05,7.218609243864194e-05,6.394291995093226e-05,7.453010039171204e-05,6.635885802097619e-05,8.958487887866795e-05,7.767576607875526e-05,6.814239895902574e-05,6.358770042425022e-05,5.7209068472729996e-05,5.0986389396712184e-05,4.642048224923201e-05,
mae,0.14054574072360992,0.09546385705471039,0.04383201152086258,0.025542471557855606,0.026746483519673347,0.021282851696014404,0.019937647506594658,0.021755430847406387,0.01767364703118801,0.020998980849981308,0.019877953454852104,0.01734847016632557,0.016398364678025246,0.013210991397500038,0.012426098808646202,0.012022202834486961,0.014849797822535038,0.015416774898767471,0.012379348278045654,0.011463130824267864,0.010110249742865562,0.009388934820890427,0.012434258125722408,0.01120561733841896,0.01075806375592947,0.01002089399844408,0.011774063110351562,0.011309819296002388,0.00989844836294651,0.009404468350112438,0.009046533145010471,0.008702954277396202,0.008397096768021584,0.009247311390936375,0.01122816652059555,0.010038946755230427,0.008863652125000954,0.00849979929625988,0.008317438885569572,0.00797016266733408,0.007143850438296795,0.007125502917915583,0.007680460810661316,0.008515230379998684,0.008689234033226967,0.00925285741686821,0.009308503940701485,0.008245762437582016,0.008409474045038223,0.009011495858430862,0.008596095256507397,0.008470059372484684,0.008459853939712048,0.008090503513813019,0.007455587852746248,0.006848371587693691,0.006648612208664417,0.008124084211885929,0.007844574749469757,0.007270260248333216,0.006416140589863062,0.0059542907401919365,0.005875072441995144,0.005776648409664631,0.006873573176562786,0.007826289162039757,0.008188891224563122,0.008562159724533558,0.008310643956065178,0.006626763381063938,0.005448677111417055,0.006723834667354822,0.008180270902812481,0.007626265287399292,0.007058643735945225,0.0063367802649736404,0.006599487736821175,0.007127751596271992,0.007296351250261068,0.006860680412501097,0.00637345714494586,0.005839909426867962,0.005701887421309948,0.005047490820288658,0.005108153913170099,0.006062151398509741,0.005936064291745424,0.005933104548603296,0.007646262180060148,0.006776683963835239,0.006175824906677008,0.006702999584376812,0.0064588929526507854,0.007354064378887415,0.007015575654804707,0.00659811869263649,0.006326820235699415,0.005846118554472923,0.005488502327352762,0.005221226718276739,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 34563     
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 34564     
=================================================================
Total params: 69,127
Trainable params: 69,127
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 256)               1280      
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 128)               32896     
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 387       
=================================================================
Total params: 34,563
Trainable params: 34,563
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 128)               512       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 256)               33024     
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 1028      
=================================================================
Total params: 34,564
Trainable params: 34,564
Non-trainable params: 0
_________________________________________________________________
