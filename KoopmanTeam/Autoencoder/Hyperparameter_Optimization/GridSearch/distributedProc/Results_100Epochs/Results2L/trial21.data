2021-06-26
loss,0.31075114011764526,0.15104465186595917,0.06929289549589157,0.054033055901527405,0.06358928978443146,0.055035948753356934,0.037885744124650955,0.03476720303297043,0.03919756039977074,0.04011460766196251,0.03313051536679268,0.037383899092674255,0.030525855720043182,0.026161331683397293,0.024226905778050423,0.023010384291410446,0.022797374054789543,0.021932821720838547,0.021807122975587845,0.022468235343694687,0.022217417135834694,0.03110606223344803,0.02521737851202488,0.02233092114329338,0.020182466134428978,0.0238083116710186,0.029776806011795998,0.025260139256715775,0.022527718916535378,0.021083833649754524,0.01979217492043972,0.02013104036450386,0.021939575672149658,0.02275358885526657,0.02225733920931816,0.02169240079820156,0.022048544138669968,0.019836869090795517,0.01866348460316658,0.018408218398690224,0.01831892319023609,0.016240837052464485,0.017034046351909637,0.02017362043261528,0.024142742156982422,0.02035605162382126,0.019881341606378555,0.018947497010231018,0.01778416335582733,0.016099585220217705,0.015417072921991348,0.019016534090042114,0.02068493515253067,0.01881946250796318,0.01688222587108612,0.017239324748516083,0.018936924636363983,0.01901070773601532,0.017608264461159706,0.0180142130702734,0.01759728230535984,0.017267638817429543,0.01657858118414879,0.01621573604643345,0.015431612730026245,0.016554323956370354,0.016048597171902657,0.015549849718809128,0.014827417209744453,0.01413263101130724,0.015848619863390923,0.01583978720009327,0.014464020729064941,0.013369676657021046,0.015160330571234226,0.01630917377769947,0.018664799630641937,0.017245188355445862,0.015590585768222809,0.015434921719133854,0.015595963224768639,0.015052423812448978,0.013599198311567307,0.014401333406567574,0.015563925728201866,0.016780337318778038,0.015682825818657875,0.015133483335375786,0.014250400476157665,0.013286901637911797,0.012466070242226124,0.011870454996824265,0.011375347152352333,0.013205710798501968,0.01440876442939043,0.012878399342298508,0.0120464488863945,0.013885212130844593,0.014286487363278866,0.01488551963120699,
mse,0.03527257591485977,0.00839148461818695,0.0015623451909050345,0.00093363894848153,0.0012040224391967058,0.0009039948345161974,0.0004419680335558951,0.0003656292683444917,0.0004474514862522483,0.0004552101017907262,0.0003225574910175055,0.00041091477032750845,0.00027037813561037183,0.00020010836306028068,0.00017442040552850813,0.00015743868425488472,0.00015587278176099062,0.0001483044761698693,0.00014866201672703028,0.00015012838412076235,0.00015006662579253316,0.0002697920426726341,0.00018071055819746107,0.0001450685813324526,0.0001261150900973007,0.00018039700808003545,0.0002443455159664154,0.00017924113490153104,0.00014261575415730476,0.0001271851360797882,0.0001137254512286745,0.0001261926954612136,0.00015121352043934166,0.000153108878294006,0.00013445074728224427,0.0001378805172862485,0.00013432120613288134,0.00011137367982883006,9.87240273389034e-05,9.996970766223967e-05,0.00010305779869668186,7.974223990458995e-05,8.631101081846282e-05,0.00011959093535551801,0.00016031092673074454,0.0001202250859932974,0.00011136336979689077,0.00010402677435195073,9.436736581847072e-05,7.964137330418453e-05,7.431038829963654e-05,0.00010544274118728936,0.00011864559928653762,9.953169501386583e-05,8.236146823037416e-05,9.04023545444943e-05,0.00010376288264524192,9.864632011158392e-05,8.777157199801877e-05,9.418784611625597e-05,9.058642899617553e-05,8.115571836242452e-05,7.859312609070912e-05,7.632008782820776e-05,7.148249278543517e-05,7.59183822083287e-05,7.156007632147521e-05,6.868346827104688e-05,6.554016727022827e-05,6.0725375078618526e-05,7.45654760976322e-05,7.296483090613037e-05,6.44112951704301e-05,5.5459095165133476e-05,7.016916060820222e-05,7.72031708038412e-05,9.615945600671694e-05,8.155075192917138e-05,6.782796117477119e-05,6.941761239431798e-05,7.038728654151782e-05,6.578720785910264e-05,5.7944824220612645e-05,6.131797272246331e-05,6.776256486773491e-05,7.654489309061319e-05,6.803638098062947e-05,6.386437598848715e-05,5.605523620033637e-05,5.010365930502303e-05,4.5452015910996124e-05,4.2009487515315413e-05,3.867227860610001e-05,5.261369369691238e-05,6.185078382259235e-05,5.172983219381422e-05,4.50186817033682e-05,5.6139462685678154e-05,5.751737262471579e-05,6.375301745720208e-05,
mae,0.1362190544605255,0.0654233917593956,0.029791850596666336,0.02311662957072258,0.027386851608753204,0.023550035431981087,0.016275351867079735,0.014773980714380741,0.016659138724207878,0.01707950048148632,0.014052079059183598,0.01593315228819847,0.012817034497857094,0.011031015776097775,0.010290256701409817,0.009863892570137978,0.009713957086205482,0.009369534440338612,0.00927519891411066,0.009575732983648777,0.009527980349957943,0.01325791236013174,0.01073155365884304,0.009594209492206573,0.008611568249762058,0.01014784537255764,0.012747019529342651,0.010824363678693771,0.00957834254950285,0.00898001715540886,0.008349818177521229,0.008554876782000065,0.00939763244241476,0.009563351050019264,0.009667880833148956,0.009244891814887524,0.009376740083098412,0.008489906787872314,0.008045115508139133,0.007886187173426151,0.007845626212656498,0.006919526495039463,0.00723367091268301,0.008657545782625675,0.010458685457706451,0.008849858306348324,0.008242313750088215,0.008013460785150528,0.007438587956130505,0.006808689795434475,0.006467917468398809,0.00820270087569952,0.008840840309858322,0.007924742996692657,0.006962260231375694,0.0072501664981245995,0.00808916799724102,0.00808028969913721,0.00748297618702054,0.007640208583325148,0.007403035648167133,0.007454745937138796,0.007209605071693659,0.006955669727176428,0.006576939951628447,0.007053205277770758,0.006781242322176695,0.0065591903403401375,0.006291935220360756,0.005985972471535206,0.006768388673663139,0.006755522917956114,0.006188939791172743,0.005645455792546272,0.006445819512009621,0.0070031434297561646,0.007951012812554836,0.007185297552496195,0.006674017757177353,0.0064348881132900715,0.0063648344948887825,0.006256993860006332,0.005959129426628351,0.006040236447006464,0.0065902117639780045,0.007218680344521999,0.006606193259358406,0.0064055053517222404,0.006074931006878614,0.005603241268545389,0.005284536629915237,0.005046898033469915,0.0048688561655581,0.005561260040849447,0.006149242632091045,0.005513833835721016,0.005121218040585518,0.005996616091579199,0.006021569948643446,0.006111555732786655,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 35139     
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 35140     
=================================================================
Total params: 70,279
Trainable params: 70,279
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 64)                320       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 512)               33280     
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 1539      
=================================================================
Total params: 35,139
Trainable params: 35,139
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 512)               2048      
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 64)                32832     
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 260       
=================================================================
Total params: 35,140
Trainable params: 35,140
Non-trainable params: 0
_________________________________________________________________
