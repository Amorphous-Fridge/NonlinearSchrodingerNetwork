2021-06-26
loss,0.3762552738189697,0.21273301541805267,0.0951242670416832,0.053002797067165375,0.04114106670022011,0.035461798310279846,0.03132246807217598,0.028891680762171745,0.0269264355301857,0.02554471418261528,0.024223845452070236,0.02325918711721897,0.022170213982462883,0.021445617079734802,0.02090059034526348,0.020096685737371445,0.01970796100795269,0.019500071182847023,0.018995027989149094,0.018454842269420624,0.018094515427947044,0.017720507457852364,0.017612561583518982,0.017358487471938133,0.016955388709902763,0.016703052446246147,0.016583945602178574,0.016232840716838837,0.015962861478328705,0.015817826613783836,0.015591979026794434,0.015151542611420155,0.01537484023720026,0.015085156075656414,0.014985674992203712,0.014761466532945633,0.014562482014298439,0.014458337798714638,0.014128419570624828,0.0144618796184659,0.014283139258623123,0.01398027315735817,0.013913643546402454,0.013798750936985016,0.01360279694199562,0.013582957908511162,0.013361359015107155,0.013380859978497028,0.013308852910995483,0.01317243929952383,0.012986399233341217,0.01297425851225853,0.01292281411588192,0.012811618857085705,0.012712039984762669,0.012502184137701988,0.01256219670176506,0.012582331895828247,0.012420345097780228,0.012328296899795532,0.012268696911633015,0.012309465557336807,0.012197916395962238,0.012042634189128876,0.011960054747760296,0.012162081897258759,0.011983668431639671,0.011944829486310482,0.011973226442933083,0.011714884079992771,0.011807943694293499,0.011755619198083878,0.011688761413097382,0.011713135987520218,0.011490795761346817,0.011606140062212944,0.011437293142080307,0.01152844075113535,0.011557234451174736,0.011278526857495308,0.011369850486516953,0.011328441090881824,0.011323997750878334,0.01129036396741867,0.011183436959981918,0.011266651563346386,0.011156801134347916,0.011017649434506893,0.011100614443421364,0.010873574763536453,0.011006460525095463,0.01105477474629879,0.010946262627840042,0.010984760709106922,0.01087385043501854,0.010728512890636921,0.01089787483215332,0.010832130908966064,0.01075174193829298,0.01071704737842083,
mse,0.05234894901514053,0.01565985009074211,0.0036321834195405245,0.0011846540728583932,0.0007149725570343435,0.0005255640717223287,0.00040579569758847356,0.00034027124638669193,0.00029464869294315577,0.0002623094478622079,0.00023237428104039282,0.00021361387916840613,0.00019332888768985868,0.00018063856987282634,0.0001703863381408155,0.00015727132267784327,0.00015066518972162157,0.000144445089972578,0.00013690847845282406,0.00013015673903282732,0.00012588650861289352,0.00011871601600432768,0.00011621040175668895,0.00011439235822763294,0.00010772405948955566,0.00010507204569876194,0.00010276620014337823,9.883732855087146e-05,9.510936797596514e-05,9.454188693780452e-05,9.182371286442503e-05,8.649053052067757e-05,8.847054414218292e-05,8.669987437315285e-05,8.428253204328939e-05,8.076031372183934e-05,7.920744246803224e-05,7.91091297287494e-05,7.494772580685094e-05,7.691443170187995e-05,7.513611490139738e-05,7.245725282700732e-05,7.185184222180396e-05,7.055154856061563e-05,6.810301420046017e-05,6.759237294318154e-05,6.579581531696022e-05,6.497792492154986e-05,6.438435229938477e-05,6.363349530147389e-05,6.10420829616487e-05,6.056289203115739e-05,6.048551222193055e-05,5.913620043429546e-05,5.795120887341909e-05,5.609657091554254e-05,5.595143011305481e-05,5.605176920653321e-05,5.503927968675271e-05,5.313575820764527e-05,5.3167059377301484e-05,5.344122473616153e-05,5.337275069905445e-05,5.120124842505902e-05,5.054003850091249e-05,5.176546619622968e-05,5.035278809373267e-05,4.9982903874479234e-05,5.043112832936458e-05,4.755797272082418e-05,4.870213888352737e-05,4.8104324378073215e-05,4.727345731225796e-05,4.777084177476354e-05,4.575204729917459e-05,4.677457764046267e-05,4.492794323596172e-05,4.563985567074269e-05,4.6082004701020196e-05,4.3419837311375886e-05,4.461654316401109e-05,4.367310248198919e-05,4.358602382126264e-05,4.326899579609744e-05,4.238033579895273e-05,4.328986688051373e-05,4.247154720360413e-05,4.140946111874655e-05,4.174675632384606e-05,3.994852158939466e-05,4.0978080505738035e-05,4.129628723603673e-05,4.006179005955346e-05,4.0472779801348224e-05,3.988516618846916e-05,3.8533871702384204e-05,4.006003655376844e-05,3.921406459994614e-05,3.875892434734851e-05,3.856307012028992e-05,
mae,0.15509770810604095,0.08096759766340256,0.03956391662359238,0.022757915779948235,0.017635636031627655,0.015167509205639362,0.013389003463089466,0.012354246340692043,0.011502498760819435,0.010907676070928574,0.010339926928281784,0.00992929469794035,0.009484332986176014,0.0091832485049963,0.00893864780664444,0.008594033308327198,0.008439149707555771,0.008359448052942753,0.0081435926258564,0.007920244708657265,0.007766845170408487,0.007571938447654247,0.007537903729826212,0.007455841172486544,0.0072705005295574665,0.007160885259509087,0.00710485503077507,0.0069803111255168915,0.0068300883285701275,0.006793898530304432,0.006683709565550089,0.0064465440809726715,0.006611962337046862,0.006466628983616829,0.006433735601603985,0.006326306611299515,0.006219789851456881,0.006199462339282036,0.006040175445377827,0.006197209935635328,0.006137180142104626,0.005991884507238865,0.005952068604528904,0.005940099246799946,0.005839801859110594,0.0058345128782093525,0.005718165077269077,0.005743388086557388,0.0057267616502940655,0.005645023658871651,0.0055937510915100574,0.0055627357214689255,0.00555025227367878,0.005490656476467848,0.005463908892124891,0.005349625833332539,0.005387577228248119,0.005417787004262209,0.005338715389370918,0.005299728363752365,0.005265623796731234,0.005275726318359375,0.005235167685896158,0.005182962864637375,0.005120115354657173,0.005229013040661812,0.005150782410055399,0.005119307432323694,0.005168790929019451,0.004996107425540686,0.005073423497378826,0.005058485083281994,0.005024911370128393,0.005044257268309593,0.004934289492666721,0.004995748866349459,0.004905319306999445,0.004945713095366955,0.00499784154817462,0.004848039709031582,0.004908636678010225,0.004863590933382511,0.004872548393905163,0.004856523592025042,0.004793310537934303,0.004842466674745083,0.004802817013114691,0.004718990996479988,0.0047739725559949875,0.004656398203223944,0.00471563171595335,0.0047585368156433105,0.004707966931164265,0.004727959632873535,0.0046943482011556625,0.004590922500938177,0.004690596368163824,0.004668306093662977,0.004601103253662586,0.00462481239810586,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 1315      
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 1316      
=================================================================
Total params: 2,631
Trainable params: 2,631
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 32)                160       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 32)                1056      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 99        
=================================================================
Total params: 1,315
Trainable params: 1,315
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 32)                128       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 32)                1056      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 132       
=================================================================
Total params: 1,316
Trainable params: 1,316
Non-trainable params: 0
_________________________________________________________________
