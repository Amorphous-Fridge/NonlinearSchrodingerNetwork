2021-06-26
loss,0.3579995334148407,0.21054308116436005,0.09994015097618103,0.05167606845498085,0.04153965786099434,0.03596973791718483,0.030906669795513153,0.027265727519989014,0.025317862629890442,0.023619960993528366,0.022275280207395554,0.021217787638306618,0.020576423034071922,0.019690681248903275,0.01914716511964798,0.01852170191705227,0.01807594858109951,0.01757056638598442,0.017267974093556404,0.016740823164582253,0.016512518748641014,0.016100136563181877,0.015786655247211456,0.01553824171423912,0.015183351933956146,0.014970545656979084,0.014821847900748253,0.014594651758670807,0.014433176256716251,0.014227752573788166,0.014066471718251705,0.013893160969018936,0.013620316982269287,0.013582642190158367,0.013462705537676811,0.013299358077347279,0.01327067706733942,0.013070029206573963,0.0130086038261652,0.012798110023140907,0.01276926789432764,0.012679032050073147,0.012533052824437618,0.012460626661777496,0.012365060858428478,0.0123407281935215,0.012247500009834766,0.012154225260019302,0.012056793086230755,0.011969262734055519,0.011876280419528484,0.011899891309440136,0.011762316338717937,0.011726178228855133,0.011590910144150257,0.011755202896893024,0.011624935083091259,0.011545216664671898,0.011462455615401268,0.0114039471372962,0.01134685892611742,0.011311117559671402,0.01123658288270235,0.01124916598200798,0.011120923794806004,0.011101655662059784,0.011063462123274803,0.011069709435105324,0.011029538698494434,0.010956848971545696,0.010948244482278824,0.010839714668691158,0.010836528614163399,0.010898743756115437,0.010788832791149616,0.010779136791825294,0.01073890645056963,0.010666701942682266,0.01067758072167635,0.010642558336257935,0.01050903182476759,0.010619336739182472,0.010515663772821426,0.010477554984390736,0.010496065951883793,0.010458710603415966,0.010402982123196125,0.010368755087256432,0.010319766588509083,0.010366763919591904,0.010305937379598618,0.010230239480733871,0.010207041166722775,0.01023196429014206,0.010210514068603516,0.01013464666903019,0.010138078592717648,0.010068576782941818,0.01016398798674345,0.010120543651282787,
mse,0.04399975761771202,0.015581943094730377,0.00406287144869566,0.0010309108765795827,0.0006607441464439034,0.00046947572263889015,0.0003354711225256324,0.0002570294891484082,0.00021804688731208444,0.00018836214439943433,0.0001647286699153483,0.00014905392890796065,0.0001383925264235586,0.00012575052096508443,0.00011830456787720323,0.0001100997906178236,0.00010420820763101801,9.80264157988131e-05,9.4265938969329e-05,8.851663005771115e-05,8.596771658631042e-05,8.13672668300569e-05,7.779953011777252e-05,7.537766941823065e-05,7.193695637397468e-05,6.980283069424331e-05,6.816364475525916e-05,6.611654680455104e-05,6.39726931694895e-05,6.256123742787167e-05,6.124587525846437e-05,5.922248965362087e-05,5.671753388014622e-05,5.6455075537087396e-05,5.521033381228335e-05,5.3944782848702744e-05,5.3756757552037016e-05,5.22207046742551e-05,5.156326005817391e-05,4.98202680319082e-05,4.9510588723933324e-05,4.8814385081641376e-05,4.739883661386557e-05,4.711549627245404e-05,4.638598329620436e-05,4.591214019455947e-05,4.5364740799413994e-05,4.448428444447927e-05,4.365072163636796e-05,4.325310874264687e-05,4.259646448190324e-05,4.2717507312772796e-05,4.1522071114741266e-05,4.12723638874013e-05,4.022340726805851e-05,4.126231942791492e-05,4.061035724589601e-05,3.9904756704345345e-05,3.918266520486213e-05,3.888804349116981e-05,3.849671338684857e-05,3.8295049307635054e-05,3.799925252678804e-05,3.799648038693704e-05,3.69964400306344e-05,3.692008613143116e-05,3.6509485653368756e-05,3.624580494943075e-05,3.629919228842482e-05,3.5725810448639095e-05,3.55467273038812e-05,3.494672637316398e-05,3.4869080991484225e-05,3.522351471474394e-05,3.440387445152737e-05,3.4264143323525786e-05,3.411508441786282e-05,3.363832729519345e-05,3.376071617822163e-05,3.339743125252426e-05,3.2478757930221036e-05,3.3145501220133156e-05,3.249836299801245e-05,3.240459045628086e-05,3.249490328016691e-05,3.227520574000664e-05,3.186300091329031e-05,3.149160329485312e-05,3.12767515424639e-05,3.165634916513227e-05,3.1166076951194555e-05,3.0512805096805096e-05,3.0282699299277738e-05,3.067216312047094e-05,3.0555635021300986e-05,3.0037197575438768e-05,2.9982997148181312e-05,2.990707798744552e-05,3.0250455893110484e-05,2.9863747840863653e-05,
mae,0.15034160017967224,0.08648178726434708,0.041519779711961746,0.02224784530699253,0.017826983705163002,0.015353215858340263,0.013148678466677666,0.011623884551227093,0.01079534087330103,0.01012396439909935,0.009518803097307682,0.009068545885384083,0.00881231389939785,0.008413531817495823,0.008239999413490295,0.007926758378744125,0.007737032603472471,0.007509699556976557,0.0073966956697404385,0.0072036017663776875,0.00704916100949049,0.006882121786475182,0.0067255087196826935,0.0066438764333724976,0.006483282893896103,0.006395723205059767,0.006297933869063854,0.006219958886504173,0.006148744374513626,0.0060932268388569355,0.005950249265879393,0.005967685487121344,0.005850170273333788,0.005797266494482756,0.0057207439094781876,0.005671565420925617,0.0056441109627485275,0.005562787875533104,0.0055564702488482,0.005440400913357735,0.005456607788801193,0.005426784977316856,0.005333207082003355,0.005304237827658653,0.005252797156572342,0.005227609071880579,0.005270734429359436,0.005145771894603968,0.005083711817860603,0.005082753952592611,0.005041365046054125,0.0050571938045322895,0.005036148242652416,0.004992326721549034,0.004950728267431259,0.004996722564101219,0.004894566722214222,0.004859083332121372,0.0049339099787175655,0.004854043014347553,0.004802594892680645,0.004849215503782034,0.004792234394699335,0.004746139049530029,0.0047149816527962685,0.004698360338807106,0.004686559084802866,0.0047098184004426,0.004697517957538366,0.004669541027396917,0.0046296874061226845,0.004637220408767462,0.004525049589574337,0.004599934909492731,0.004575078375637531,0.004600736778229475,0.004564446862787008,0.004499906208366156,0.0045436080545187,0.0045313043519854546,0.004479279741644859,0.004490440711379051,0.004423585254698992,0.00446704588830471,0.0044352952390909195,0.004448354709893465,0.00433503370732069,0.004346719477325678,0.00439056009054184,0.0043201870284974575,0.0043655140325427055,0.004424224607646465,0.004360825289040804,0.004284976050257683,0.004272034857422113,0.004296405240893364,0.004282794427126646,0.004218725487589836,0.0042805783450603485,0.00426235469058156,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 2467      
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 2468      
=================================================================
Total params: 4,935
Trainable params: 4,935
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 32)                160       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 64)                2112      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 195       
=================================================================
Total params: 2,467
Trainable params: 2,467
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 64)                256       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 32)                2080      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 132       
=================================================================
Total params: 2,468
Trainable params: 2,468
Non-trainable params: 0
_________________________________________________________________
