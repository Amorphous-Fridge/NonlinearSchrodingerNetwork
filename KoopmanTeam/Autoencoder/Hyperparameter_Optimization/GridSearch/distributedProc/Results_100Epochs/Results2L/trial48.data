2021-06-26
loss,0.33142948150634766,0.14182524383068085,0.06108968332409859,0.04216278716921806,0.04447838291525841,0.04361964389681816,0.04378627613186836,0.039441417902708054,0.032630547881126404,0.03522825613617897,0.03243311867117882,0.03016117587685585,0.03532982990145683,0.032290030270814896,0.029335174709558487,0.023961419239640236,0.036011241376399994,0.032807543873786926,0.029667574912309647,0.026480289176106453,0.025238048285245895,0.02708878554403782,0.024094698950648308,0.0206842590123415,0.02014373242855072,0.022070221602916718,0.030242756009101868,0.029021941125392914,0.026254141703248024,0.02383214421570301,0.02214096114039421,0.02086600847542286,0.019822023808956146,0.01849995367228985,0.019957398995757103,0.025255780667066574,0.024073131382465363,0.018184708431363106,0.019972825422883034,0.02470420114696026,0.023337289690971375,0.020977571606636047,0.019591502845287323,0.018671955913305283,0.017757240682840347,0.017134161666035652,0.018283013254404068,0.015189401805400848,0.015562096610665321,0.016915732994675636,0.022837504744529724,0.025615058839321136,0.022990578785538673,0.021204298362135887,0.019762057811021805,0.019136613234877586,0.018166175112128258,0.01998726837337017,0.01976221799850464,0.019344398751854897,0.017476368695497513,0.016530664637684822,0.015933847054839134,0.015535636804997921,0.015021801926195621,0.014492474496364594,0.014023962430655956,0.01356663927435875,0.013035411015152931,0.013389140367507935,0.015065767802298069,0.01865369640290737,0.019727595150470734,0.020897414535284042,0.01933855004608631,0.016932247206568718,0.015349533408880234,0.019164321944117546,0.0183661300688982,0.01705152727663517,0.016107339411973953,0.015128896571695805,0.014538261108100414,0.014586802572011948,0.015463250689208508,0.017277011647820473,0.015356507152318954,0.01725311391055584,0.01707596704363823,0.015696149319410324,0.01478674728423357,0.013987728394567966,0.012872915714979172,0.011578039266169071,0.011836476624011993,0.011584983207285404,0.014128230512142181,0.014082283712923527,0.014259835705161095,0.014846120961010456,
mse,0.03943965211510658,0.007762949913740158,0.0012551124673336744,0.0005623639444820583,0.0006053483230061829,0.0005741462227888405,0.0005525571759790182,0.0004584202542901039,0.00031892716651782393,0.00035972759360447526,0.0002966168976854533,0.0002578329585958272,0.0003653163730632514,0.0003024526813533157,0.00025422676117159426,0.00017848852439783514,0.00036881701089441776,0.0003070574894081801,0.0002528260229155421,0.0001989078737096861,0.00019191962201148272,0.00020627824414987117,0.00016699638217687607,0.0001294437242904678,0.00012160539336036891,0.00014003500109538436,0.00025008313241414726,0.0002359475620323792,0.0001909583224914968,0.00015885243192315102,0.00013894157018512487,0.00012424586748238653,0.00011266020010225475,9.987965313484892e-05,0.00012351087934803218,0.0001837753370637074,0.00016319374844897538,9.968365338863805e-05,0.0001168950111605227,0.00016830603999551386,0.00015281476953532547,0.00012080527085345238,0.00010570741869742051,9.735717321746051e-05,8.852250903146341e-05,8.609401265857741e-05,9.868387860478833e-05,6.936475983820856e-05,7.215169898699969e-05,8.476504444843158e-05,0.00014588405610993505,0.00018284581892658025,0.00014858416398055851,0.00012743324623443186,0.00011104775330750272,0.00010419083992019296,9.673021850176156e-05,0.00011168047058163211,0.00010864168143598363,0.00010361711611039937,8.387211710214615e-05,7.617053051944822e-05,7.106827251845971e-05,6.859865243313834e-05,6.410666537703946e-05,5.961985152680427e-05,5.654175402014516e-05,5.251379843684845e-05,4.929132774122991e-05,5.534316733246669e-05,7.04654012224637e-05,0.00010339374421164393,0.00011028114386135712,0.0001200092228827998,0.00010576714703347534,8.459779201075435e-05,6.966732325963676e-05,0.00010356748680351302,9.453811071580276e-05,8.360853098565713e-05,7.233049109345302e-05,6.450882938224822e-05,6.0039106756448746e-05,6.19795682723634e-05,6.986535299802199e-05,8.25662791612558e-05,6.872211815789342e-05,8.520411211065948e-05,8.255876309704036e-05,6.903657777002081e-05,6.083619155106135e-05,5.5617434554733336e-05,4.9802274588728324e-05,4.2252871935488656e-05,4.373809861135669e-05,4.165022255619988e-05,6.088894951972179e-05,5.8829562476603314e-05,5.829025394632481e-05,6.024208778399043e-05,
mae,0.1418067216873169,0.0633995309472084,0.026227157562971115,0.017922259867191315,0.018925447016954422,0.018726252019405365,0.01861535757780075,0.01669977605342865,0.013976487331092358,0.01528753712773323,0.01416870392858982,0.013121843338012695,0.015089579857885838,0.01382733229547739,0.01239794958382845,0.01018325425684452,0.015665030106902122,0.014023225754499435,0.012331319972872734,0.01095335278660059,0.010836143046617508,0.011488735675811768,0.010092895478010178,0.0088012320920825,0.008553232997655869,0.009440615773200989,0.012718900106847286,0.012177764438092709,0.01090268138796091,0.010079221799969673,0.009342539124190807,0.008864670060575008,0.008456318639218807,0.007837923243641853,0.008347599767148495,0.010732494294643402,0.010036390274763107,0.007738042622804642,0.008499961346387863,0.010516630485653877,0.009925457648932934,0.008968639187514782,0.00852519366890192,0.008131447248160839,0.007699336390942335,0.007317211478948593,0.00783920381218195,0.00646096421405673,0.006639830302447081,0.007160448934882879,0.00965676549822092,0.010719788260757923,0.00951232761144638,0.009030458517372608,0.008406588807702065,0.008066161535680294,0.007670381106436253,0.008440033532679081,0.008311174809932709,0.008072865195572376,0.007375889457762241,0.007041145581752062,0.00680781714618206,0.006651778239756823,0.006376626435667276,0.006133376155048609,0.005877990275621414,0.0056506572291255,0.005481639411300421,0.005741545930504799,0.006390165071934462,0.007865536026656628,0.00823004636913538,0.008775177411735058,0.00809531006962061,0.006999790668487549,0.006471579894423485,0.00811038725078106,0.00772078987210989,0.007350152358412743,0.006751251872628927,0.006247710436582565,0.005883235950022936,0.006092546507716179,0.006488035898655653,0.007218679878860712,0.006512532010674477,0.007301567122340202,0.00721342209726572,0.006462204270064831,0.006223846692591906,0.005873951129615307,0.005572405643761158,0.004989088978618383,0.0049986992962658405,0.004942834377288818,0.006110547110438347,0.005932436790317297,0.006095947232097387,0.006340880878269672,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 21571     
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 21572     
=================================================================
Total params: 43,143
Trainable params: 43,143
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 1024)              5120      
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 16)                16400     
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 51        
=================================================================
Total params: 21,571
Trainable params: 21,571
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 16)                64        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 1024)              17408     
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 4100      
=================================================================
Total params: 21,572
Trainable params: 21,572
Non-trainable params: 0
_________________________________________________________________
