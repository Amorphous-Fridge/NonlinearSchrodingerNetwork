2021-06-26
loss,4.411290168762207,4.424264907836914,4.4241838455200195,4.424347400665283,4.424081325531006,4.4140496253967285,4.424114227294922,4.424466133117676,4.424398422241211,4.423989772796631,4.424266815185547,4.424126148223877,4.424193382263184,4.424046516418457,4.424317836761475,4.42422342300415,4.424267768859863,4.424432754516602,4.424078464508057,4.424423694610596,4.424160003662109,4.4241437911987305,4.424071788787842,4.424165725708008,4.424149990081787,4.424168586730957,4.424172401428223,4.424080848693848,4.424073219299316,4.424316883087158,4.423964023590088,4.42430305480957,4.424383163452148,4.424140930175781,4.424349784851074,4.424232482910156,4.424261569976807,4.424010753631592,4.424063205718994,4.424355983734131,4.424467086791992,4.424192428588867,4.4242401123046875,4.424056529998779,4.547015190124512,5.824188232421875,4.424093246459961,4.4243316650390625,4.424248695373535,4.424153804779053,4.423852920532227,4.424389839172363,4.424220561981201,4.423965930938721,4.4246416091918945,4.424332141876221,4.423996925354004,4.424102783203125,4.423945426940918,4.424155235290527,4.424220085144043,4.4241108894348145,4.424165725708008,4.424176216125488,4.424540996551514,4.424363136291504,4.424402713775635,4.424290180206299,4.423986434936523,4.424231052398682,4.4242658615112305,4.424189567565918,4.424383163452148,4.4240031242370605,4.424114227294922,4.42416524887085,4.4240288734436035,4.424330234527588,4.424409866333008,4.424143314361572,4.424304962158203,4.424222946166992,4.424164295196533,4.4240217208862305,4.424051761627197,4.424197196960449,4.424335479736328,4.424300670623779,4.424256801605225,4.424075603485107,4.424219608306885,4.424036979675293,4.423957347869873,4.423990726470947,4.424273490905762,4.424095630645752,4.424169540405273,4.424116611480713,4.424061298370361,4.424304962158203,
mse,4.948059558868408,4.894423961639404,4.8942461013793945,4.894598484039307,4.894021034240723,4.87275505065918,4.894094944000244,4.894866943359375,4.894719123840332,4.893819808959961,4.894428253173828,4.894120216369629,4.894266128540039,4.89393949508667,4.89454460144043,4.89433479309082,4.894430637359619,4.894794940948486,4.894014358520508,4.894772529602051,4.894191741943359,4.894152641296387,4.893998146057129,4.894203186035156,4.8941731452941895,4.894213676452637,4.894221305847168,4.894014835357666,4.89399528503418,4.894537448883057,4.8937602043151855,4.8945088386535645,4.894683837890625,4.894153118133545,4.894613265991211,4.894347667694092,4.894411087036133,4.893861770629883,4.893977165222168,4.894622802734375,4.894868850708008,4.8942646980285645,4.894369125366211,4.893959999084473,5.540102481842041,18.716976165771484,4.894049644470215,4.8945698738098145,4.8943915367126465,4.894176959991455,4.8935160636901855,4.894699573516846,4.894331455230713,4.893760681152344,4.895247936248779,4.894568920135498,4.893835067749023,4.894068717956543,4.893723964691162,4.894189357757568,4.89432430267334,4.894082069396973,4.894205093383789,4.894224643707275,4.8950300216674805,4.894638538360596,4.8947224617004395,4.8944783210754395,4.89381217956543,4.894347190856934,4.894423007965088,4.894251346588135,4.894683837890625,4.89384651184082,4.894095420837402,4.894198417663574,4.893902778625488,4.894570350646973,4.894735336303711,4.894159317016602,4.89451265335083,4.894329071044922,4.894201755523682,4.893890857696533,4.893949031829834,4.8942742347717285,4.89457893371582,4.894502639770508,4.894403457641602,4.894005298614502,4.894321918487549,4.893919467926025,4.893741607666016,4.893819808959961,4.894444942474365,4.8940510749816895,4.89421272277832,4.894105911254883,4.893977165222168,4.8945088386535645,
mae,2.1807861328125,2.1999168395996094,2.199866533279419,2.1999683380126953,2.1998231410980225,2.193981170654297,2.1998207569122314,2.200040578842163,2.199998378753662,2.199742555618286,2.199915885925293,2.1998283863067627,2.1998701095581055,2.1997768878936768,2.199949264526367,2.1998894214630127,2.1999168395996094,2.2000200748443604,2.199798345565796,2.200014114379883,2.1998486518859863,2.1998374462127686,2.1997933387756348,2.1998515129089355,2.199843168258667,2.199854850769043,2.199857473373413,2.199798345565796,2.1997928619384766,2.199946880340576,2.199726104736328,2.199939012527466,2.19998836517334,2.1998376846313477,2.1999683380126953,2.1998932361602783,2.199911117553711,2.1997554302215576,2.1997880935668945,2.199972152709961,2.2000415325164795,2.1998696327209473,2.1999001502990723,2.199800729751587,2.247363328933716,2.6218366622924805,2.199808120727539,2.199955940246582,2.1999056339263916,2.1998445987701416,2.1996567249298096,2.19999361038208,2.199887752532959,2.199725866317749,2.2001492977142334,2.199955940246582,2.199747085571289,2.1998136043548584,2.1997156143188477,2.199847936630249,2.1998865604400635,2.199817419052124,2.199852705001831,2.1998579502105713,2.200087070465088,2.1999757289886475,2.1999998092651367,2.199930429458618,2.1997408866882324,2.1998932361602783,2.19991397857666,2.19986629486084,2.199988603591919,2.1997504234313965,2.1998212337493896,2.1998510360717773,2.1997663974761963,2.1999564170837402,2.2000036239624023,2.1998395919799805,2.1999399662017822,2.199887752532959,2.1998517513275146,2.199763298034668,2.199779510498047,2.1998724937438965,2.1999588012695312,2.199937343597412,2.199908494949341,2.1997952461242676,2.1998863220214844,2.199770927429199,2.199720621109009,2.1997430324554443,2.199920415878296,2.1998090744018555,2.1998543739318848,2.199824094772339,2.1997873783111572,2.1999387741088867,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 296515    
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 296516    
=================================================================
Total params: 593,031
Trainable params: 593,031
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 64)                320       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 256)               16640     
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 512)               131584    
_________________________________________________________________
encoding_layer_4 (Dense)     (None, 256)               131328    
_________________________________________________________________
encoding_layer_5 (Dense)     (None, 64)                16448     
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 195       
=================================================================
Total params: 296,515
Trainable params: 296,515
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 64)                256       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 256)               16640     
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 512)               131584    
_________________________________________________________________
decoding_layer_4 (Dense)     (None, 256)               131328    
_________________________________________________________________
decoding_layer_5 (Dense)     (None, 64)                16448     
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 260       
=================================================================
Total params: 296,516
Trainable params: 296,516
Non-trainable params: 0
_________________________________________________________________
