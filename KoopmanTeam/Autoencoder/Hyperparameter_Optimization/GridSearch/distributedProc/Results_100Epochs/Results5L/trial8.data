2021-06-26
loss,0.24759148061275482,0.07968838512897491,0.04986954107880592,0.04107106849551201,0.03672437742352486,0.034377094358205795,0.03210151195526123,0.03069225512444973,0.029517145827412605,0.028150023892521858,0.027431076392531395,0.026518909260630608,0.025566430762410164,0.025105755776166916,0.024433044716715813,0.02392173744738102,0.023472463712096214,0.022959940135478973,0.022877926006913185,0.02252569980919361,0.021944232285022736,0.02182689495384693,0.021252920851111412,0.02121654339134693,0.020903725177049637,0.020731715485453606,0.020421957597136497,0.020154060795903206,0.019804587587714195,0.019760573282837868,0.019606618210673332,0.019340135157108307,0.019191164523363113,0.019093669950962067,0.018979832530021667,0.01872006617486477,0.018509194254875183,0.018413398414850235,0.018343910574913025,0.0181271992623806,0.018052563071250916,0.017704568803310394,0.017638592049479485,0.01763829030096531,0.017482975497841835,0.01727403700351715,0.017063377425074577,0.017253126949071884,0.016980353742837906,0.016855817288160324,0.016695447266101837,0.01668260060250759,0.016578136011958122,0.016497544944286346,0.016428204253315926,0.01629372499883175,0.016209186986088753,0.016127154231071472,0.01609273999929428,0.015881748870015144,0.015776311978697777,0.015762129798531532,0.015717769041657448,0.015645714476704597,0.01559341512620449,0.015463531948626041,0.015439287759363651,0.015344176441431046,0.01534796878695488,0.01522049866616726,0.015032593160867691,0.015112784691154957,0.014917617663741112,0.014956895262002945,0.014915328472852707,0.014777137897908688,0.014732538722455502,0.014601444825530052,0.014639314264059067,0.014618868939578533,0.014516646973788738,0.01442975364625454,0.014375543221831322,0.014238664880394936,0.01438154187053442,0.01425428967922926,0.01430701743811369,0.014109031297266483,0.014163486659526825,0.01407745759934187,0.013940539211034775,0.013976836577057838,0.013872398994863033,0.013885761611163616,0.013720729388296604,0.013822666369378567,0.013664736412465572,0.013852911069989204,0.013707231730222702,0.013647091574966908,
mse,0.024275703355669975,0.0021653214935213327,0.0008000208181329072,0.0005314085865393281,0.0004191812768112868,0.00036594978882931173,0.00031746376771479845,0.00028674729401245713,0.0002648770168889314,0.00024029443738982081,0.00022683314455207437,0.00021124098566360772,0.0001965777191799134,0.00018916452245321125,0.0001783894986147061,0.00017082160047721118,0.0001638494577491656,0.00015664700185880065,0.000154596971697174,0.00015022420848254114,0.00014275609282776713,0.0001402942289132625,0.00013360840966925025,0.00013304228195920587,0.00012840221461374313,0.00012646120740100741,0.00012283067917451262,0.00011908781016245484,0.00011534600344020873,0.00011430097947595641,0.00011240408639423549,0.00010977585770888254,0.00010762058809632435,0.000106205785414204,0.0001052183288265951,0.00010231955093331635,9.99258627416566e-05,9.888130443869159e-05,9.806667367229238e-05,9.56182848312892e-05,9.477411367697641e-05,9.152465645456687e-05,9.071252134162933e-05,9.007292828755453e-05,8.886928844731301e-05,8.664028428029269e-05,8.453258487861603e-05,8.642527973279357e-05,8.380296640098095e-05,8.256697765318677e-05,8.064381836447865e-05,8.10211495263502e-05,7.994413317646831e-05,7.911760621936992e-05,7.812405965523794e-05,7.682255818508565e-05,7.592969632241875e-05,7.51274565118365e-05,7.499080675188452e-05,7.310508954105899e-05,7.187234587036073e-05,7.183286652434617e-05,7.157379877753556e-05,7.069392449920997e-05,7.031730638118461e-05,6.89266889821738e-05,6.868129275972024e-05,6.811856292188168e-05,6.770771142328158e-05,6.689848669338971e-05,6.533550913445652e-05,6.591738201677799e-05,6.403650331776589e-05,6.425854371627793e-05,6.417800614144653e-05,6.312347250059247e-05,6.257602944970131e-05,6.138009484857321e-05,6.171191489556804e-05,6.141727499198169e-05,6.086727444198914e-05,5.978405897621997e-05,5.976039756205864e-05,5.826761116622947e-05,5.967867036815733e-05,5.8765966969076544e-05,5.8910845837090164e-05,5.720730041502975e-05,5.752844299422577e-05,5.694265564670786e-05,5.602199598797597e-05,5.6373643019469455e-05,5.545687963603996e-05,5.5452004744438455e-05,5.431628233054653e-05,5.481010157382116e-05,5.365531251300126e-05,5.5169326515169814e-05,5.418566433945671e-05,5.3533123718807474e-05,
mae,0.10561921447515488,0.033895719796419144,0.021290166303515434,0.017538534477353096,0.015636222437024117,0.014658931642770767,0.01366607565432787,0.01306877750903368,0.012584500014781952,0.012018539011478424,0.011691628955304623,0.01131522562354803,0.01088335644453764,0.01074046641588211,0.010433287359774113,0.010206027887761593,0.01000568550080061,0.009788994677364826,0.009752459824085236,0.009617198258638382,0.009359177201986313,0.009304908104240894,0.009014969691634178,0.009030744433403015,0.008924003690481186,0.008863049559295177,0.008710375055670738,0.008648935705423355,0.008432342670857906,0.008382762782275677,0.008352420292794704,0.008247881196439266,0.00815836526453495,0.00815231166779995,0.008108504116535187,0.00797036848962307,0.007846653461456299,0.007874369621276855,0.0077918702736496925,0.007775455713272095,0.007680664770305157,0.007515053264796734,0.007490320596843958,0.007527706678956747,0.007463925052434206,0.007375714369118214,0.007249378599226475,0.007393773179501295,0.007178873289376497,0.0071751330979168415,0.007116583175957203,0.007086249068379402,0.007045132573693991,0.007052979897707701,0.0069737862795591354,0.006991280242800713,0.0069108279421925545,0.006877512205392122,0.006862051319330931,0.006798832211643457,0.006730871275067329,0.006696024443954229,0.006733892485499382,0.006637491751462221,0.006635785102844238,0.006587984971702099,0.006570182275027037,0.006487554404884577,0.006531489547342062,0.0065145655535161495,0.006401857826858759,0.006448647473007441,0.0063494169153273106,0.0063766553066670895,0.0063856556080281734,0.006291407160460949,0.006290378049015999,0.006258638110011816,0.006281936541199684,0.006174254231154919,0.006250785198062658,0.006178365554660559,0.006083718501031399,0.006080025341361761,0.0061165462248027325,0.0060526966117322445,0.006071743555366993,0.0060221171006560326,0.006063440814614296,0.005995079409331083,0.005933674983680248,0.005888677202165127,0.005965325981378555,0.005924972239881754,0.005837630480527878,0.005877715535461903,0.0057773105800151825,0.005929897539317608,0.005815632175654173,0.005856689065694809,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 4587      
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 4588      
=================================================================
Total params: 9,175
Trainable params: 9,175
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 8)                 40        
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 16)                144       
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 128)               2176      
_________________________________________________________________
encoding_layer_4 (Dense)     (None, 16)                2064      
_________________________________________________________________
encoding_layer_5 (Dense)     (None, 8)                 136       
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 27        
=================================================================
Total params: 4,587
Trainable params: 4,587
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 8)                 32        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 16)                144       
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 128)               2176      
_________________________________________________________________
decoding_layer_4 (Dense)     (None, 16)                2064      
_________________________________________________________________
decoding_layer_5 (Dense)     (None, 8)                 136       
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 36        
=================================================================
Total params: 4,588
Trainable params: 4,588
Non-trainable params: 0
_________________________________________________________________
