2021-06-26
loss,3.531480073928833,2.9375033378601074,2.237661600112915,2.2227697372436523,2.221547842025757,2.2227563858032227,2.219858169555664,2.2222533226013184,2.221043586730957,2.2214884757995605,2.2195520401000977,2.217390775680542,2.219907760620117,2.3149826526641846,2.2234504222869873,0.6603394150733948,0.4460381269454956,0.4448821246623993,0.44492027163505554,0.44426825642585754,0.44480571150779724,0.44512900710105896,0.44450733065605164,0.44528111815452576,0.444817453622818,0.4445269703865051,0.44463101029396057,0.44498035311698914,0.4446560740470886,0.4450784921646118,0.4449647068977356,0.44514286518096924,0.4451506733894348,0.4444173574447632,0.4455248713493347,0.4451219439506531,0.4457055628299713,0.4448980987071991,0.4447305202484131,0.4536498785018921,0.44676825404167175,0.44533833861351013,0.44516173005104065,0.44510355591773987,0.4447877109050751,0.44515886902809143,0.4450135827064514,0.4456903040409088,0.4445750415325165,0.4447188973426819,0.44503945112228394,0.4457486867904663,0.4452827572822571,0.44535884261131287,0.44540566205978394,0.4455051124095917,0.4460688531398773,0.44631263613700867,0.44564583897590637,0.4456460177898407,0.44551292061805725,0.4455025792121887,0.44625380635261536,0.4454614520072937,0.44597819447517395,0.44524818658828735,0.44505131244659424,0.44545218348503113,0.4458991587162018,0.4450816810131073,0.44550490379333496,0.44520628452301025,0.44478970766067505,0.4449005424976349,0.44498616456985474,0.4451451599597931,0.4452892541885376,0.44533807039260864,0.4447392523288727,0.4457401633262634,0.4456575810909271,0.4458751976490021,0.4461047351360321,0.44558948278427124,0.4450169801712036,0.44495826959609985,0.4457020163536072,0.4445725381374359,0.4447006583213806,0.44457724690437317,0.4452103078365326,0.44470569491386414,0.4448355734348297,0.4452466070652008,0.4450119137763977,0.44529736042022705,0.4454677104949951,0.44562602043151855,0.4449799358844757,0.44525161385536194,
mse,3.1905384063720703,2.1970174312591553,1.2650662660598755,1.2486271858215332,1.2473554611206055,1.2485824823379517,1.245452880859375,1.2481094598770142,1.2467402219772339,1.2473111152648926,1.2451684474945068,1.2428122758865356,1.2455323934555054,1.358425498008728,1.3408070802688599,0.13367293775081635,0.055445410311222076,0.05515207350254059,0.05513520538806915,0.055019818246364594,0.05514686554670334,0.05519133433699608,0.055066727101802826,0.05524088442325592,0.05514056980609894,0.055066704750061035,0.05510011315345764,0.05517946556210518,0.05508362874388695,0.055180586874485016,0.05518180504441261,0.05522116273641586,0.05521399527788162,0.0550525039434433,0.05528458580374718,0.05522669851779938,0.05533153563737869,0.05515787750482559,0.05510120093822479,0.057606231421232224,0.055607870221138,0.055285338312387466,0.05521291121840477,0.055224765092134476,0.05512561276555061,0.055216241627931595,0.0551835261285305,0.05533411353826523,0.05509158968925476,0.05513572320342064,0.05522533506155014,0.05534778907895088,0.0552368201315403,0.05529100075364113,0.05529039353132248,0.05532003566622734,0.05543963611125946,0.05547082796692848,0.05533325672149658,0.055346228182315826,0.05529847741127014,0.055300984531641006,0.05546288937330246,0.05531897023320198,0.055398885160684586,0.05523419752717018,0.0552268885076046,0.05533638969063759,0.055409979075193405,0.05518479645252228,0.05530808866024017,0.055270519107580185,0.05516042560338974,0.05516253039240837,0.05516372248530388,0.05522606894373894,0.05524751543998718,0.055296093225479126,0.055170223116874695,0.05538201704621315,0.055350884795188904,0.055354319512844086,0.05546313151717186,0.05529768764972687,0.055197346955537796,0.0551646463572979,0.055346179753541946,0.05511065572500229,0.05510769784450531,0.05508212000131607,0.05525442585349083,0.0551312118768692,0.05513216182589531,0.05523034557700157,0.0551854744553566,0.05524684488773346,0.055274251848459244,0.05533389747142792,0.05517026036977768,0.05525083839893341,
mae,1.5615774393081665,1.1076487302780151,0.6932200789451599,0.6589179039001465,0.6532190442085266,0.658847451210022,0.6520669460296631,0.6545315384864807,0.6521626114845276,0.6523644328117371,0.652273952960968,0.6438535451889038,0.653195858001709,0.7834919095039368,0.7406572699546814,0.28468137979507446,0.19777624309062958,0.19736118614673615,0.1973351240158081,0.19709450006484985,0.19729258120059967,0.19750504195690155,0.19722366333007812,0.19749735295772552,0.19729284942150116,0.1971810758113861,0.19721448421478271,0.19732826948165894,0.197241872549057,0.19741827249526978,0.197405606508255,0.19743898510932922,0.19745749235153198,0.1971389800310135,0.1976294368505478,0.19750246405601501,0.19773153960704803,0.19731466472148895,0.1972481906414032,0.20061321556568146,0.1980663686990738,0.19748596847057343,0.19740839302539825,0.19742511212825775,0.19729572534561157,0.19744516909122467,0.19736269116401672,0.19763058423995972,0.1971675157546997,0.19711992144584656,0.19732597470283508,0.1976659595966339,0.19752910733222961,0.1974492222070694,0.19748392701148987,0.19762077927589417,0.19777517020702362,0.19787278771400452,0.19769322872161865,0.19757717847824097,0.19759513437747955,0.19750472903251648,0.1978832632303238,0.19760411977767944,0.19785183668136597,0.19742263853549957,0.19734543561935425,0.19751733541488647,0.19770783185958862,0.19738203287124634,0.19756798446178436,0.19750599563121796,0.19727963209152222,0.19730216264724731,0.19739839434623718,0.19746683537960052,0.19751662015914917,0.19750985503196716,0.1972115933895111,0.1976832002401352,0.1976097822189331,0.19774295389652252,0.19784225523471832,0.19764111936092377,0.19739775359630585,0.1974136382341385,0.19768032431602478,0.19718962907791138,0.19727985560894012,0.19720153510570526,0.1975470930337906,0.19723735749721527,0.19729094207286835,0.1974474936723709,0.1974537968635559,0.19757647812366486,0.19758851826190948,0.197635218501091,0.19737882912158966,0.19747118651866913,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 313219    
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 313220    
=================================================================
Total params: 626,439
Trainable params: 626,439
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 128)               640       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 256)               33024     
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 512)               131584    
_________________________________________________________________
encoding_layer_4 (Dense)     (None, 256)               131328    
_________________________________________________________________
encoding_layer_5 (Dense)     (None, 64)                16448     
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 195       
=================================================================
Total params: 313,219
Trainable params: 313,219
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 64)                256       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 256)               16640     
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 512)               131584    
_________________________________________________________________
decoding_layer_4 (Dense)     (None, 256)               131328    
_________________________________________________________________
decoding_layer_5 (Dense)     (None, 128)               32896     
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 516       
=================================================================
Total params: 313,220
Trainable params: 313,220
Non-trainable params: 0
_________________________________________________________________
