2021-06-26
loss,4.37455940246582,3.914038896560669,3.8324739933013916,3.8294684886932373,3.8296892642974854,3.8286612033843994,3.829291343688965,3.8294315338134766,3.829585075378418,3.8289055824279785,3.8294057846069336,3.8287415504455566,3.829132080078125,3.8289759159088135,3.8290090560913086,3.829296827316284,3.8297111988067627,3.8294501304626465,3.829252004623413,3.8295671939849854,3.8286523818969727,3.829490900039673,3.829080104827881,3.8286147117614746,3.8292715549468994,4.266039848327637,4.920973300933838,4.423898696899414,4.424269676208496,4.424304008483887,4.4243059158325195,4.424283504486084,4.424218654632568,4.424249172210693,4.424251556396484,4.424206256866455,4.424281597137451,4.424208164215088,4.424126148223877,4.424292087554932,4.424239158630371,4.42420768737793,4.424286365509033,4.4243483543396,4.424444675445557,4.423939228057861,4.424407482147217,4.424177646636963,4.424210071563721,4.424300670623779,4.424190998077393,4.424162864685059,4.424175262451172,4.424254894256592,4.4239912033081055,4.424142360687256,4.4242095947265625,4.424315929412842,4.424279689788818,4.424276351928711,4.424238681793213,4.424240589141846,4.665318489074707,4.423978805541992,4.424249172210693,4.424358367919922,4.424267292022705,4.424280643463135,4.424227714538574,4.4242472648620605,4.424368381500244,4.424036979675293,4.424135208129883,4.424363136291504,4.4242119789123535,4.424080848693848,4.424228191375732,4.424193382263184,4.424164295196533,4.424192905426025,4.4242448806762695,4.42435884475708,4.424178123474121,4.424136161804199,4.42430305480957,4.423946857452393,4.424293041229248,4.42408561706543,4.424062728881836,4.424427032470703,4.424173355102539,4.424158573150635,4.424291133880615,4.424099922180176,4.424035549163818,4.424238204956055,4.424156665802002,4.424318313598633,4.424088478088379,4.424304008483887,
mse,4.887326240539551,3.8394815921783447,3.677197217941284,3.6714670658111572,3.6719088554382324,3.6699750423431396,3.6711580753326416,3.671407461166382,3.671687364578247,3.6704328060150146,3.671358108520508,3.670118808746338,3.670846939086914,3.6705574989318848,3.6706149578094482,3.671161413192749,3.6719326972961426,3.6714301109313965,3.671064853668213,3.671694040298462,3.6699705123901367,3.671518325805664,3.6707563400268555,3.669926166534424,3.6711266040802,5.536972522735596,7.841550827026367,4.893612861633301,4.894438743591309,4.894506931304932,4.894507884979248,4.894463539123535,4.894323825836182,4.894384860992432,4.894392013549805,4.8942952156066895,4.894466876983643,4.894299507141113,4.894113540649414,4.894481658935547,4.89436674118042,4.894301891326904,4.894467353820801,4.8946003913879395,4.894813060760498,4.893706798553467,4.894741058349609,4.894230842590332,4.894296646118164,4.89450216293335,4.894256114959717,4.894198894500732,4.894219875335693,4.894400596618652,4.8938212394714355,4.8941545486450195,4.894299507141113,4.894527435302734,4.894448757171631,4.894443511962891,4.894365310668945,4.894364833831787,6.228723526000977,4.893792629241943,4.894388198852539,4.8946213722229,4.8944268226623535,4.8944549560546875,4.894342422485352,4.894384860992432,4.894650936126709,4.893918991088867,4.89414119720459,4.894637584686279,4.894306182861328,4.894014835357666,4.894340991973877,4.894273281097412,4.894201278686523,4.894261360168457,4.894382953643799,4.894629001617432,4.8942365646362305,4.894137382507324,4.894506931304932,4.893722057342529,4.894484996795654,4.894028186798096,4.893980979919434,4.894781589508057,4.894219875335693,4.894194602966309,4.894479274749756,4.894060134887695,4.893915176391602,4.894364833831787,4.894184112548828,4.894543170928955,4.894036769866943,4.894512176513672,
mae,2.1443917751312256,1.8035598993301392,1.677086591720581,1.655455470085144,1.6538833379745483,1.652779221534729,1.6526771783828735,1.6524614095687866,1.6522945165634155,1.6518138647079468,1.6518681049346924,1.65144944190979,1.651525616645813,1.651350736618042,1.651276707649231,1.6513534784317017,1.6514232158660889,1.651261568069458,1.6511311531066895,1.6512101888656616,1.6507717370986938,1.6510627269744873,1.6508897542953491,1.6506850719451904,1.651329755783081,1.9440181255340576,2.3813703060150146,2.1996841430664062,2.1999192237854004,2.1999382972717285,2.1999387741088867,2.1999258995056152,2.1998863220214844,2.1999032497406006,2.1999056339263916,2.199878215789795,2.19992733001709,2.1998796463012695,2.19982647895813,2.1999311447143555,2.1998982429504395,2.1998798847198486,2.199927568435669,2.199965000152588,2.2000255584716797,2.1997110843658447,2.200004816055298,2.199859857559204,2.1998789310455322,2.199937105178833,2.1998674869537354,2.1998507976531982,2.199856996536255,2.1999080181121826,2.1997427940368652,2.1998379230499268,2.1998794078826904,2.199944257736206,2.1999218463897705,2.199920415878296,2.1998987197875977,2.1998984813690186,2.262441635131836,2.199734687805176,2.199904680252075,2.1999711990356445,2.1999154090881348,2.1999237537384033,2.1998918056488037,2.199903726577759,2.199979782104492,2.199770927429199,2.1998348236083984,2.1999759674072266,2.199881076812744,2.199798345565796,2.1998910903930664,2.199871778488159,2.1998510360717773,2.1998684406280518,2.1999030113220215,2.199972629547119,2.1998610496520996,2.1998331546783447,2.1999382972717285,2.1997148990631104,2.199932336807251,2.1998019218444824,2.1997885704040527,2.2000174522399902,2.1998565196990967,2.1998491287231445,2.19992995262146,2.1998112201690674,2.199769973754883,2.1998977661132812,2.1998462677001953,2.19994854927063,2.1998043060302734,2.199939727783203,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 329859    
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 329860    
=================================================================
Total params: 659,719
Trainable params: 659,719
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 128)               640       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 256)               33024     
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 512)               131584    
_________________________________________________________________
encoding_layer_4 (Dense)     (None, 256)               131328    
_________________________________________________________________
encoding_layer_5 (Dense)     (None, 128)               32896     
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 387       
=================================================================
Total params: 329,859
Trainable params: 329,859
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 128)               512       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 256)               33024     
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 512)               131584    
_________________________________________________________________
decoding_layer_4 (Dense)     (None, 256)               131328    
_________________________________________________________________
decoding_layer_5 (Dense)     (None, 128)               32896     
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 516       
=================================================================
Total params: 329,860
Trainable params: 329,860
Non-trainable params: 0
_________________________________________________________________
