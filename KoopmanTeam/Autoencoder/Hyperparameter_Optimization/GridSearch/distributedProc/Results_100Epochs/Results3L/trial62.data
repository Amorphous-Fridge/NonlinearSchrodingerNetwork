2021-06-26
loss,0.3512226343154907,0.17905935645103455,0.10159499198198318,0.06416257470846176,0.0513094887137413,0.05846647918224335,0.057331278920173645,0.040726881474256516,0.041710324585437775,0.03593830019235611,0.049869075417518616,0.051711272448301315,0.045814938843250275,0.040498241782188416,0.04013269022107124,0.041810713708400726,0.039854563772678375,0.03295058384537697,0.02918965183198452,0.026988904923200607,0.026150953024625778,0.03459622710943222,0.03542102500796318,0.03292776644229889,0.03569649159908295,0.033242564648389816,0.030161919072270393,0.027244383469223976,0.025863025337457657,0.019587842747569084,0.017247116193175316,0.01682385243475437,0.01657673716545105,0.01621095836162567,0.016260776668787003,0.01603231206536293,0.015896443277597427,0.0158198531717062,0.01570228859782219,0.01554939616471529,0.015471620485186577,0.015414770692586899,0.015238954685628414,0.015299376100301743,0.015074746683239937,0.015136498026549816,0.015016844496130943,0.014913314953446388,0.014803571626543999,0.014766237698495388,0.014573502354323864,0.014525255188345909,0.014447575435042381,0.014418567530810833,0.014307285659015179,0.014169064350426197,0.014187615364789963,0.014084813185036182,0.013956128619611263,0.013942119665443897,0.013828213326632977,0.013751102611422539,0.013758320361375809,0.013701438903808594,0.013642732053995132,0.013496306724846363,0.013435883447527885,0.01336379162967205,0.013236086815595627,0.013219594024121761,0.013061807490885258,0.01313877385109663,0.013045590370893478,0.012908821925520897,0.012957997620105743,0.012784804217517376,0.012803827412426472,0.012782732024788857,0.012653006240725517,0.012611610814929008,0.012487036176025867,0.012516901828348637,0.012448625639081001,0.012309555895626545,0.012294161133468151,0.012182818725705147,0.012196331284940243,0.012164836749434471,0.012079493142664433,0.012021568603813648,0.011962294578552246,0.011975198052823544,0.01188950426876545,0.011864782311022282,0.018736083060503006,0.019771523773670197,0.01726478524506092,0.018233753740787506,0.020901739597320557,0.020454375073313713,
mse,0.042704157531261444,0.011167301796376705,0.0032676486298441887,0.0012502778554335237,0.0007999900262802839,0.0010840225731953979,0.001005251077003777,0.0005017360672354698,0.0005245448555797338,0.00037905731005594134,0.0007429532706737518,0.0007734285318292677,0.0006002785521559417,0.0004628576571121812,0.0004533337487373501,0.0004895619349554181,0.0004533075261861086,0.0003044067998416722,0.0002387978893239051,0.00020719491294585168,0.00019340649305377156,0.0003543388447724283,0.00036732834996655583,0.00030977753340266645,0.00036303006345406175,0.0003055883862543851,0.0002519122208468616,0.00020951706392224878,0.00019188084115739912,0.00011196345440112054,8.519017865182832e-05,8.042655827011913e-05,7.833247946109623e-05,7.548384746769443e-05,7.557153730886057e-05,7.309281500056386e-05,7.133431063266471e-05,7.058522169245407e-05,6.916259007994086e-05,6.825556192779914e-05,6.738185038557276e-05,6.688340363325551e-05,6.666993431281298e-05,6.582221249118447e-05,6.579581531696022e-05,6.39670470263809e-05,6.321233377093449e-05,6.224002572707832e-05,6.157577445264906e-05,6.121690239524469e-05,5.940648406976834e-05,5.9431895351735875e-05,5.887999213882722e-05,5.8887777413474396e-05,5.692891136277467e-05,5.641351526719518e-05,5.647107900585979e-05,5.547754335566424e-05,5.4170905059436336e-05,5.425213748821989e-05,5.458794112200849e-05,5.262011109152809e-05,5.393945684772916e-05,5.3225288866087794e-05,5.138046253705397e-05,5.075048829894513e-05,5.0185310101369396e-05,4.978871584171429e-05,4.94431697006803e-05,4.884996815235354e-05,4.849293691222556e-05,4.905671812593937e-05,4.798836380359717e-05,4.641271152650006e-05,4.6826367906760424e-05,4.56096458947286e-05,4.653235009755008e-05,4.5787095587002113e-05,4.5375483750831336e-05,4.433593130670488e-05,4.376035576569848e-05,4.408534005051479e-05,4.296014230931178e-05,4.21569639001973e-05,4.2487678001634777e-05,4.1932555177481845e-05,4.253397855791263e-05,4.1428567783441395e-05,4.114579496672377e-05,4.0545804949942976e-05,4.0194729081122205e-05,4.011241253465414e-05,3.9790986193111166e-05,4.011358760180883e-05,0.00010539650247665122,0.00010841149924090132,8.61834196257405e-05,9.888582280837e-05,0.0001250485802302137,0.00012066428462276235,
mae,0.14876504242420197,0.07149304449558258,0.042345836758613586,0.027188068255782127,0.021814599633216858,0.024803942069411278,0.024174120277166367,0.017328601330518723,0.01780056394636631,0.015114836394786835,0.021154329180717468,0.02212563343346119,0.019758151844143867,0.017271138727664948,0.01701858453452587,0.01783478818833828,0.016833417117595673,0.014125167392194271,0.012765610590577126,0.011787963099777699,0.011334437876939774,0.014781580306589603,0.014917577616870403,0.01416691206395626,0.014956098049879074,0.01377367228269577,0.012405035085976124,0.011823444627225399,0.01112255360931158,0.008289791643619537,0.007206269074231386,0.0071218591183424,0.006967674009501934,0.0068474579602479935,0.006823203060775995,0.006767279002815485,0.006689044646918774,0.00669157924130559,0.0066675711423158646,0.0065979743376374245,0.00651672063395381,0.006496617570519447,0.006478579249233007,0.006438429933041334,0.006382980849593878,0.006434527691453695,0.0063275969587266445,0.006241444498300552,0.006280976347625256,0.006247946992516518,0.0061895716935396194,0.006088199559599161,0.006198163144290447,0.00610993942245841,0.006060915999114513,0.0059992047026753426,0.005961671005934477,0.005941902287304401,0.005890253931283951,0.005886752158403397,0.005857499316334724,0.005754370242357254,0.005837955977767706,0.005783832632005215,0.005725155584514141,0.005670784041285515,0.005647544749081135,0.005605759099125862,0.005595477297902107,0.00557391531765461,0.0055292402394115925,0.005564938765019178,0.005547655746340752,0.005467559676617384,0.005443922709673643,0.005397488363087177,0.005432965233922005,0.005388668272644281,0.005365551915019751,0.0053427014499902725,0.005303079262375832,0.005257700104266405,0.005243058316409588,0.0052059851586818695,0.005150599405169487,0.005130266770720482,0.005180103238672018,0.005162765271961689,0.005083469208329916,0.005055544897913933,0.005091403611004353,0.005074314307421446,0.005009116139262915,0.005030128173530102,0.007986974902451038,0.008272610604763031,0.007233513984829187,0.007713751867413521,0.008519303984940052,0.008500580675899982,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 17043     
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 17044     
=================================================================
Total params: 34,087
Trainable params: 34,087
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 16)                80        
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 512)               8704      
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 16)                8208      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 51        
=================================================================
Total params: 17,043
Trainable params: 17,043
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 16)                64        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 512)               8704      
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 16)                8208      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 68        
=================================================================
Total params: 17,044
Trainable params: 17,044
Non-trainable params: 0
_________________________________________________________________
