2021-06-26
loss,0.2960941791534424,0.22126613557338715,0.15446783602237701,0.06769660860300064,0.04503775015473366,0.036932628601789474,0.033332113176584244,0.030742177739739418,0.029074711725115776,0.02735833078622818,0.026039734482765198,0.025026723742485046,0.02414892427623272,0.02372238226234913,0.022958526387810707,0.022563165053725243,0.021920518949627876,0.021634764969348907,0.021144868806004524,0.02079012617468834,0.020578555762767792,0.020239200443029404,0.019858554005622864,0.019687194377183914,0.01943138800561428,0.019177302718162537,0.018902858719229698,0.018754737451672554,0.01857425458729267,0.018368668854236603,0.01810675486922264,0.018103886395692825,0.017882229760289192,0.01770060509443283,0.017441045492887497,0.017345964908599854,0.01720147766172886,0.01707461290061474,0.016921862959861755,0.01679648831486702,0.016641195863485336,0.016574962064623833,0.01642797514796257,0.016275886446237564,0.01617109589278698,0.016181020066142082,0.015826568007469177,0.015954865142703056,0.01585596241056919,0.015681883320212364,0.015622292645275593,0.015501653775572777,0.015444491058588028,0.015435414388775826,0.01527169719338417,0.015158058144152164,0.015091759152710438,0.014988226816058159,0.015015407465398312,0.014915061183273792,0.014770817942917347,0.014759066514670849,0.014730406925082207,0.014606012031435966,0.014600229449570179,0.014465655200183392,0.014438300393521786,0.014405612833797932,0.014273185282945633,0.01422176044434309,0.014202089048922062,0.014160837046802044,0.014100936241447926,0.014043563976883888,0.013958782888948917,0.013955052010715008,0.013905645348131657,0.013753044418990612,0.013818789273500443,0.013710425235331059,0.013629663735628128,0.013523039408028126,0.013598669320344925,0.013576396740972996,0.013487880118191242,0.013477598316967487,0.013436716981232166,0.01329873688519001,0.013314575888216496,0.013223065994679928,0.013241397216916084,0.013274569064378738,0.013029197230935097,0.013094240799546242,0.013164236210286617,0.012960560619831085,0.012899038381874561,0.013062103651463985,0.012949815951287746,0.012916816398501396,
mse,0.030497999861836433,0.017710257321596146,0.00905382540076971,0.0015157904708757997,0.000638683675788343,0.0004145480925217271,0.000335297838319093,0.00028474791906774044,0.000252306810580194,0.0002234737330581993,0.00020348254474811256,0.00018768679001368582,0.00017455188208259642,0.00016747530025895685,0.0001573243353050202,0.00015159742906689644,0.00014354822633322328,0.0001390627003274858,0.00013231707271188498,0.00012753102055285126,0.00012518389848992229,0.0001210164264193736,0.00011645945778582245,0.00011428089783294126,0.00011132630606880412,0.00010870706319110468,0.00010497398034203798,0.00010343196481699124,0.00010120050865225494,9.910555672831833e-05,9.602645150152966e-05,9.582602069713175e-05,9.304934064857662e-05,9.164417133433744e-05,8.8871231127996e-05,8.802305819699541e-05,8.637423889013007e-05,8.49364441819489e-05,8.362813241546974e-05,8.199607691494748e-05,8.06718526291661e-05,8.003394759725779e-05,7.839631143724546e-05,7.686251046834514e-05,7.552609895356e-05,7.614667265443131e-05,7.272737275343388e-05,7.400494359899312e-05,7.256257231347263e-05,7.135676423786208e-05,7.06775244907476e-05,6.920233136042953e-05,6.906214548507705e-05,6.889363430673257e-05,6.733232294209301e-05,6.655925972154364e-05,6.569802644662559e-05,6.515177665278316e-05,6.480413139797747e-05,6.423033482860774e-05,6.290242890827358e-05,6.312395998975262e-05,6.260144436964765e-05,6.131375266704708e-05,6.135876174084842e-05,6.030516669852659e-05,5.9902722568949685e-05,5.973538281978108e-05,5.874986891285516e-05,5.8198933402309194e-05,5.7890014431905e-05,5.755089296144433e-05,5.73733304918278e-05,5.668895391863771e-05,5.581605728366412e-05,5.583213351201266e-05,5.563218655879609e-05,5.4167354392120615e-05,5.474629870150238e-05,5.369891368900426e-05,5.312927169143222e-05,5.231241084402427e-05,5.293914000503719e-05,5.273748320178129e-05,5.212341420701705e-05,5.202746615395881e-05,5.159513602848165e-05,5.0657501560635865e-05,5.0595608627190813e-05,4.983859616913833e-05,5.007901563658379e-05,5.043132114224136e-05,4.88325058540795e-05,4.908661503577605e-05,4.9594975280342624e-05,4.79395966976881e-05,4.7641391574870795e-05,4.861839988734573e-05,4.773059845319949e-05,4.758838258567266e-05,
mae,0.12380585074424744,0.08657915145158768,0.0659608393907547,0.02870028465986252,0.019163914024829865,0.01568295992910862,0.014153326861560345,0.012996924109756947,0.012508218176662922,0.011822588741779327,0.011187314987182617,0.010811617597937584,0.01029754988849163,0.01020954828709364,0.009808400645852089,0.009618970565497875,0.009381517767906189,0.009216316975653172,0.009049154818058014,0.00886300578713417,0.008761209435760975,0.008650344796478748,0.00843314453959465,0.008404688909649849,0.008322153240442276,0.008150131441652775,0.008092543110251427,0.007995326071977615,0.007862862199544907,0.007801967207342386,0.007692642975598574,0.007740171160548925,0.007647007238119841,0.0075829667039215565,0.007352789863944054,0.007413363549858332,0.007301881443709135,0.0073868934996426105,0.007237737067043781,0.007066840305924416,0.00708971219137311,0.007075489033013582,0.006945720873773098,0.006934409029781818,0.006866848096251488,0.006979071069508791,0.006574277766048908,0.006715535651892424,0.0067563606426119804,0.006751431152224541,0.006651069037616253,0.00657563516870141,0.0065645803697407246,0.006575210019946098,0.006484007928520441,0.006366712041199207,0.006414673291146755,0.006416723132133484,0.006411364767700434,0.00639376463368535,0.0062890672124922276,0.006237221881747246,0.006279489491134882,0.006228620186448097,0.006125761196017265,0.006220508366823196,0.006171250715851784,0.0060709333047270775,0.0060501908883452415,0.0061038280837237835,0.005982350092381239,0.006041558925062418,0.005916405934840441,0.00594073161482811,0.005962901283055544,0.005859485361725092,0.005850594025105238,0.005813316907733679,0.005789965856820345,0.005788943264633417,0.005785361398011446,0.005743007175624371,0.005794631317257881,0.005649657920002937,0.005741979461163282,0.005620988085865974,0.0057383752427995205,0.005667564924806356,0.005693495273590088,0.005555866751819849,0.005623200908303261,0.005554563831537962,0.005443849600851536,0.005511624738574028,0.005715229548513889,0.005414715502411127,0.005381648428738117,0.005538213066756725,0.0055140540935099125,0.005451027769595385,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 3307      
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 3308      
=================================================================
Total params: 6,615
Trainable params: 6,615
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 8)                 40        
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 128)               1152      
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 16)                2064      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 51        
=================================================================
Total params: 3,307
Trainable params: 3,307
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 16)                64        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 128)               2176      
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 8)                 1032      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 36        
=================================================================
Total params: 3,308
Trainable params: 3,308
Non-trainable params: 0
_________________________________________________________________
