2021-06-26
loss,0.282100647687912,0.07944455742835999,0.054204072803258896,0.061599403619766235,0.0517333522439003,0.03857125714421272,0.03322713077068329,0.03180934488773346,0.03963491693139076,0.04102037101984024,0.037710610777139664,0.05307868495583534,0.042807336896657944,0.0401628315448761,0.04268784821033478,0.03625654801726341,0.03594057634472847,0.03193720430135727,0.037272460758686066,0.033966802060604095,0.029386522248387337,0.026657741516828537,0.023563770577311516,0.02077285572886467,0.019823148846626282,0.018506750464439392,0.01913359947502613,0.01819058507680893,0.017827868461608887,0.017479119822382927,0.017359590157866478,0.017506899312138557,0.01703888364136219,0.017058178782463074,0.016611479222774506,0.016399037092924118,0.016325250267982483,0.016058102250099182,0.015852760523557663,0.015695063397288322,0.015460642985999584,0.0153330834582448,0.015298550017178059,0.015018647536635399,0.014985905028879642,0.014907675795257092,0.014720108360052109,0.014591837301850319,0.01440268475562334,0.014438038691878319,0.014177152886986732,0.0141798947006464,0.014022410847246647,0.01384546235203743,0.013837907463312149,0.01375051960349083,0.013675259426236153,0.013586249202489853,0.013538671657443047,0.013445020653307438,0.013391016982495785,0.01328205969184637,0.013209202326834202,0.012998150661587715,0.013033820316195488,0.012972142547369003,0.012871252372860909,0.012864908203482628,0.012772138230502605,0.012747623026371002,0.012702538631856441,0.012624995782971382,0.012531849555671215,0.012522241100668907,0.012436645105481148,0.012319679372012615,0.012235797941684723,0.012257806025445461,0.012078927829861641,0.01217639073729515,0.012099477462470531,0.012113085016608238,0.011947517283260822,0.011969109997153282,0.011865167878568172,0.011784089729189873,0.011832074262201786,0.011665587313473225,0.011630776338279247,0.011598367244005203,0.011555355973541737,0.011562219820916653,0.011454930528998375,0.0113904420286417,0.011440418660640717,0.01139941718429327,0.011391263455152512,0.011310393922030926,0.011291620321571827,0.011272934265434742,
mse,0.03276689350605011,0.0020331472624093294,0.0008820798248052597,0.0012095049023628235,0.0008170637302100658,0.0004335448902565986,0.00032119869138114154,0.00030144010088406503,0.0004545168485492468,0.0004846084048040211,0.0004073735617566854,0.0008336012251675129,0.000534897088073194,0.00046409349306486547,0.0005297395400702953,0.0003822743892669678,0.0003690638695843518,0.00028732538339681923,0.0003914831904694438,0.0003309413732495159,0.00024184990616049618,0.00020238339493516833,0.00016634640633128583,0.000129101041238755,0.0001158729282906279,0.00010117426427314058,0.00010420627950225025,9.452025551581755e-05,9.063258039532229e-05,8.671586692798883e-05,8.833270840113983e-05,8.905153663363308e-05,8.398174395551905e-05,8.26380928629078e-05,7.791969255777076e-05,7.639272371307015e-05,7.544313848484308e-05,7.333046232815832e-05,7.12445325916633e-05,6.946465146029368e-05,6.76718118484132e-05,6.664007378276438e-05,6.649068382102996e-05,6.404564919648692e-05,6.337281956803054e-05,6.263593240873888e-05,6.161801138659939e-05,6.028517600498162e-05,5.8440455177333206e-05,5.863151454832405e-05,5.668558515026234e-05,5.662607145495713e-05,5.570983557845466e-05,5.431516547105275e-05,5.4441552492789924e-05,5.342212170944549e-05,5.2881176088703796e-05,5.2660107030533254e-05,5.176529157324694e-05,5.0824695790652186e-05,5.050597610534169e-05,4.972906754119322e-05,4.911312862532213e-05,4.7996345529099926e-05,4.834324499825016e-05,4.740211079479195e-05,4.74035223305691e-05,4.699449345935136e-05,4.598855593940243e-05,4.6271066821645945e-05,4.559818626148626e-05,4.494046879699454e-05,4.482155054574832e-05,4.429086038726382e-05,4.381440157885663e-05,4.259839988662861e-05,4.2169249354628846e-05,4.2546624172246084e-05,4.1475665057078004e-05,4.18713825638406e-05,4.1412069549551234e-05,4.1253879317082465e-05,4.025593079859391e-05,4.067215195391327e-05,3.9708291296847165e-05,3.9117963751778007e-05,3.925336932297796e-05,3.8387155655073e-05,3.808204928645864e-05,3.849372660624795e-05,3.786879460676573e-05,3.820123311015777e-05,3.709496377268806e-05,3.7010886444477364e-05,3.720719541888684e-05,3.676983396871947e-05,3.724294583662413e-05,3.6948240449419245e-05,3.604375524446368e-05,3.588899562601e-05,
mae,0.12164801359176636,0.0339483767747879,0.023105384781956673,0.025953026488423347,0.02228023298084736,0.01641148328781128,0.01432182639837265,0.013600118458271027,0.01695111021399498,0.01732785254716873,0.015672599896788597,0.022849952802062035,0.018384184688329697,0.01724374108016491,0.01832946576178074,0.015643082559108734,0.015483138151466846,0.013415192253887653,0.01615213230252266,0.014734246768057346,0.01239832118153572,0.011396100744605064,0.010094694793224335,0.008895549923181534,0.008461184799671173,0.00792174506932497,0.008264915086328983,0.007848373614251614,0.007680379319936037,0.0075073824264109135,0.007462021894752979,0.00749658327549696,0.007317512761801481,0.0073467581532895565,0.007138916756957769,0.007004017010331154,0.007026227656751871,0.006931503303349018,0.0068246447481215,0.006745051592588425,0.006661841180175543,0.0065676746889948845,0.006555371917784214,0.006390182767063379,0.006414440926164389,0.006359541323035955,0.006295439787209034,0.00627351738512516,0.006155007518827915,0.006155990529805422,0.006032915785908699,0.006098519545048475,0.005978771951049566,0.005872755777090788,0.005902305711060762,0.00584746478125453,0.0058239903301000595,0.005810118280351162,0.005760759115219116,0.005780884530395269,0.005736548453569412,0.005677776411175728,0.005618266295641661,0.005523066036403179,0.005509120877832174,0.005568910855799913,0.005470184609293938,0.005475444253534079,0.00544991297647357,0.0054343282245099545,0.005439349915832281,0.0053762453608214855,0.005314568988978863,0.005337861366569996,0.0053142220713198185,0.005268513690680265,0.005234511569142342,0.005228987894952297,0.005143972113728523,0.005194562487304211,0.005142705515027046,0.005164256785064936,0.00512150302529335,0.005119618959724903,0.005006857682019472,0.0050309645012021065,0.005020917858928442,0.004983264021575451,0.0049428255297243595,0.00492596672847867,0.004876452032476664,0.004942500032484531,0.004836014937609434,0.004858656320720911,0.004880756139755249,0.0048453002236783504,0.004844173789024353,0.004831875674426556,0.004787638317793608,0.004776864778250456,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 12755     
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 12756     
=================================================================
Total params: 25,511
Trainable params: 25,511
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 16)                80        
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 256)               4352      
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 32)                8224      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 99        
=================================================================
Total params: 12,755
Trainable params: 12,755
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 32)                128       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 256)               8448      
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 16)                4112      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 68        
=================================================================
Total params: 12,756
Trainable params: 12,756
Non-trainable params: 0
_________________________________________________________________
