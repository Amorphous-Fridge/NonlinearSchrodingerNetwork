2021-06-26
loss,0.42688339948654175,0.2369559407234192,0.171945720911026,0.12168953567743301,0.08659826219081879,0.08247805386781693,0.06805448234081268,0.06593945622444153,0.05005475506186485,0.05410570278763771,0.050907135009765625,0.048530708998441696,0.04247475042939186,0.047495607286691666,0.04073157161474228,0.037666503340005875,0.03336802497506142,0.03881002962589264,0.0316871702671051,0.031078137457370758,0.035573411732912064,0.03178773820400238,0.03498433530330658,0.030091185122728348,0.02506674826145172,0.027216248214244843,0.02802075818181038,0.029992971569299698,0.026308810338377953,0.029174812138080597,0.026477854698896408,0.02735324203968048,0.023593680933117867,0.025316663086414337,0.025654185563325882,0.02423345111310482,0.024927286431193352,0.023340361192822456,0.02204442210495472,0.02136411890387535,0.02325337380170822,0.020655902102589607,0.021085355430841446,0.022570813074707985,0.02192307449877262,0.020877204835414886,0.01796283759176731,0.01610460691154003,0.017574094235897064,0.019396238029003143,0.02102695405483246,0.020160453394055367,0.019840575754642487,0.016258152201771736,0.01599283143877983,0.019408775493502617,0.02005673199892044,0.018603693693876266,0.01857268065214157,0.01697566732764244,0.015186570584774017,0.01806158386170864,0.01824188232421875,0.017152853310108185,0.016970433294773102,0.015011552721261978,0.013790437951683998,0.017052890732884407,0.01694428361952305,0.016515590250492096,0.015993688255548477,0.014277700334787369,0.01575804315507412,0.017584918066859245,0.01604815199971199,0.014803390949964523,0.014836376532912254,0.01414720993489027,0.014479846693575382,0.01686440221965313,0.01697085238993168,0.016336075961589813,0.015797730535268784,0.015311655588448048,0.014936314895749092,0.015175129286944866,0.014591011218726635,0.012839984148740768,0.011688903905451298,0.012755199335515499,0.012766998261213303,0.015855830162763596,0.015787048265337944,0.015067117288708687,0.01503371074795723,0.013760434463620186,0.014361622743308544,0.013202419504523277,0.013193205930292606,0.014967778697609901,
mse,0.0676109790802002,0.01850486546754837,0.00917130708694458,0.004386093933135271,0.0022655134089291096,0.0021287216804921627,0.0013614173512905836,0.0012378343380987644,0.0007452830905094743,0.0008508055470883846,0.0007551237940788269,0.0006745052523910999,0.0005274974391795695,0.0006446614861488342,0.0004825403157155961,0.0004130181623622775,0.000335737771820277,0.00043063407065346837,0.00030629176762886345,0.00028879012097604573,0.00035787676461040974,0.00029927692958153784,0.0003666243574116379,0.0002667348016984761,0.0001912483712658286,0.00022136711049824953,0.00023249439254868776,0.0002566761977504939,0.0002078828401863575,0.0002456795482430607,0.0002024580171564594,0.00021868973271921277,0.00016702663560863584,0.00018804319552145898,0.00018936421838589013,0.000173080203239806,0.0001831777044571936,0.00015905978216324002,0.00014588754856958985,0.000139024734380655,0.00015597703168168664,0.0001269088825210929,0.00013123989629093558,0.00014234006812330335,0.00013561462401412427,0.00012517822324298322,9.761405090102926e-05,7.979097426868975e-05,9.35132775339298e-05,0.00011232814722461626,0.0001246890751644969,0.0001173337732325308,0.00011271707626292482,8.001244714250788e-05,7.92744685895741e-05,0.00010949443822028115,0.00011188942880835384,9.909710206557065e-05,9.821264393394813e-05,8.75876285135746e-05,6.99908850947395e-05,9.482246241532266e-05,9.544936619931832e-05,8.49322896101512e-05,8.473978959955275e-05,6.777756061637774e-05,6.147442036308348e-05,8.488951425533742e-05,8.541617717128247e-05,7.977592758834362e-05,7.514117169193923e-05,6.149257387733087e-05,7.296606054296717e-05,8.556708053220063e-05,7.173781341407448e-05,6.23578525846824e-05,6.409786874428391e-05,6.167792889755219e-05,6.52224916848354e-05,8.132093353196979e-05,8.280730980914086e-05,7.609935710206628e-05,7.166375144151971e-05,6.946632493054494e-05,6.528844096465036e-05,6.673525786027312e-05,6.202123768161982e-05,5.058991882833652e-05,4.168729356024414e-05,5.018578303861432e-05,4.9970531108556315e-05,7.449766417266801e-05,7.142598042264581e-05,6.418358680093661e-05,6.682774255750701e-05,5.622846583719365e-05,6.0902806580998003e-05,5.270520705380477e-05,5.243333362159319e-05,6.325799768092111e-05,
mae,0.1766233891248703,0.09989332407712936,0.07259144634008408,0.05118498206138611,0.03684862703084946,0.03575547784566879,0.02898670732975006,0.028211088851094246,0.021419066935777664,0.023021869361400604,0.021664081141352654,0.020615648478269577,0.01819092594087124,0.020409390330314636,0.017283178865909576,0.016095217317342758,0.01405320968478918,0.016502976417541504,0.01356141921132803,0.013196324929594994,0.015163188800215721,0.013565974310040474,0.014960485510528088,0.012808437459170818,0.010515621863305569,0.01151664461940527,0.01196926087141037,0.01276830118149519,0.011061746627092361,0.012304768897593021,0.011296807788312435,0.01169530674815178,0.009987715631723404,0.010790497064590454,0.010937436483800411,0.010202744044363499,0.010561089962720871,0.00990770198404789,0.009265217930078506,0.009098583832383156,0.009959027171134949,0.008760884404182434,0.008978922851383686,0.009617622941732407,0.009333132766187191,0.008792946115136147,0.0076453243382275105,0.006853082217276096,0.007533671334385872,0.008294528350234032,0.008877893909811974,0.008528444916009903,0.00837406050413847,0.0068939123302698135,0.006814904976636171,0.008173090405762196,0.00840727984905243,0.007826904766261578,0.007976590655744076,0.0071509359404444695,0.006432100664824247,0.007588769309222698,0.007675362750887871,0.0072027649730443954,0.007253117393702269,0.006383390631526709,0.005881400778889656,0.007240773644298315,0.007144161034375429,0.007038065232336521,0.006814054679125547,0.006125469692051411,0.0067348782904446125,0.007482351269572973,0.006803535856306553,0.006230102386325598,0.0062551070004701614,0.006054586265236139,0.0061479960568249226,0.00721111660823226,0.00723173376172781,0.006908633280545473,0.006715154275298119,0.006457685027271509,0.006304366979748011,0.006449084263294935,0.0061880433931946754,0.005324138794094324,0.004902190063148737,0.00545908510684967,0.0054105245508253574,0.006680032704025507,0.006714743562042713,0.006329874508082867,0.006428651046007872,0.005882061552256346,0.00607358617708087,0.0055607762187719345,0.005557989701628685,0.006335321813821793,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 83203     
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 83204     
=================================================================
Total params: 166,407
Trainable params: 166,407
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 128)               640       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 512)               66048     
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 32)                16416     
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 99        
=================================================================
Total params: 83,203
Trainable params: 83,203
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 32)                128       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 512)               16896     
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 128)               65664     
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 516       
=================================================================
Total params: 83,204
Trainable params: 83,204
Non-trainable params: 0
_________________________________________________________________
