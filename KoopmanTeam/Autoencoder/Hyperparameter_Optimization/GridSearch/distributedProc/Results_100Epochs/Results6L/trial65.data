2021-06-26
loss,2.397584915161133,0.45129555463790894,0.3324526250362396,0.2642389237880707,0.2519932985305786,0.23952379822731018,0.23181195557117462,0.22492989897727966,0.22248110175132751,0.22805295884609222,0.16035829484462738,0.13198775053024292,0.12622351944446564,0.11138724535703659,0.08455610275268555,0.10939261317253113,0.07181984186172485,0.06946828216314316,0.06428519636392593,0.07517044246196747,0.07413589954376221,0.07091811299324036,0.07863014936447144,0.06019121780991554,0.07486996799707413,0.055177804082632065,0.04835723340511322,0.04577246680855751,0.06059660390019417,0.054089341312646866,0.050592146813869476,0.03924988582730293,0.0527641586959362,0.05666946992278099,0.04338419437408447,0.04637153446674347,0.04944451153278351,0.03646180406212807,0.04645365849137306,0.03854722902178764,0.04113742709159851,0.0414135567843914,0.04628676548600197,0.0470423698425293,0.041804246604442596,0.04643019288778305,0.041870780289173126,0.03823741897940636,0.03267167508602142,0.032299913465976715,0.03207585960626602,0.033253155648708344,0.035916782915592194,0.04004409909248352,0.0332636684179306,0.04390016198158264,0.03932184353470802,0.04115096479654312,0.030484361574053764,0.03032534569501877,0.036672309041023254,0.03769253194332123,0.0339973084628582,0.038424961268901825,0.034533802419900894,0.031153541058301926,0.030531851574778557,0.033814460039138794,0.03990667685866356,0.03697231784462929,0.034514665603637695,0.03624481335282326,0.028820006176829338,0.031780146062374115,0.03146720305085182,0.034959837794303894,0.0296857301145792,0.02791243977844715,0.030897317454218864,0.0321052111685276,0.0346187986433506,0.029640711843967438,0.03507483750581741,0.03332291916012764,0.03381015360355377,0.03068842925131321,0.03377407416701317,0.03439876064658165,0.03176316246390343,0.030777014791965485,0.028390001505613327,0.02668120339512825,0.027277378365397453,0.028340090066194534,0.0290204044431448,0.0330730602145195,0.02771453559398651,0.030863309279084206,0.028891276568174362,0.026127168908715248,
mse,1.8995521068572998,0.05983664095401764,0.033333491533994675,0.022316355258226395,0.020651860162615776,0.01889214664697647,0.017799869179725647,0.016772739589214325,0.016117578372359276,0.016215071082115173,0.007680873852223158,0.0052880458533763885,0.004719485528767109,0.003882993245497346,0.0021689552813768387,0.0036504340823739767,0.001536224503070116,0.0014524071011692286,0.001236637239344418,0.0016376135172322392,0.001603695098310709,0.001456248457543552,0.0018002392025664449,0.0011122707510367036,0.0016809228109195828,0.0008962958818301558,0.0007013902650214732,0.0006252583116292953,0.0010913251899182796,0.000874425342772156,0.0007871360285207629,0.00046965538058429956,0.0007979771471582353,0.0009127040975727141,0.0005550461355596781,0.0006359133985824883,0.0007066981634125113,0.00040507555240765214,0.0006135705625638366,0.00043938387534581125,0.00048595762928016484,0.0005037013906985521,0.0006283594411797822,0.0006236754124984145,0.0005068332538940012,0.000653816619887948,0.0005142638692632318,0.00042032889905385673,0.0003164830559398979,0.0003135850711259991,0.00030352448811754584,0.0003300984972156584,0.0003830971836578101,0.00046986722736619413,0.00033203273778781295,0.0005523130530491471,0.0004525635449681431,0.0004808537196367979,0.0002861410321202129,0.0002793383609969169,0.00039149625808931887,0.0004128603031858802,0.0003448172938078642,0.0004293072852306068,0.00034904031781479716,0.00028593637398444116,0.00027922767912968993,0.0003338625538162887,0.00047159488894976676,0.0003976117877755314,0.0003428524360060692,0.0003871960216201842,0.00024422284332104027,0.0002965485618915409,0.0002889097377192229,0.00035544310230761766,0.00026288273511454463,0.00023502265685237944,0.0002824996772687882,0.00030233486904762685,0.00034521345514804125,0.0002690932888071984,0.0003634269814938307,0.00031515094451606274,0.00033376921783201396,0.00028605072293430567,0.0003316570946481079,0.00033584266202524304,0.0002925585431512445,0.00028304828447289765,0.00023141983547247946,0.0002133450616383925,0.00022883352357894182,0.0002362851082580164,0.00024666893295943737,0.0003174278244841844,0.00021934602409601212,0.0002747968246694654,0.00023640235303901136,0.00020181226136628538,
mae,1.0425667762756348,0.1946060061454773,0.1442168653011322,0.1163301169872284,0.11263472586870193,0.10598140954971313,0.10067152231931686,0.09658126533031464,0.09470786154270172,0.09615928679704666,0.06668588519096375,0.055934153497219086,0.05348797142505646,0.04727514833211899,0.03534815460443497,0.04536594823002815,0.030205097049474716,0.02935522049665451,0.027208643034100533,0.031506143510341644,0.03152952343225479,0.03035856783390045,0.03257782384753227,0.025447530671954155,0.031042220070958138,0.023448389023542404,0.020222987979650497,0.0194232277572155,0.025807663798332214,0.023234710097312927,0.02126348949968815,0.016370058059692383,0.02181427925825119,0.02378799207508564,0.018039748072624207,0.01964251510798931,0.02099231258034706,0.01545765995979309,0.019517455250024796,0.016096683219075203,0.017345480620861053,0.01746746338903904,0.01918499916791916,0.019800150766968727,0.01779516413807869,0.019477730616927147,0.01787557452917099,0.016272423788905144,0.013846739195287228,0.013655299320816994,0.01364341750741005,0.014011872932314873,0.01532159373164177,0.016635768115520477,0.014027596451342106,0.018654007464647293,0.016164565458893776,0.016912035644054413,0.012984837405383587,0.01286089327186346,0.015776269137859344,0.015935461968183517,0.014197040349245071,0.01620119996368885,0.01466342806816101,0.013336163014173508,0.012877420522272587,0.014359688386321068,0.016812128946185112,0.015851305797696114,0.014503996819257736,0.01542503759264946,0.01238669827580452,0.013391449116170406,0.013262081891298294,0.014580816961824894,0.012524932622909546,0.01191481202840805,0.013208651915192604,0.013588274829089642,0.014479629695415497,0.012365861795842648,0.014647022821009159,0.01432276051491499,0.014442934654653072,0.013244633562862873,0.014245233498513699,0.014252599328756332,0.013371225446462631,0.013064703904092312,0.012034635990858078,0.011414949782192707,0.011738922446966171,0.012108784168958664,0.012372415512800217,0.013785064220428467,0.01182971615344286,0.01310103852301836,0.012121574021875858,0.011047529987990856,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 313259    
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 313260    
=================================================================
Total params: 626,519
Trainable params: 626,519
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 8)                 40        
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 32)                288       
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 512)               16896     
_________________________________________________________________
encoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
encoding_layer_5 (Dense)     (None, 64)                32832     
_________________________________________________________________
encoding_layer_6 (Dense)     (None, 8)                 520       
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 27        
=================================================================
Total params: 313,259
Trainable params: 313,259
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 8)                 32        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 64)                576       
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 512)               33280     
_________________________________________________________________
decoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
decoding_layer_5 (Dense)     (None, 32)                16416     
_________________________________________________________________
decoding_layer_6 (Dense)     (None, 8)                 264       
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 36        
=================================================================
Total params: 313,260
Trainable params: 313,260
Non-trainable params: 0
_________________________________________________________________
