2021-06-26
loss,0.5477572083473206,0.23189686238765717,0.19896680116653442,0.14410708844661713,0.11945901811122894,0.09874691069126129,0.10434967279434204,0.09643054008483887,0.09938883036375046,0.1172037348151207,0.0999903455376625,0.08341536670923233,0.08449413627386093,0.07878320664167404,0.06622057408094406,0.0697912648320198,0.06364772468805313,0.06191928684711456,0.0615399107336998,0.055330414324998856,0.05562986433506012,0.05101167783141136,0.055415499955415726,0.049231670796871185,0.05207138508558273,0.05881893262267113,0.05036886781454086,0.04405348375439644,0.03812595456838608,0.060719478875398636,0.05647457763552666,0.05932651460170746,0.051422376185655594,0.03767608851194382,0.038942329585552216,0.05093749985098839,0.04381061717867851,0.04412848502397537,0.04206418618559837,0.04068107530474663,0.04547334834933281,0.04597720131278038,0.03928597271442413,0.03540489077568054,0.031626634299755096,0.03256120905280113,0.042789146304130554,0.046348296105861664,0.042612411081790924,0.042288638651371,0.04100702702999115,0.03576819226145744,0.03740588575601578,0.03419112786650658,0.03879797086119652,0.03393006697297096,0.03132899850606918,0.03361380845308304,0.035402700304985046,0.03186731040477753,0.035950325429439545,0.03840671479701996,0.031793005764484406,0.029623940587043762,0.03489024564623833,0.030980782583355904,0.035784728825092316,0.03343266621232033,0.03249812126159668,0.031475234776735306,0.03271939605474472,0.0331035740673542,0.028036026284098625,0.028223762288689613,0.030342424288392067,0.03135189786553383,0.030651070177555084,0.030270233750343323,0.03464142978191376,0.027270881459116936,0.02659394033253193,0.02534654550254345,0.03104691207408905,0.030340541154146194,0.02809528261423111,0.025970792397856712,0.029486900195479393,0.03026523068547249,0.02840673364698887,0.024928560480475426,0.023259872570633888,0.030257733538746834,0.030557740479707718,0.026496611535549164,0.02558828331530094,0.026520710438489914,0.026435069739818573,0.029169278219342232,0.024841096252202988,0.02825128845870495,
mse,0.12778936326503754,0.017351429909467697,0.01189812459051609,0.006102606654167175,0.004088180605322123,0.0027591439429670572,0.0032243242021650076,0.0027610722463577986,0.0028967743273824453,0.0038834139704704285,0.002904988592490554,0.001974404091015458,0.0019803079776465893,0.0017599399434402585,0.001313140382990241,0.0013840383617207408,0.0011771084973588586,0.0010959082283079624,0.0010568760335445404,0.0008629141957499087,0.0008811482111923397,0.0007583448314107955,0.0008821015944704413,0.0006945885252207518,0.0007895799935795367,0.0009730610181577504,0.0007223327411338687,0.0005754561279900372,0.00041187903843820095,0.0010548537829890847,0.0009105983190238476,0.0009738742373883724,0.0007240008562803268,0.0004277046537026763,0.0004525544063653797,0.0007692671497352421,0.0005490549374371767,0.0005382851231843233,0.00048255641013383865,0.00047686253674328327,0.0005718260072171688,0.0005858237855136395,0.0004356939753051847,0.00035415132879279554,0.0002751131250988692,0.0003018566931132227,0.0005236599827185273,0.0006160066113807261,0.000499653397127986,0.0004962676903232932,0.0004623974091373384,0.000349377776728943,0.0003918875299859792,0.00032485590782016516,0.0004211491032037884,0.00032783986534923315,0.00028303361614234746,0.0003088427765760571,0.00034986037644557655,0.00027815040084533393,0.0003624112578108907,0.00040333555079996586,0.00028279461548663676,0.0002507846802473068,0.00033803784754127264,0.00027239465271122754,0.00036276309401728213,0.0003088098019361496,0.00029019583598710597,0.0002798180503305048,0.000300630897982046,0.0003022811724804342,0.00022295385133475065,0.00022620480740442872,0.00025232124608010054,0.00028735195519402623,0.0002614659897517413,0.00025313158403150737,0.0003289272717665881,0.00021105140331201255,0.00019538402557373047,0.0001789136731531471,0.00026856831391341984,0.0002546129107940942,0.00021894114615861326,0.00019129931752104312,0.00024087607744149864,0.0002555612300056964,0.00021729068248532712,0.00017437536735087633,0.00016283524746540934,0.0002595318655949086,0.0002553011872805655,0.00019450791296549141,0.00018355278007220477,0.00019559942302294075,0.00019664863066282123,0.00023168038751464337,0.00017163451411761343,0.0002274874277645722,
mae,0.23187732696533203,0.09850906580686569,0.08493878692388535,0.061595700681209564,0.051245175302028656,0.04223190248012543,0.04459323734045029,0.041474372148513794,0.04264281317591667,0.05012500658631325,0.042889952659606934,0.03473980352282524,0.0362701341509819,0.033282846212387085,0.02839287370443344,0.030399717390537262,0.026812059804797173,0.02575775794684887,0.025942599400877953,0.023497791960835457,0.023664390668272972,0.02169964462518692,0.023760797455906868,0.020825546234846115,0.022337088361382484,0.02429109625518322,0.02178773656487465,0.018736954778432846,0.01620711386203766,0.02607332170009613,0.024251379072666168,0.025981388986110687,0.022235684096813202,0.01662190817296505,0.016506657004356384,0.021800572052598,0.01852756179869175,0.018601838499307632,0.017843492329120636,0.01723971962928772,0.01898159086704254,0.01929103396832943,0.016957907006144524,0.014608033932745457,0.0136661222204566,0.013660451397299767,0.018292631953954697,0.019979193806648254,0.018010688945651054,0.017920061945915222,0.0176017414778471,0.015491876751184464,0.015886567533016205,0.01434797141700983,0.016465866938233376,0.01452687382698059,0.013430003076791763,0.014131427742540836,0.014900505542755127,0.013166937977075577,0.015253591351211071,0.016523459926247597,0.014030453749001026,0.012612524442374706,0.014743044041097164,0.013431644067168236,0.015666745603084564,0.014417693950235844,0.014102596789598465,0.013657339848577976,0.014048472978174686,0.013722620904445648,0.0120604382827878,0.01202815305441618,0.013036894612014294,0.013478636741638184,0.01317833922803402,0.013048077002167702,0.015111038461327553,0.011282767169177532,0.011169017292559147,0.0108507564291358,0.013220957480370998,0.012980364263057709,0.011961412616074085,0.011130941100418568,0.012622364796698093,0.012649336829781532,0.012280433438718319,0.010554912500083447,0.0099263247102499,0.012769252061843872,0.013100875541567802,0.010950691998004913,0.010809946805238724,0.011502918787300587,0.011217101477086544,0.012499963864684105,0.010701632127165794,0.012166569009423256,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 35347     
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 35348     
=================================================================
Total params: 70,695
Trainable params: 70,695
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 16)                80        
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 64)                1088      
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 128)               8320      
_________________________________________________________________
encoding_layer_4 (Dense)     (None, 128)               16512     
_________________________________________________________________
encoding_layer_5 (Dense)     (None, 64)                8256      
_________________________________________________________________
encoding_layer_6 (Dense)     (None, 16)                1040      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 51        
=================================================================
Total params: 35,347
Trainable params: 35,347
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 16)                64        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 64)                1088      
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 128)               8320      
_________________________________________________________________
decoding_layer_4 (Dense)     (None, 128)               16512     
_________________________________________________________________
decoding_layer_5 (Dense)     (None, 64)                8256      
_________________________________________________________________
decoding_layer_6 (Dense)     (None, 16)                1040      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 68        
=================================================================
Total params: 35,348
Trainable params: 35,348
Non-trainable params: 0
_________________________________________________________________
