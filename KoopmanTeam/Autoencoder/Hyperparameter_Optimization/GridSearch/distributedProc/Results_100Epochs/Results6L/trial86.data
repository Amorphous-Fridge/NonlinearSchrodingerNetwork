2021-06-26
loss,4.4372053146362305,4.424245834350586,4.424288749694824,4.424069881439209,4.424009323120117,4.424237251281738,4.424399375915527,4.424291610717773,4.4243621826171875,4.424283981323242,4.424164295196533,4.423977375030518,4.424182415008545,4.424185276031494,4.424135684967041,4.424052715301514,4.424391269683838,4.424102306365967,4.42410135269165,4.424510478973389,4.424255847930908,4.4242119789123535,4.424204349517822,4.4240899085998535,4.424261093139648,4.423982620239258,4.424232006072998,4.424149990081787,4.42416524887085,4.4242048263549805,4.424170017242432,4.424069404602051,4.4243550300598145,4.424300670623779,4.4241485595703125,4.424358367919922,4.424304008483887,4.424030303955078,4.424192905426025,4.424138069152832,4.424422740936279,4.42436408996582,4.424189567565918,4.424168109893799,4.424136161804199,4.424200534820557,4.424365997314453,4.424032688140869,4.424424171447754,4.424505233764648,4.4242329597473145,4.4241719245910645,4.424322128295898,4.424139499664307,4.4241743087768555,4.4240922927856445,4.424354076385498,4.424300193786621,4.424249172210693,4.424222469329834,4.424044609069824,4.424276351928711,4.424396991729736,4.424150466918945,4.424160957336426,4.424190521240234,4.4242472648620605,4.42435884475708,4.4243998527526855,4.424133777618408,4.424271106719971,4.424316883087158,4.424380302429199,4.4242329597473145,4.424190044403076,4.424152851104736,4.4242262840271,4.4241108894348145,4.424293518066406,4.424343585968018,4.4240875244140625,4.42425537109375,4.424296855926514,4.424088954925537,4.42426872253418,4.424133777618408,4.424266815185547,4.424312114715576,4.423947334289551,4.424163341522217,4.424056529998779,4.424191951751709,4.424142360687256,4.4242658615112305,4.424208641052246,4.424280166625977,4.424167633056641,4.424468517303467,4.424310207366943,4.42418909072876,
mse,4.992538928985596,4.894380569458008,4.894474983215332,4.893989086151123,4.893856525421143,4.894365310668945,4.89471960067749,4.8944830894470215,4.894639015197754,4.894458293914795,4.894200325012207,4.893792629241943,4.894241809844971,4.894245147705078,4.894141674041748,4.893960475921631,4.894697666168213,4.894057750701904,4.894064426422119,4.894965648651123,4.894405364990234,4.894311904907227,4.894290924072266,4.894036769866943,4.894418239593506,4.893805980682373,4.894348621368408,4.894176006317139,4.894200801849365,4.894288539886475,4.894216537475586,4.893987655639648,4.8946213722229,4.894503593444824,4.894165992736816,4.8946332931518555,4.89451265335083,4.893906593322754,4.89426326751709,4.894143104553223,4.894769191741943,4.894646167755127,4.894257068634033,4.894209384918213,4.894143581390381,4.894289493560791,4.894646167755127,4.893916606903076,4.894768714904785,4.894952297210693,4.894354820251465,4.894216537475586,4.8945488929748535,4.8941497802734375,4.894219398498535,4.894044876098633,4.894618034362793,4.8944993019104,4.894395351409912,4.894330024719238,4.893937587738037,4.894445896148682,4.894710540771484,4.894169807434082,4.894187927246094,4.894257068634033,4.894379615783691,4.8946380615234375,4.894720077514648,4.8941264152526855,4.894434928894043,4.894535064697266,4.894674777984619,4.894352912902832,4.894260406494141,4.894176959991455,4.89433479309082,4.894083499908447,4.894488334655762,4.894594669342041,4.8940348625183105,4.894392013549805,4.894487380981445,4.89402961730957,4.894431114196777,4.894129753112793,4.894424915313721,4.89453125,4.893728256225586,4.894200325012207,4.893962860107422,4.894260406494141,4.8941545486450195,4.8944220542907715,4.894302845001221,4.894458770751953,4.894210338592529,4.8948774337768555,4.894526481628418,4.8942551612854,
mae,2.1903574466705322,2.1999027729034424,2.1999292373657227,2.199791193008423,2.199753522872925,2.1998982429504395,2.199998617172241,2.1999313831329346,2.1999757289886475,2.1999247074127197,2.1998510360717773,2.199734687805176,2.1998627185821533,2.199863910675049,2.1998348236083984,2.199782609939575,2.1999928951263428,2.199810266494751,2.199812412261963,2.200068712234497,2.1999099254608154,2.1998825073242188,2.1998772621154785,2.1998045444488525,2.1999130249023438,2.1997387409210205,2.199892997741699,2.1998448371887207,2.1998510360717773,2.199876546859741,2.1998555660247803,2.1997909545898438,2.1999704837799072,2.199937582015991,2.1998414993286133,2.199974775314331,2.1999404430389404,2.1997671127319336,2.1998696327209473,2.1998348236083984,2.2000129222869873,2.1999776363372803,2.1998674869537354,2.1998534202575684,2.1998353004455566,2.1998767852783203,2.1999785900115967,2.199769973754883,2.200012683868408,2.2000646591186523,2.199894666671753,2.199855327606201,2.1999504566192627,2.199836492538452,2.1998560428619385,2.1998064517974854,2.19996976852417,2.1999363899230957,2.199906826019287,2.199888229370117,2.1997761726379395,2.199920892715454,2.19999623298645,2.1998424530029297,2.1998472213745117,2.1998674869537354,2.199901819229126,2.1999757289886475,2.1999990940093994,2.1998300552368164,2.199917793273926,2.199946165084839,2.1999855041503906,2.199894666671753,2.1998679637908936,2.1998441219329834,2.1998894214630127,2.1998183727264404,2.1999332904815674,2.199963331222534,2.199803590774536,2.1999053955078125,2.199932813644409,2.1998026371002197,2.1999170780181885,2.199831008911133,2.1999151706695557,2.1999454498291016,2.199716806411743,2.1998512744903564,2.1997838020324707,2.1998682022094727,2.1998379230499268,2.19991397857666,2.1998796463012695,2.1999242305755615,2.1998541355133057,2.2000439167022705,2.1999430656433105,2.199867010116577,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 397683    
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 397684    
=================================================================
Total params: 795,367
Trainable params: 795,367
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 16)                80        
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 128)               2176      
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 512)               66048     
_________________________________________________________________
encoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
encoding_layer_5 (Dense)     (None, 128)               65664     
_________________________________________________________________
encoding_layer_6 (Dense)     (None, 8)                 1032      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 27        
=================================================================
Total params: 397,683
Trainable params: 397,683
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 8)                 32        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 128)               1152      
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 512)               66048     
_________________________________________________________________
decoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
decoding_layer_5 (Dense)     (None, 128)               65664     
_________________________________________________________________
decoding_layer_6 (Dense)     (None, 16)                2064      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 68        
=================================================================
Total params: 397,684
Trainable params: 397,684
Non-trainable params: 0
_________________________________________________________________
