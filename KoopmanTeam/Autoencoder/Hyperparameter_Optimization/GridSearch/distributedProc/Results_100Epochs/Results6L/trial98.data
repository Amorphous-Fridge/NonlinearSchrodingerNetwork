2021-06-26
loss,2.5246334075927734,2.2143943309783936,2.210995674133301,2.2096903324127197,2.21016788482666,2.2100584506988525,2.2101998329162598,2.211702823638916,2.2090749740600586,2.2106595039367676,2.2118136882781982,2.2098796367645264,2.2082972526550293,2.2087082862854004,2.20980167388916,2.2075958251953125,2.2082059383392334,2.209679365158081,2.2080531120300293,2.2078917026519775,2.204633951187134,2.4329140186309814,1.3506370782852173,0.3844372630119324,0.3638647794723511,0.3610762357711792,0.36001625657081604,0.3581635653972626,0.3568503260612488,0.35640084743499756,0.35771387815475464,0.35715657472610474,0.37188273668289185,0.35968539118766785,0.3558003902435303,0.35573893785476685,0.35525473952293396,0.354960173368454,0.3548929989337921,0.35492485761642456,0.35419586300849915,0.35385262966156006,0.3542811870574951,0.35466039180755615,0.3541441857814789,0.35463589429855347,0.35414379835128784,0.3538294732570648,0.35425281524658203,0.3545941114425659,0.35446158051490784,0.35435977578163147,0.35406169295310974,0.35420674085617065,0.35380032658576965,0.3542869985103607,0.3538050353527069,0.35385721921920776,0.3537693917751312,0.3580990731716156,0.35536086559295654,0.35397961735725403,0.35402438044548035,0.35414737462997437,0.3546806573867798,0.35428282618522644,0.35403987765312195,0.3543056547641754,0.3543194830417633,0.35412102937698364,0.3546757102012634,0.3559444844722748,0.3573387563228607,0.3538771867752075,0.3536350131034851,0.3539119362831116,0.3543201684951782,0.35397589206695557,0.3536631464958191,0.35359156131744385,0.3548046946525574,0.35422420501708984,0.35391250252723694,0.3535224497318268,0.35467156767845154,0.35410743951797485,0.3537111282348633,0.35406383872032166,0.3540405333042145,0.3539530634880066,0.3536891043186188,0.353885680437088,0.3542194664478302,0.35360047221183777,0.3544991612434387,0.3540383577346802,0.5701581835746765,0.362249493598938,0.355093389749527,0.35451096296310425,
mse,1.6548253297805786,1.2393757104873657,1.2357122898101807,1.2342944145202637,1.2347941398620605,1.2347434759140015,1.2348829507827759,1.236524224281311,1.2336844205856323,1.2353841066360474,1.2366359233856201,1.2345010042190552,1.2327593564987183,1.2333135604858398,1.2344149351119995,1.2320075035095215,1.2327615022659302,1.2344179153442383,1.232572078704834,1.2324514389038086,1.228834867477417,1.662245273590088,0.5955039858818054,0.042411305010318756,0.03845490142703056,0.03813254460692406,0.03799000382423401,0.037722907960414886,0.03759291023015976,0.03750409185886383,0.037718337029218674,0.03755859285593033,0.04029808193445206,0.03803949058055878,0.03746575489640236,0.037474364042282104,0.03736524656414986,0.03735075891017914,0.03735113516449928,0.037362974137067795,0.037287160754203796,0.03719098120927811,0.03723611682653427,0.03735179826617241,0.037230778485536575,0.0373530276119709,0.03725318983197212,0.03718644008040428,0.03730226680636406,0.03733799606561661,0.03732043877243996,0.03728760406374931,0.03724617883563042,0.037253402173519135,0.03719642758369446,0.037279728800058365,0.03720232471823692,0.0372203104197979,0.03721024841070175,0.037816036492586136,0.03750333935022354,0.037268634885549545,0.03728688135743141,0.03727356344461441,0.03733917325735092,0.03729010745882988,0.037258878350257874,0.037294261157512665,0.03731068968772888,0.03727179765701294,0.03736628592014313,0.037517886608839035,0.03774828091263771,0.03720683977007866,0.03716282919049263,0.03721589222550392,0.0372936949133873,0.0372137576341629,0.03716498613357544,0.03716036677360535,0.037385646253824234,0.03728058561682701,0.037205975502729416,0.03715766593813896,0.037352193146944046,0.0372423492372036,0.037186458706855774,0.03724655508995056,0.037255916744470596,0.037229448556900024,0.037187326699495316,0.03721771761775017,0.03728233650326729,0.03716526925563812,0.03732427954673767,0.03724569454789162,0.14767253398895264,0.03864355385303497,0.03739212453365326,0.037352606654167175,
mae,0.927584707736969,0.6316509246826172,0.6154657602310181,0.6123824119567871,0.610633134841919,0.6090822815895081,0.6083845496177673,0.6173213720321655,0.6081474423408508,0.6081954836845398,0.620470404624939,0.609347403049469,0.6077091097831726,0.6078706979751587,0.6133447885513306,0.6085991859436035,0.6081131100654602,0.6130782961845398,0.6086371541023254,0.6093249320983887,0.6099403500556946,0.8798380494117737,0.47390803694725037,0.1636599600315094,0.15175378322601318,0.14947982132434845,0.14951591193675995,0.1486363261938095,0.14782167971134186,0.1479867398738861,0.14901544153690338,0.1495666354894638,0.1579885333776474,0.15109820663928986,0.14824062585830688,0.1479724496603012,0.1478073000907898,0.14753565192222595,0.14764612913131714,0.14772331714630127,0.1471886783838272,0.1470397412776947,0.14728683233261108,0.14750461280345917,0.14722710847854614,0.1473015546798706,0.1472090184688568,0.14701300859451294,0.14715053141117096,0.14743328094482422,0.14726977050304413,0.14724428951740265,0.14722391963005066,0.1472131758928299,0.1468581259250641,0.14725250005722046,0.1469263881444931,0.14693476259708405,0.1468105912208557,0.15080738067626953,0.14683197438716888,0.14636236429214478,0.14657336473464966,0.14670564234256744,0.14717473089694977,0.14710161089897156,0.14689499139785767,0.14715242385864258,0.14722077548503876,0.14690551161766052,0.14720605313777924,0.14865870773792267,0.14941082894802094,0.146804079413414,0.14670783281326294,0.1468384712934494,0.14690011739730835,0.14692629873752594,0.14675207436084747,0.1466163992881775,0.14721739292144775,0.14680533111095428,0.14681968092918396,0.14651289582252502,0.14704884588718414,0.14667408168315887,0.1466836780309677,0.1466810405254364,0.1467612385749817,0.14682607352733612,0.1465560495853424,0.14670991897583008,0.14680901169776917,0.146542489528656,0.1469227820634842,0.14673109352588654,0.24669407308101654,0.15101124346256256,0.14532427489757538,0.14537642896175385,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 463307    
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 463308    
=================================================================
Total params: 926,615
Trainable params: 926,615
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 8)                 40        
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 256)               2304      
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 512)               131584    
_________________________________________________________________
encoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
encoding_layer_5 (Dense)     (None, 128)               65664     
_________________________________________________________________
encoding_layer_6 (Dense)     (None, 8)                 1032      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 27        
=================================================================
Total params: 463,307
Trainable params: 463,307
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 8)                 32        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 128)               1152      
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 512)               66048     
_________________________________________________________________
decoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
decoding_layer_5 (Dense)     (None, 256)               131328    
_________________________________________________________________
decoding_layer_6 (Dense)     (None, 8)                 2056      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 36        
=================================================================
Total params: 463,308
Trainable params: 463,308
Non-trainable params: 0
_________________________________________________________________
