2021-06-26
loss,4.396615982055664,4.4241838455200195,4.424148082733154,4.424371242523193,4.4244771003723145,4.424536228179932,4.423954010009766,4.424355983734131,4.42432975769043,4.424077033996582,4.424079418182373,4.42430305480957,4.424223899841309,4.424093723297119,4.424297332763672,4.424149990081787,4.424400806427002,4.424180507659912,4.423973560333252,4.424525737762451,4.424116134643555,4.424323081970215,4.424227714538574,4.424263000488281,4.424067497253418,4.424313545227051,6.546750068664551,4.424199104309082,4.424127578735352,4.424201965332031,4.424301624298096,4.423859596252441,4.42393684387207,4.424190044403076,4.424376010894775,4.424291133880615,4.423962593078613,4.424103260040283,4.424175262451172,4.424111843109131,4.42422342300415,4.424066543579102,4.424263000488281,4.42416524887085,4.424135208129883,4.424524307250977,4.424180507659912,4.424309253692627,4.424267768859863,4.423975944519043,4.424088478088379,4.424289703369141,4.424376487731934,4.424253940582275,4.424106121063232,4.424093723297119,4.424381256103516,4.4241132736206055,4.424176216125488,4.424000263214111,4.424048900604248,4.424017429351807,4.423991680145264,4.4243268966674805,4.424444675445557,4.4240827560424805,4.424170017242432,4.424383640289307,4.424209117889404,4.424381256103516,4.4244585037231445,4.424104690551758,4.424165725708008,4.424293518066406,4.424618721008301,4.424259185791016,4.424192428588867,4.42408561706543,4.4242844581604,4.424283027648926,4.424295425415039,4.4242777824401855,4.424126148223877,4.424026966094971,4.424089431762695,4.424383640289307,4.424206733703613,4.424116611480713,4.424200534820557,4.424299716949463,4.424194812774658,4.424217700958252,4.42401647567749,4.424308776855469,4.424250602722168,4.4241414070129395,4.423964977264404,4.424365520477295,4.424277305603027,4.424116134643555,
mse,4.874764442443848,4.894239902496338,4.894166469573975,4.894657611846924,4.894891262054443,4.895013809204102,4.893740177154541,4.894619941711426,4.894567012786865,4.894003391265869,4.894016742706299,4.894506931304932,4.894326210021973,4.894049167633057,4.894495964050293,4.894169807434082,4.8947248458862305,4.89423942565918,4.893777847290039,4.8950018882751465,4.894093036651611,4.894556999206543,4.894346714019775,4.894419193267822,4.893989562988281,4.894533634185791,28.201438903808594,4.8942790031433105,4.894119739532471,4.894287586212158,4.894498825073242,4.893531322479248,4.893700122833252,4.894261837005615,4.894664764404297,4.894482135772705,4.893754959106445,4.8940653800964355,4.894223213195801,4.894094944000244,4.8943305015563965,4.893984317779541,4.894418239593506,4.894209384918213,4.894132614135742,4.894993782043457,4.894232749938965,4.8945207595825195,4.8944315910339355,4.8937859535217285,4.8940348625183105,4.894478797912598,4.89466667175293,4.894400119781494,4.894073009490967,4.894048690795898,4.89467716217041,4.894094944000244,4.894225597381592,4.8938398361206055,4.893945217132568,4.893877983093262,4.8938212394714355,4.894562244415283,4.894811153411865,4.894021511077881,4.894215106964111,4.894689083099365,4.8942999839782715,4.894680500030518,4.894850254058838,4.8940749168396,4.894207000732422,4.894480228424072,4.895202159881592,4.894416809082031,4.89426326751709,4.894027233123779,4.8944621086120605,4.8944597244262695,4.894495010375977,4.8944501876831055,4.8941168785095215,4.893899917602539,4.894033908843994,4.894683361053467,4.894291400909424,4.894096851348877,4.8942790031433105,4.894497871398926,4.894268035888672,4.894319534301758,4.893874645233154,4.894516944885254,4.894391059875488,4.894153118133545,4.893762111663818,4.894646644592285,4.894451141357422,4.8940958976745605,
mae,2.1792404651641846,2.199862003326416,2.1998414993286133,2.199981451034546,2.200047492980957,2.200082778930664,2.1997201442718506,2.1999707221984863,2.199955463409424,2.1997954845428467,2.199798822402954,2.1999380588531494,2.1998870372772217,2.199807643890381,2.1999351978302,2.1998422145843506,2.200000047683716,2.199862480163574,2.19973087310791,2.2000784873962402,2.1998207569122314,2.1999521255493164,2.199892997741699,2.199913740158081,2.199791669845581,2.1999475955963135,2.8542232513427734,2.199873447418213,2.1998283863067627,2.199875831604004,2.1999363899230957,2.1996607780456543,2.199709177017212,2.1998682022094727,2.1999831199645996,2.1999313831329346,2.1997246742248535,2.1998131275177,2.1998579502105713,2.199821710586548,2.199887990951538,2.1997900009155273,2.199913263320923,2.1998531818389893,2.199831962585449,2.2000768184661865,2.1998603343963623,2.199941635131836,2.1999166011810303,2.1997334957122803,2.1998038291931152,2.1999306678771973,2.199983835220337,2.1999080181121826,2.199815034866333,2.199808359146118,2.1999869346618652,2.1998209953308105,2.1998579502105713,2.1997487545013428,2.1997787952423096,2.1997597217559814,2.1997432708740234,2.19995379447937,2.2000246047973633,2.1998000144958496,2.1998555660247803,2.1999902725219727,2.1998791694641113,2.1999871730804443,2.200035810470581,2.199815511703491,2.1998536586761475,2.1999311447143555,2.2001357078552246,2.1999123096466064,2.19986891746521,2.1998021602630615,2.199925422668457,2.199924945831299,2.199934482574463,2.1999220848083496,2.1998276710510254,2.199765682220459,2.199803352355957,2.199988603591919,2.1998767852783203,2.199821710586548,2.199873924255371,2.1999356746673584,2.1998703479766846,2.199885368347168,2.199759006500244,2.199941635131836,2.1999056339263916,2.199838161468506,2.1997268199920654,2.1999778747558594,2.1999223232269287,2.1998212337493896,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 550851    
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 550852    
=================================================================
Total params: 1,101,703
Trainable params: 1,101,703
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 64)                320       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 256)               16640     
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 512)               131584    
_________________________________________________________________
encoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
encoding_layer_5 (Dense)     (None, 256)               131328    
_________________________________________________________________
encoding_layer_6 (Dense)     (None, 32)                8224      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 99        
=================================================================
Total params: 550,851
Trainable params: 550,851
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 32)                128       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 256)               8448      
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 512)               131584    
_________________________________________________________________
decoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
decoding_layer_5 (Dense)     (None, 256)               131328    
_________________________________________________________________
decoding_layer_6 (Dense)     (None, 64)                16448     
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 260       
=================================================================
Total params: 550,852
Trainable params: 550,852
Non-trainable params: 0
_________________________________________________________________
