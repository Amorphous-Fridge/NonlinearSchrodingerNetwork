2021-06-26
loss,4.107998371124268,3.835879325866699,3.8292646408081055,3.829296588897705,3.8297083377838135,3.8293707370758057,3.8294315338134766,3.8286588191986084,3.8288707733154297,3.82975435256958,3.8292746543884277,3.828932762145996,3.828829050064087,3.829141855239868,3.8685803413391113,3.876328706741333,4.71035099029541,4.4240403175354,4.424228668212891,4.424194812774658,4.424123764038086,4.424246788024902,4.424242973327637,4.424096584320068,4.42418098449707,4.424098968505859,4.4240827560424805,4.424165725708008,4.424436569213867,4.4242939949035645,4.424155235290527,4.424264430999756,4.4241204261779785,4.424150466918945,4.424294948577881,4.424034118652344,4.424090385437012,4.424045085906982,4.424267292022705,4.424106597900391,4.424241542816162,4.4244465827941895,4.424193859100342,4.424158096313477,4.424265384674072,4.424158573150635,4.424406051635742,4.424283981323242,4.4242448806762695,4.4242095947265625,4.4242119789123535,4.4243364334106445,4.42402982711792,4.424332141876221,4.4243268966674805,4.424235820770264,4.424556255340576,4.42411470413208,4.424169540405273,4.424319744110107,4.4241719245910645,4.424393653869629,4.424007892608643,4.424318313598633,4.4243621826171875,4.424279689788818,4.424554347991943,4.424163341522217,4.424264430999756,4.424405097961426,4.424365043640137,4.424539566040039,4.424356460571289,4.424185752868652,4.424319267272949,4.424215793609619,4.42430305480957,4.424251556396484,4.424370288848877,4.424248695373535,4.423981189727783,4.424102306365967,4.424158096313477,4.424263954162598,4.42401123046875,4.424025058746338,4.424293518066406,4.423892974853516,4.42414665222168,4.424350738525391,4.424137115478516,4.4240403175354,4.424417018890381,4.424151420593262,4.423994064331055,4.424290657043457,4.424231052398682,4.424233913421631,4.424107551574707,4.424273490905762,
mse,4.357211112976074,3.6837880611419678,3.6711270809173584,3.671128273010254,3.6719281673431396,3.6713128089904785,3.6714086532592773,3.669968843460083,3.6703591346740723,3.6720571517944336,3.6711313724517822,3.6704580783843994,3.6703014373779297,3.6708719730377197,3.7490923404693604,3.7660019397735596,7.804229736328125,4.893928527832031,4.894347667694092,4.894270420074463,4.894111633300781,4.894382953643799,4.8943705558776855,4.894055366516113,4.89423942565918,4.89405632019043,4.894018650054932,4.894208908081055,4.8947978019714355,4.894482612609863,4.894182205200195,4.894425392150879,4.894105434417725,4.894171237945557,4.894494533538818,4.893918037414551,4.894038677215576,4.893941879272461,4.8944315910339355,4.894075870513916,4.894367218017578,4.894824028015137,4.894259452819824,4.894190788269043,4.894427299499512,4.894189357757568,4.894735813140869,4.894467353820801,4.894378662109375,4.894305229187012,4.894308567047119,4.894582748413086,4.89390754699707,4.894571304321289,4.894556999206543,4.894356727600098,4.895060062408447,4.894090175628662,4.894211769104004,4.894540309906006,4.894211292266846,4.894705772399902,4.893857955932617,4.894537448883057,4.89463472366333,4.894455432891846,4.8950605392456055,4.894198417663574,4.894423961639404,4.894731521606445,4.8946452140808105,4.895031452178955,4.894628524780273,4.894250392913818,4.894536018371582,4.894313335418701,4.894500255584717,4.8943915367126465,4.8946533203125,4.894385814666748,4.893800258636475,4.894063472747803,4.8941850662231445,4.8944172859191895,4.8938703536987305,4.893892288208008,4.894487380981445,4.893601894378662,4.894164562225342,4.8946123123168945,4.894138336181641,4.893926620483398,4.894754886627197,4.894177436828613,4.893828868865967,4.8944783210754395,4.894351959228516,4.894361972808838,4.894078731536865,4.894441604614258,
mae,1.9525768756866455,1.6906734704971313,1.6565790176391602,1.6540907621383667,1.653425931930542,1.6526681184768677,1.6522679328918457,1.6516139507293701,1.6516450643539429,1.6658825874328613,1.653099775314331,1.6514564752578735,1.6512603759765625,1.6514325141906738,1.7188961505889893,1.7473766803741455,2.0631778240203857,2.1997742652893066,2.19989275932312,2.1998708248138428,2.1998257637023926,2.1999030113220215,2.199899673461914,2.1998095512390137,2.199861764907837,2.199810028076172,2.1997992992401123,2.1998536586761475,2.200021743774414,2.1999313831329346,2.199845314025879,2.1999154090881348,2.199824333190918,2.199842929840088,2.199934959411621,2.199770927429199,2.1998050212860107,2.199777841567993,2.1999170780181885,2.199815273284912,2.1998987197875977,2.200028419494629,2.1998682022094727,2.1998488903045654,2.199915885925293,2.199848175048828,2.2000038623809814,2.19992733001709,2.199902057647705,2.199881076812744,2.1998822689056396,2.1999599933624268,2.1997673511505127,2.1999566555023193,2.199953079223633,2.1998960971832275,2.2000956535339355,2.1998202800750732,2.1998543739318848,2.1999475955963135,2.1998543739318848,2.1999943256378174,2.199753761291504,2.199946880340576,2.199974536895752,2.199923515319824,2.2000958919525146,2.1998510360717773,2.1999146938323975,2.2000017166137695,2.1999776363372803,2.200087308883667,2.1999728679656982,2.1998653411865234,2.199946641921997,2.199883460998535,2.199936628341675,2.1999053955078125,2.199979782104492,2.199903964996338,2.199737787246704,2.199812173843384,2.1998465061187744,2.1999130249023438,2.1997573375701904,2.1997640132904053,2.1999330520629883,2.199681282043457,2.199840784072876,2.1999683380126953,2.199833631515503,2.199773073196411,2.2000086307525635,2.1998443603515625,2.1997458934783936,2.199930191040039,2.1998941898345947,2.199897527694702,2.1998164653778076,2.1999194622039795,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 559171    
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 559172    
=================================================================
Total params: 1,118,343
Trainable params: 1,118,343
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 64)                320       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 256)               16640     
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 512)               131584    
_________________________________________________________________
encoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
encoding_layer_5 (Dense)     (None, 256)               131328    
_________________________________________________________________
encoding_layer_6 (Dense)     (None, 64)                16448     
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 195       
=================================================================
Total params: 559,171
Trainable params: 559,171
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 64)                256       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 256)               16640     
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 512)               131584    
_________________________________________________________________
decoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
decoding_layer_5 (Dense)     (None, 256)               131328    
_________________________________________________________________
decoding_layer_6 (Dense)     (None, 64)                16448     
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 260       
=================================================================
Total params: 559,172
Trainable params: 559,172
Non-trainable params: 0
_________________________________________________________________
