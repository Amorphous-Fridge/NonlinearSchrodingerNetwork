2021-06-26
loss,0.5240615606307983,0.23423771560192108,0.12369021773338318,0.0951509103178978,0.12922422587871552,0.0788203701376915,0.08745373785495758,0.10334904491901398,0.06923229992389679,0.10182209312915802,0.08946576714515686,0.08527743071317673,0.10547056049108505,0.09649433940649033,0.08044613152742386,0.04364058002829552,0.05702488124370575,0.061646055430173874,0.04558965563774109,0.042530592530965805,0.041002389043569565,0.050051745027303696,0.07125895470380783,0.061586443334817886,0.05749659612774849,0.053499653935432434,0.05118599534034729,0.04848681017756462,0.06642995029687881,0.05787816643714905,0.05678638443350792,0.05138855054974556,0.04450191929936409,0.03663701191544533,0.04478012025356293,0.05100766569375992,0.053613461554050446,0.0477432906627655,0.04420430213212967,0.05027589201927185,0.04651305079460144,0.04267655313014984,0.037973228842020035,0.03133107349276543,0.029240887612104416,0.03696518763899803,0.03824685886502266,0.045657724142074585,0.0497862808406353,0.041277337819337845,0.04175069183111191,0.04427443817257881,0.039041049778461456,0.0316183939576149,0.0338422916829586,0.040082912892103195,0.04340582340955734,0.04156392067670822,0.03589961677789688,0.03469914570450783,0.031190473586320877,0.03749934211373329,0.03989027813076973,0.03762003406882286,0.03278432413935661,0.036324381828308105,0.03421121463179588,0.029996182769536972,0.026038849726319313,0.035112034529447556,0.03429913520812988,0.03262612596154213,0.03674205392599106,0.03762024641036987,0.03365490213036537,0.027448022738099098,0.02471490204334259,0.024221057072281837,0.02224550023674965,0.03216519206762314,0.03729448840022087,0.03274782374501228,0.028203582391142845,0.03253106772899628,0.029435433447360992,0.02860972471535206,0.026216277852654457,0.024087291210889816,0.02346893586218357,0.023301277309656143,0.021464558318257332,0.02808060124516487,0.034627046436071396,0.03657930716872215,0.031571581959724426,0.031178193166851997,0.03176334127783775,0.028130073100328445,0.024497831240296364,0.0255600493401289,
mse,0.09756889939308167,0.018206840381026268,0.004616936668753624,0.0026808169204741716,0.005187488626688719,0.0018211239948868752,0.0022020628675818443,0.003166968934237957,0.0013722797157242894,0.003407272743061185,0.002274750964716077,0.002179679460823536,0.003298912663012743,0.0025622043758630753,0.0018318073125556111,0.0005654271226376295,0.0009318948723375797,0.0010792036773636937,0.0005944163422100246,0.0005133569939061999,0.0004676886019296944,0.0007408439414575696,0.0014328707475215197,0.00108781224116683,0.0009066408965736628,0.000800680834800005,0.0007146854186430573,0.0007116153719834983,0.0012392179341986775,0.0009578126482665539,0.0008796692709438503,0.000740932475309819,0.0005665433709509671,0.0003963798226322979,0.0005786667461507022,0.0007272184011526406,0.0008148372871801257,0.0006258066860027611,0.0005709342076443136,0.0007128252182155848,0.0005897796945646405,0.0005007626605220139,0.0004044826200697571,0.0002781665825750679,0.0002414825139567256,0.0003905203484464437,0.000413680070778355,0.0005815720651298761,0.0006940559251233935,0.0004724677710328251,0.0004893974401056767,0.0005413655890151858,0.0004352469404693693,0.00029377330793067813,0.0003240849473513663,0.0004569084558170289,0.0005138150881975889,0.0004767307545989752,0.00035946533898822963,0.0003610769927036017,0.0002833172620739788,0.0003915793786291033,0.00043818450649268925,0.00039676669985055923,0.00030453180079348385,0.00037027616053819656,0.0003185473324265331,0.00025064341025426984,0.00019546999828889966,0.0003537176817189902,0.00032684762845747173,0.0002927343302872032,0.00038038488128222525,0.0003934107953682542,0.000308229704387486,0.0002161587035516277,0.0001754827972035855,0.00016849080566316843,0.0001377845910610631,0.0002927690511569381,0.0003858034033328295,0.00029892491875216365,0.00022491594427265227,0.0002906698500737548,0.00023512639745604247,0.00022421899484470487,0.000193501211469993,0.00016778479039203376,0.0001598730159457773,0.00015071185771375895,0.00012970490206498653,0.000220497531699948,0.0003423506859689951,0.00037437648279592395,0.00028230264433659613,0.0002701910270843655,0.0002792115556076169,0.00022320181597024202,0.00016558814968448132,0.00019180348317604512,
mae,0.2265087366104126,0.10383257269859314,0.053454622626304626,0.0407274104654789,0.05629052221775055,0.033755578100681305,0.03760434687137604,0.044078532606363297,0.029642660170793533,0.043693557381629944,0.03866737335920334,0.03605423867702484,0.04610292613506317,0.041683293879032135,0.035888418555259705,0.018534105271100998,0.024201519787311554,0.02737545408308506,0.0194382481276989,0.018193304538726807,0.0177433080971241,0.0214507058262825,0.031032925471663475,0.025856398046016693,0.025379950180649757,0.023055994883179665,0.023117752745747566,0.02113836444914341,0.029497196897864342,0.025281129404902458,0.023829152807593346,0.02275853604078293,0.01905418001115322,0.015712710097432137,0.019151704385876656,0.02198699675500393,0.0233147032558918,0.02077370695769787,0.01913168467581272,0.02233491651713848,0.020541131496429443,0.018996551632881165,0.016693685203790665,0.013594936579465866,0.012630951590836048,0.015999404713511467,0.01649429276585579,0.019667917862534523,0.021983778104186058,0.018318409100174904,0.01844475045800209,0.018587099388241768,0.016642384231090546,0.013370204716920853,0.014199232682585716,0.017222188413143158,0.01878073625266552,0.018847130239009857,0.015579641796648502,0.014607804827392101,0.0133372126147151,0.016100607812404633,0.016850247979164124,0.015872633084654808,0.014080210588872433,0.015890508890151978,0.014938782900571823,0.013114666566252708,0.011186557821929455,0.014997929334640503,0.014789505861699581,0.014353478327393532,0.015353861264884472,0.016374792903661728,0.01454852893948555,0.011583474464714527,0.01043202169239521,0.010343980975449085,0.009508561342954636,0.013777646236121655,0.016492566093802452,0.014336076565086842,0.012268600054085255,0.013885575346648693,0.012308609671890736,0.011921984143555164,0.011197347193956375,0.010299401357769966,0.009991022758185863,0.0101638063788414,0.009261250495910645,0.011893799528479576,0.014851403422653675,0.015378790907561779,0.013507001101970673,0.013186855241656303,0.01378722209483385,0.012216564267873764,0.010635001584887505,0.01098512765020132,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 25483     
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 25484     
=================================================================
Total params: 50,967
Trainable params: 50,967
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 8)                 40        
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 32)                288       
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 128)               4224      
_________________________________________________________________
encoding_layer_4 (Dense)     (None, 128)               16512     
_________________________________________________________________
encoding_layer_5 (Dense)     (None, 32)                4128      
_________________________________________________________________
encoding_layer_6 (Dense)     (None, 8)                 264       
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 27        
=================================================================
Total params: 25,483
Trainable params: 25,483
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 8)                 32        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 32)                288       
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 128)               4224      
_________________________________________________________________
decoding_layer_4 (Dense)     (None, 128)               16512     
_________________________________________________________________
decoding_layer_5 (Dense)     (None, 32)                4128      
_________________________________________________________________
decoding_layer_6 (Dense)     (None, 8)                 264       
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 36        
=================================================================
Total params: 25,484
Trainable params: 25,484
Non-trainable params: 0
_________________________________________________________________
