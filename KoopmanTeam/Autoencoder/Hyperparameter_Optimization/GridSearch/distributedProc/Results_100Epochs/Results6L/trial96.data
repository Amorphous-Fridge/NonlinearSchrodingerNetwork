2021-06-26
loss,4.410147190093994,4.424114227294922,4.424063205718994,4.423915863037109,4.42420768737793,4.424282073974609,4.424136161804199,4.553647994995117,4.416520595550537,4.424186706542969,4.424211502075195,4.424428939819336,4.424170017242432,4.424488067626953,4.4241414070129395,4.424093246459961,4.424286842346191,4.424131870269775,4.424051284790039,4.424105644226074,4.424272537231445,4.424079895019531,4.424109935760498,4.424181938171387,4.4240617752075195,4.424225330352783,4.424325466156006,4.4240288734436035,4.424335479736328,4.4242262840271,4.424180030822754,4.424391269683838,4.424117565155029,4.424277305603027,4.424312114715576,4.423860549926758,4.424074172973633,4.424179553985596,4.424354076385498,4.424114227294922,4.424288749694824,4.424348831176758,4.42423152923584,4.423987865447998,4.424394130706787,4.424098968505859,4.424222469329834,4.424485683441162,4.424243450164795,4.423966884613037,4.424360752105713,4.4242448806762695,4.424506664276123,4.424283981323242,4.424400329589844,4.424211025238037,4.424253463745117,4.424200057983398,4.424101829528809,4.424429416656494,4.424297332763672,4.424232006072998,4.424246311187744,4.424279689788818,4.424217700958252,4.424218654632568,4.424218654632568,4.423956871032715,4.424182415008545,4.424094200134277,4.424339771270752,4.424474716186523,4.424158573150635,4.424392223358154,4.424149990081787,4.424240589141846,4.424124240875244,4.424285411834717,4.423880100250244,4.424103736877441,4.42417049407959,4.424052715301514,4.424253463745117,4.424177169799805,4.424306392669678,4.424196243286133,4.424258232116699,4.424264907836914,4.424100399017334,4.424196720123291,4.424215793609619,4.4244065284729,4.423866271972656,4.424126148223877,4.4239583015441895,4.424450874328613,4.42409086227417,4.424159049987793,4.424217700958252,4.424048900604248,
mse,4.9500813484191895,4.894088268280029,4.893980503082275,4.893653392791748,4.894294738769531,4.8944573402404785,4.894144058227539,5.400896072387695,4.8781843185424805,4.894251346588135,4.894298553466797,4.894784450531006,4.894214153289795,4.894914627075195,4.89415168762207,4.894043922424316,4.894472122192383,4.894131660461426,4.893960475921631,4.894069671630859,4.894437313079834,4.894013404846191,4.894082069396973,4.89423942565918,4.893974781036377,4.894336700439453,4.894557952880859,4.893908500671387,4.894580841064453,4.894340515136719,4.894238471984863,4.894700527191162,4.894099712371826,4.894445896148682,4.894525051116943,4.893533229827881,4.894003868103027,4.894238471984863,4.894618034362793,4.894090175628662,4.894469738006592,4.894604682922363,4.894347667694092,4.8938093185424805,4.894706726074219,4.894053936004639,4.894330024719238,4.894906997680664,4.894370079040527,4.893768787384033,4.894636631011963,4.8943772315979,4.894956111907959,4.894465446472168,4.894723892211914,4.89431095123291,4.8944010734558105,4.894280910491943,4.894061088562012,4.894781589508057,4.89449405670166,4.894349575042725,4.894384384155273,4.894455909729004,4.894320487976074,4.894318103790283,4.894315242767334,4.893744945526123,4.894240856170654,4.894044876098633,4.894591808319092,4.894887447357178,4.894190311431885,4.8947038650512695,4.894170761108398,4.89437198638916,4.894113063812256,4.894467353820801,4.8935723304748535,4.894064426422119,4.8942108154296875,4.893954277038574,4.894402980804443,4.894222736358643,4.894513130187988,4.894268989562988,4.894407749176025,4.894423961639404,4.894057750701904,4.894269943237305,4.894315242767334,4.8947367668151855,4.893542289733887,4.894120693206787,4.89375114440918,4.894832611083984,4.894037246704102,4.894190788269043,4.894321441650391,4.893945217132568,
mae,2.1748459339141846,2.199819803237915,2.1997897624969482,2.1996960639953613,2.199878692626953,2.1999258995056152,2.1998372077941895,2.244053602218628,2.1953482627868652,2.1998658180236816,2.1998791694641113,2.200016975402832,2.199854850769043,2.200054407119751,2.1998367309570312,2.1998069286346436,2.1999285221099854,2.199831962585449,2.199782609939575,2.1998133659362793,2.199918270111084,2.1997978687286377,2.199817657470703,2.199862241744995,2.199786901473999,2.199889898300171,2.1999528408050537,2.199767827987671,2.1999597549438477,2.1998915672302246,2.199862003326416,2.199993371963501,2.199822425842285,2.1999213695526123,2.1999433040618896,2.1996614933013916,2.1997954845428467,2.199862241744995,2.19996976852417,2.199819564819336,2.199927806854248,2.1999661922454834,2.199892997741699,2.199739933013916,2.199995517730713,2.1998097896575928,2.1998887062072754,2.200051784515381,2.199899673461914,2.199728488922119,2.1999752521514893,2.199901819229126,2.200065851211548,2.1999268531799316,2.200000047683716,2.1998822689056396,2.1999082565307617,2.199873685836792,2.1998119354248047,2.2000162601470947,2.199934244155884,2.1998932361602783,2.1999034881591797,2.1999237537384033,2.1998848915100098,2.1998844146728516,2.1998839378356934,2.1997220516204834,2.1998629570007324,2.1998071670532227,2.199962854385376,2.2000467777252197,2.1998488903045654,2.1999943256378174,2.1998424530029297,2.199899673461914,2.199826240539551,2.19992733001709,2.1996724605560303,2.199812412261963,2.1998538970947266,2.1997807025909424,2.199908494949341,2.1998579502105713,2.1999402046203613,2.1998708248138428,2.1999096870422363,2.1999149322509766,2.199810743331909,2.199871301651001,2.1998839378356934,2.2000033855438232,2.1996636390686035,2.1998283863067627,2.199723482131958,2.20003080368042,2.1998045444488525,2.1998484134674072,2.199885368347168,2.1997787952423096,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 411459    
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 411460    
=================================================================
Total params: 822,919
Trainable params: 822,919
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 64)                320       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 128)               8320      
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 512)               66048     
_________________________________________________________________
encoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
encoding_layer_5 (Dense)     (None, 128)               65664     
_________________________________________________________________
encoding_layer_6 (Dense)     (None, 64)                8256      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 195       
=================================================================
Total params: 411,459
Trainable params: 411,459
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 64)                256       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 128)               8320      
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 512)               66048     
_________________________________________________________________
decoding_layer_4 (Dense)     (None, 512)               262656    
_________________________________________________________________
decoding_layer_5 (Dense)     (None, 128)               65664     
_________________________________________________________________
decoding_layer_6 (Dense)     (None, 64)                8256      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 260       
=================================================================
Total params: 411,460
Trainable params: 411,460
Non-trainable params: 0
_________________________________________________________________
