2021-06-26
loss,3.855027437210083,3.8304266929626465,3.8287465572357178,3.829629421234131,3.8292999267578125,3.8290276527404785,3.8295106887817383,3.829744815826416,3.8291656970977783,3.8294296264648438,3.8296027183532715,3.828439712524414,3.829197406768799,3.8293983936309814,3.829336643218994,3.9586098194122314,2.9807281494140625,2.2392256259918213,1.4212208986282349,0.4522310197353363,0.44485679268836975,0.44624659419059753,0.4462811350822449,0.44714346528053284,0.4461630880832672,0.4475449323654175,0.4455116391181946,0.44550129771232605,0.4466748833656311,0.4466961920261383,0.4452262222766876,0.44506335258483887,0.4454190731048584,0.4459947943687439,0.4459936022758484,0.4466422200202942,0.44513198733329773,0.4456411302089691,0.44591858983039856,0.44523489475250244,0.4457494616508484,0.44516292214393616,0.4459253251552582,0.4457005262374878,0.4449525475502014,0.445196270942688,0.44621920585632324,0.44566139578819275,0.4469188451766968,0.4447133243083954,0.4459838569164276,0.445387601852417,0.44605782628059387,0.44495606422424316,0.4450662136077881,0.44518253207206726,0.44490280747413635,0.44620734453201294,0.4466177821159363,0.4461728036403656,0.44567862153053284,0.44528284668922424,0.44520172476768494,0.44600266218185425,0.44605666399002075,0.44463634490966797,0.44607865810394287,0.4455513656139374,0.44620808959007263,0.4449533522129059,0.4452785849571228,0.44523173570632935,0.4442063868045807,0.4457627534866333,0.44566208124160767,0.44662559032440186,0.44573265314102173,0.4448867440223694,0.4448128044605255,0.4453786611557007,0.4466332197189331,0.4454638957977295,0.44555962085723877,0.44563841819763184,0.44570788741111755,0.44592902064323425,0.44465938210487366,0.44576847553253174,0.4449952244758606,0.44561827182769775,0.44482481479644775,0.44504058361053467,0.44599127769470215,0.44561755657196045,0.44516313076019287,0.4461047053337097,0.4445759952068329,0.44589152932167053,0.44537603855133057,0.4461957812309265,
mse,3.752742052078247,3.673313617706299,3.6701390743255615,3.6718008518218994,3.67118501663208,3.6706502437591553,3.6715712547302246,3.6720218658447266,3.6709506511688232,3.6714022159576416,3.6717405319213867,3.6695375442504883,3.6709847450256348,3.671368360519409,3.6712257862091064,3.957659959793091,2.3489933013916016,1.2664945125579834,0.6869558095932007,0.05712117254734039,0.05517981946468353,0.055526625365018845,0.055501069873571396,0.055710021406412125,0.05543704330921173,0.05593332275748253,0.05534587427973747,0.05528409406542778,0.05561518296599388,0.05560915544629097,0.05526244640350342,0.055204473435878754,0.05525246635079384,0.055428050458431244,0.05543232336640358,0.05560377985239029,0.0552455298602581,0.0553865060210228,0.05540701001882553,0.05524899438023567,0.055355772376060486,0.055235300213098526,0.05543186143040657,0.055361732840538025,0.05520908907055855,0.05521777644753456,0.055511727929115295,0.0553460493683815,0.05568985641002655,0.05510759353637695,0.0554499588906765,0.0553094707429409,0.0554349422454834,0.05517788976430893,0.05516592413187027,0.0552058070898056,0.055165696889162064,0.05547812953591347,0.05554931238293648,0.0554700642824173,0.05534582957625389,0.05521346628665924,0.05522604286670685,0.055429864674806595,0.05542511120438576,0.055117350071668625,0.05543748289346695,0.055306121706962585,0.05543087422847748,0.05522124841809273,0.0552835576236248,0.05523466691374779,0.05500583350658417,0.05534260720014572,0.05535178259015083,0.05556783080101013,0.055329836905002594,0.05514400079846382,0.05514286085963249,0.055305104702711105,0.05558020994067192,0.05529690906405449,0.05528601258993149,0.05531447380781174,0.05536641180515289,0.05539694428443909,0.05511445179581642,0.0554276704788208,0.05517933890223503,0.05532606691122055,0.055177200585603714,0.0551781952381134,0.055405277758836746,0.05532163754105568,0.05521193519234657,0.05542447417974472,0.05511481314897537,0.05539451166987419,0.055259671062231064,0.05548281967639923,
mae,1.7649914026260376,1.6702133417129517,1.6545000076293945,1.6532570123672485,1.6523040533065796,1.6517101526260376,1.6516345739364624,1.6515426635742188,1.6511520147323608,1.6511646509170532,1.6511813402175903,1.6506136655807495,1.6509153842926025,1.6509901285171509,1.6510058641433716,1.801350712776184,1.2190814018249512,0.7024306654930115,0.5283196568489075,0.19990991055965424,0.19728004932403564,0.19780315458774567,0.19790969789028168,0.19811271131038666,0.19790805876255035,0.19825595617294312,0.1975688338279724,0.19756250083446503,0.19803379476070404,0.19804757833480835,0.19748026132583618,0.19736289978027344,0.19755855202674866,0.1976364552974701,0.19780096411705017,0.19795015454292297,0.1974216103553772,0.19758544862270355,0.19767604768276215,0.19750265777111053,0.19774986803531647,0.19739389419555664,0.1977374255657196,0.19758962094783783,0.19731462001800537,0.19734014570713043,0.1977885663509369,0.19758300483226776,0.19805961847305298,0.1971406787633896,0.19774170219898224,0.19748768210411072,0.19783955812454224,0.19730490446090698,0.19735132157802582,0.19733543694019318,0.19728708267211914,0.19779260456562042,0.19804653525352478,0.19781237840652466,0.19754169881343842,0.19749870896339417,0.19746656715869904,0.19757919013500214,0.1977919042110443,0.1972312182188034,0.1978122740983963,0.19756963849067688,0.19790033996105194,0.197222501039505,0.1974545419216156,0.1974286586046219,0.1969612091779709,0.1977003514766693,0.19754642248153687,0.197998046875,0.1975535899400711,0.19730423390865326,0.19723844528198242,0.1974562108516693,0.19796761870384216,0.19761060178279877,0.19752174615859985,0.19760054349899292,0.19769442081451416,0.1977800726890564,0.19718219339847565,0.1976514458656311,0.19734878838062286,0.19755497574806213,0.1972777247428894,0.19740860164165497,0.19776535034179688,0.19752714037895203,0.19734680652618408,0.19785287976264954,0.19714948534965515,0.19775915145874023,0.19753515720367432,0.1977744698524475,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 288163    
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 288164    
=================================================================
Total params: 576,327
Trainable params: 576,327
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 32)                160       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 256)               8448      
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 512)               131584    
_________________________________________________________________
encoding_layer_4 (Dense)     (None, 256)               131328    
_________________________________________________________________
encoding_layer_5 (Dense)     (None, 64)                16448     
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 195       
=================================================================
Total params: 288,163
Trainable params: 288,163
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 64)                256       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 256)               16640     
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 512)               131584    
_________________________________________________________________
decoding_layer_4 (Dense)     (None, 256)               131328    
_________________________________________________________________
decoding_layer_5 (Dense)     (None, 32)                8224      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 132       
=================================================================
Total params: 288,164
Trainable params: 288,164
Non-trainable params: 0
_________________________________________________________________
