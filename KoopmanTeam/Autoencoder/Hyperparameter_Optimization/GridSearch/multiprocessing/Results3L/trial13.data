2021-06-26
loss,12.76456069946289,5.1396918296813965,3.4481232166290283,2.977583408355713,2.90701961517334,3.9125559329986572,3.33558988571167,2.426546096801758,2.5789949893951416,1.845505952835083,2.255145788192749,2.798673629760742,2.0201361179351807,2.272808074951172,2.2211902141571045,1.9183282852172852,2.029581308364868,0.8617376685142517,1.5017765760421753,2.6411099433898926,1.1508334875106812,1.238936424255371,1.010194182395935,0.8510892391204834,1.320631980895996,1.3326005935668945,0.9619892239570618,0.7910516262054443,0.8489654660224915,1.391790509223938,1.0598981380462646,0.7703020572662354,0.8074564933776855,0.7905673384666443,0.8154834508895874,0.9177911281585693,0.9875454902648926,0.6886545419692993,0.4901176691055298,0.5925784111022949,0.6273573637008667,0.6914145946502686,0.9601991772651672,0.44206345081329346,0.5565780401229858,0.6548214554786682,1.0968010425567627,0.7853547930717468,1.6291333436965942,1.8632721900939941,0.8049904704093933,0.6701400279998779,0.7370378375053406,0.9162208437919617,1.060774803161621,0.8944140672683716,0.9771053791046143,1.4781746864318848,0.5400841236114502,0.620326817035675,0.6105844974517822,0.585800290107727,1.1339926719665527,0.9758133888244629,0.9621590971946716,0.6998174786567688,0.6960958242416382,0.6940548419952393,0.5300838947296143,0.529573917388916,0.6743886470794678,1.0598437786102295,1.4084681272506714,1.0204297304153442,0.6634918451309204,0.6279343962669373,0.7940326929092407,0.5938804745674133,0.7926108837127686,0.9637718796730042,0.5662918090820312,0.5359246730804443,0.7140905261039734,0.715232789516449,0.720183253288269,0.6624996066093445,0.6510983109474182,0.4982663691043854,0.8739712238311768,0.7038822770118713,1.0068186521530151,0.6744125485420227,0.7366880178451538,0.6917217969894409,0.7564563751220703,0.47366711497306824,0.6963043808937073,0.4589272737503052,0.5459511280059814,0.5403520464897156,
mse,0.6067811250686646,0.6305593252182007,0.5414302945137024,0.5227751135826111,0.5255822539329529,0.4970394968986511,0.4893798828125,0.5405961871147156,0.5379121899604797,0.5380480289459229,0.5548714995384216,0.5936711430549622,0.6331174969673157,0.6320052742958069,0.635215163230896,0.6558495163917542,0.6631986498832703,0.6465832591056824,0.6588014960289001,0.6658496856689453,0.6391180753707886,0.652346134185791,0.6676504611968994,0.680478572845459,0.6968609690666199,0.7074549198150635,0.7072179913520813,0.6977512836456299,0.6966716647148132,0.7128788232803345,0.7171376943588257,0.7090129852294922,0.7090975046157837,0.6981207132339478,0.6969488263130188,0.7006914019584656,0.6967981457710266,0.6913176774978638,0.6893653869628906,0.6866517066955566,0.6940151453018188,0.6951980590820312,0.6862117648124695,0.6899076104164124,0.688275933265686,0.6909989714622498,0.6994980573654175,0.692644476890564,0.6610970497131348,0.6721035838127136,0.6944596171379089,0.6928781270980835,0.689797043800354,0.6786115169525146,0.6797813177108765,0.6866558194160461,0.6890869140625,0.6977196335792542,0.6979110240936279,0.6916377544403076,0.6890708208084106,0.6923226118087769,0.6966320872306824,0.7096413373947144,0.6807396411895752,0.6716954708099365,0.6836570501327515,0.6873958706855774,0.6836717128753662,0.6860490441322327,0.6797437071800232,0.6918599009513855,0.6944595575332642,0.7011972069740295,0.6996473670005798,0.6999441385269165,0.6976041197776794,0.6988418102264404,0.6901068687438965,0.7012324333190918,0.7007505893707275,0.6945599913597107,0.691209077835083,0.7001670598983765,0.7069194912910461,0.7039639949798584,0.698813796043396,0.6933214664459229,0.695293664932251,0.703456699848175,0.7150702476501465,0.7132689952850342,0.7039543986320496,0.6976312398910522,0.6832355260848999,0.6963205933570862,0.6977787017822266,0.6956915259361267,0.6932437419891357,0.6947125792503357,
mae,0.6233952045440674,0.6295621991157532,0.5814343690872192,0.5712882876396179,0.5722944140434265,0.5637782216072083,0.5536186099052429,0.5859119892120361,0.5913440585136414,0.5867003798484802,0.5999033451080322,0.6314382553100586,0.6650960445404053,0.6502050161361694,0.6513113975524902,0.6666774153709412,0.6704177856445312,0.653218686580658,0.6648190021514893,0.6838536262512207,0.6540387868881226,0.651818037033081,0.6476003527641296,0.6479337215423584,0.6575558185577393,0.6597511172294617,0.6589644551277161,0.6432045102119446,0.6432438492774963,0.6559916138648987,0.6596879363059998,0.6591350436210632,0.6583442687988281,0.6485276818275452,0.6469231247901917,0.6468122601509094,0.6445553302764893,0.639610230922699,0.640245258808136,0.6315513253211975,0.6446503400802612,0.6416265964508057,0.6389669179916382,0.6401124000549316,0.6345744132995605,0.6382759213447571,0.6460314393043518,0.638656735420227,0.6211917996406555,0.6258447170257568,0.6412115693092346,0.6446031332015991,0.6394186615943909,0.6339151263237,0.6259588003158569,0.6365857124328613,0.6398135423660278,0.644706666469574,0.6465251445770264,0.6377094388008118,0.6323857307434082,0.636349618434906,0.643198549747467,0.6593578457832336,0.6418681144714355,0.6318253874778748,0.6369866132736206,0.6363755464553833,0.6306399703025818,0.6309968829154968,0.6269019246101379,0.6405948400497437,0.640201210975647,0.6510671973228455,0.6456385850906372,0.6443694829940796,0.6463930010795593,0.6440456509590149,0.6390432119369507,0.6469502449035645,0.6426154375076294,0.6394261121749878,0.6388318538665771,0.6484782695770264,0.6531616449356079,0.6499099731445312,0.6428356766700745,0.6416709423065186,0.6439899206161499,0.6494283080101013,0.6634191274642944,0.6598387956619263,0.6497570872306824,0.6663785576820374,0.6546312570571899,0.6551871299743652,0.654075026512146,0.6445615291595459,0.641853392124176,0.6435982584953308,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 12867     
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 12868     
=================================================================
Total params: 25,735
Trainable params: 25,735
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 64)                320       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 128)               8320      
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 32)                4128      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 99        
=================================================================
Total params: 12,867
Trainable params: 12,867
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 32)                128       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 128)               4224      
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 64)                8256      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 260       
=================================================================
Total params: 12,868
Trainable params: 12,868
Non-trainable params: 0
_________________________________________________________________
