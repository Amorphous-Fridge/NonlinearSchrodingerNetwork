2021-06-26
loss,8.729557037353516,5.183377265930176,7.748021125793457,4.073934555053711,3.539053440093994,4.079619407653809,2.3319289684295654,2.9080519676208496,3.043614149093628,2.11684513092041,2.209045171737671,2.315002679824829,2.220982551574707,2.8982326984405518,2.6568734645843506,3.6969993114471436,2.9979448318481445,2.198347806930542,2.641012191772461,2.4795663356781006,2.8059585094451904,1.9249171018600464,2.5156090259552,1.8029060363769531,1.8989651203155518,1.73746919631958,2.083749771118164,2.4857399463653564,2.014247179031372,1.4278182983398438,2.0040194988250732,2.141829490661621,1.5005369186401367,1.8530200719833374,1.8904834985733032,1.8018701076507568,1.3671298027038574,1.7932343482971191,1.6333907842636108,1.789175271987915,1.7967159748077393,1.4357866048812866,1.1662631034851074,1.1980849504470825,1.0223156213760376,1.1817131042480469,1.0685619115829468,0.8425545692443848,1.0252739191055298,0.8851559162139893,1.1114085912704468,1.1674199104309082,0.8403312563896179,0.7684381008148193,0.9703649282455444,0.8534658551216125,0.9364493489265442,0.844471275806427,0.6195231080055237,0.8477576375007629,1.0480716228485107,0.8705853819847107,1.1191726922988892,1.5304673910140991,1.1322193145751953,1.0023398399353027,0.9760293364524841,0.7221348285675049,0.6640778183937073,0.8399303555488586,1.0148711204528809,0.7387478351593018,0.7536168098449707,0.6603714823722839,0.9691853523254395,0.6622254848480225,0.9009795188903809,0.669512152671814,0.7380408644676208,0.837433397769928,0.6549177765846252,0.6626914143562317,0.7044164538383484,0.5327568054199219,0.7529534697532654,1.2471098899841309,3.4753613471984863,2.4045681953430176,1.1033695936203003,1.1304192543029785,0.916811466217041,1.0635616779327393,1.1862136125564575,0.8292510509490967,0.8282040357589722,0.8218370079994202,0.6460318565368652,0.669273853302002,0.8079025745391846,0.8202122449874878,
mse,0.7960569262504578,0.7402005195617676,0.5793851613998413,0.6239243745803833,0.647633969783783,0.646740198135376,0.6682018041610718,0.6557518839836121,0.6579946279525757,0.6714490652084351,0.6956835389137268,0.6845080852508545,0.6944835782051086,0.6537165641784668,0.6483508944511414,0.6695260405540466,0.6688603162765503,0.6569055318832397,0.6513410806655884,0.6296498775482178,0.6453742384910583,0.6408935785293579,0.6694872379302979,0.666677713394165,0.6311240196228027,0.6302645206451416,0.6344708800315857,0.6601865291595459,0.6435796618461609,0.6364779472351074,0.6513693928718567,0.6344901323318481,0.6391943097114563,0.6350959539413452,0.6287095546722412,0.6412739753723145,0.6189756989479065,0.6058347821235657,0.6222525238990784,0.6334227919578552,0.6098054647445679,0.6093348264694214,0.5987626910209656,0.6031454205513,0.6112622618675232,0.5960650444030762,0.6128554344177246,0.6123995780944824,0.5989933013916016,0.6021739840507507,0.6081435680389404,0.6120206713676453,0.6062723398208618,0.6107147336006165,0.5980909466743469,0.6011037826538086,0.6052566170692444,0.6194174885749817,0.60146564245224,0.6116198897361755,0.6106769442558289,0.6163564920425415,0.618084728717804,0.5988552570343018,0.5808460116386414,0.6108775734901428,0.6035572290420532,0.5941047668457031,0.5990233421325684,0.5877048373222351,0.5885673761367798,0.5771976709365845,0.5816553235054016,0.5910821557044983,0.5905803442001343,0.5921961069107056,0.5997188091278076,0.6026054620742798,0.5793211460113525,0.5779194235801697,0.5799077153205872,0.5963139533996582,0.586933434009552,0.5928823947906494,0.5914421677589417,0.60349041223526,0.6040027737617493,0.5517498850822449,0.5113955140113831,0.5604861378669739,0.590129017829895,0.5945236086845398,0.5828511714935303,0.581201434135437,0.5845677256584167,0.5974471569061279,0.6011234521865845,0.5914125442504883,0.5780484080314636,0.5581941604614258,
mae,0.748056948184967,0.6953070163726807,0.6061744093894958,0.6025866270065308,0.619375467300415,0.6156308054924011,0.636559247970581,0.6234016418457031,0.6282467246055603,0.6429833173751831,0.6615947484970093,0.6473391652107239,0.6549643874168396,0.6187483668327332,0.6054494380950928,0.6276792287826538,0.6266420483589172,0.6175609230995178,0.6130711436271667,0.5981016755104065,0.6061426401138306,0.5981351733207703,0.6180999875068665,0.6172485947608948,0.5899980664253235,0.5866730213165283,0.594748854637146,0.6124827861785889,0.6026532649993896,0.5919488668441772,0.6050199866294861,0.5920946598052979,0.5922224521636963,0.5902371406555176,0.5863009095191956,0.5963086485862732,0.5823779702186584,0.5721557140350342,0.5876257419586182,0.5998988151550293,0.5795760750770569,0.581149160861969,0.5718046426773071,0.5780484080314636,0.5853689908981323,0.5729937553405762,0.5890755653381348,0.5873203277587891,0.5779604315757751,0.5795373320579529,0.5866295099258423,0.5866202712059021,0.583114504814148,0.5880992412567139,0.5753335952758789,0.5777689814567566,0.5831992030143738,0.594201385974884,0.5769530534744263,0.5879033803939819,0.5873539447784424,0.5911619067192078,0.5943314433097839,0.5791423916816711,0.5637047290802002,0.5874890089035034,0.5798817276954651,0.5730308890342712,0.5801733136177063,0.5655822157859802,0.571643054485321,0.5601533651351929,0.5615742206573486,0.569306492805481,0.5697076320648193,0.571953296661377,0.5773002505302429,0.5811100602149963,0.5582237839698792,0.5598466396331787,0.5602229833602905,0.5763689279556274,0.5645879507064819,0.5729402303695679,0.5696712732315063,0.5827416777610779,0.593346357345581,0.5592251420021057,0.5203527808189392,0.5488228797912598,0.5714933276176453,0.5773974657058716,0.5675662159919739,0.560873806476593,0.5629141330718994,0.5745106339454651,0.5787854790687561,0.5698060989379883,0.5627997517585754,0.5425611734390259,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 3)                 9699      
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 9700      
=================================================================
Total params: 19,399
Trainable params: 19,399
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 64)                320       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 128)               8320      
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 8)                 1032      
_________________________________________________________________
bottleneck (Dense)           (None, 3)                 27        
=================================================================
Total params: 9,699
Trainable params: 9,699
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 3)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 8)                 32        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 128)               1152      
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 64)                8256      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 260       
=================================================================
Total params: 9,700
Trainable params: 9,700
Non-trainable params: 0
_________________________________________________________________
