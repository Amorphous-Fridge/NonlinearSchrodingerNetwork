2021-06-25
loss,7.289373397827148,2.328892469406128,4.372564315795898,4.136916160583496,3.792179584503174,2.4720542430877686,2.6685943603515625,2.3020243644714355,1.4769467115402222,1.5510084629058838,0.9145586490631104,1.7035603523254395,1.2123289108276367,1.4368269443511963,0.9285814166069031,0.9021693468093872,0.9696140289306641,0.7889450192451477,0.8852929472923279,2.311384677886963,1.0031815767288208,1.107627511024475,0.7038456797599792,0.5490484833717346,1.0574049949645996,1.1061985492706299,0.9507148861885071,1.2840795516967773,1.1501365900039673,0.946500301361084,0.6796157956123352,0.6859095096588135,1.204034686088562,1.2370518445968628,0.9223284721374512,0.726033091545105,0.660198450088501,0.7544940114021301,1.1233007907867432,0.6544225811958313,1.0136780738830566,0.9038863182067871,0.8799059987068176,1.0175700187683105,0.743195652961731,0.6459688544273376,0.5669782757759094,0.8356857895851135,0.7642573714256287,0.833108127117157,0.9280969500541687,0.5629362463951111,0.49487608671188354,0.5075851678848267,0.7623345255851746,0.8869668841362,1.1469253301620483,0.8156182169914246,0.6518069505691528,0.6908077001571655,0.6567854881286621,0.6450912356376648,0.6443402171134949,0.8344159722328186,1.2782384157180786,0.7273939251899719,0.66949462890625,0.5353690385818481,0.5224767327308655,0.8879969120025635,0.6017348766326904,0.7862033247947693,0.6372639536857605,0.7854827642440796,0.774614691734314,0.774268388748169,0.7876119017601013,0.6906955242156982,0.701143205165863,0.8297799825668335,0.5480185747146606,0.48652324080467224,0.5604156851768494,0.5904491543769836,0.6363606452941895,0.6478331685066223,0.5555435419082642,0.6794008016586304,0.6351854801177979,0.5758554935455322,0.5944567918777466,0.6168626546859741,0.6724651455879211,0.5092540979385376,0.49013885855674744,0.593838095664978,0.5329586267471313,0.612470805644989,0.503738284111023,0.9264629483222961,
mse,0.6282300353050232,0.7038900852203369,0.708792507648468,0.6898424029350281,0.7162690758705139,0.7211881875991821,0.7177140116691589,0.7026870250701904,0.7057563066482544,0.7149753570556641,0.7218713164329529,0.7189645171165466,0.7188866138458252,0.7212271094322205,0.7090566158294678,0.710628867149353,0.7037568092346191,0.7079300880432129,0.7140284776687622,0.7096845507621765,0.7126641273498535,0.7104803323745728,0.7162352204322815,0.7194616198539734,0.7235954999923706,0.7197772264480591,0.7134129405021667,0.7101247906684875,0.7249290943145752,0.7275202870368958,0.7128221392631531,0.7146784067153931,0.7106821537017822,0.7228637933731079,0.7242591381072998,0.7210884094238281,0.7154691815376282,0.7153447866439819,0.7206423878669739,0.7166216373443604,0.7204115390777588,0.7249950170516968,0.7221865653991699,0.7279245853424072,0.7259611487388611,0.7299989461898804,0.7253190875053406,0.7288695573806763,0.7307324409484863,0.7172418236732483,0.7198628187179565,0.7309259176254272,0.7217786908149719,0.7192666530609131,0.7161238193511963,0.7286723256111145,0.729084312915802,0.7199841141700745,0.7157216668128967,0.7227333784103394,0.7288210988044739,0.7306243777275085,0.722362220287323,0.726870059967041,0.7362532615661621,0.7232130169868469,0.7184321880340576,0.7107167840003967,0.709296464920044,0.7124687433242798,0.7126390933990479,0.7121304273605347,0.713442862033844,0.7182196974754333,0.7258003950119019,0.7224556803703308,0.7250914573669434,0.7244603633880615,0.7280746698379517,0.7272628545761108,0.7252112030982971,0.7249993681907654,0.7260836958885193,0.7241272926330566,0.7205756902694702,0.7179669737815857,0.7243332862854004,0.7191778421401978,0.7203572988510132,0.724895179271698,0.7259537577629089,0.7236793637275696,0.7197238802909851,0.7194404006004333,0.7215316891670227,0.7243061661720276,0.7242110967636108,0.7227857708930969,0.7268427014350891,0.7286717295646667,
mae,0.6086773872375488,0.6715292930603027,0.6453942060470581,0.6204450726509094,0.6330892443656921,0.6382816433906555,0.6309303045272827,0.6172547340393066,0.6305282115936279,0.6459898352622986,0.6570549011230469,0.6544440388679504,0.6397802233695984,0.6528505682945251,0.6462263464927673,0.6608235239982605,0.6464605927467346,0.6526712775230408,0.6535819172859192,0.6437530517578125,0.6570329070091248,0.6534438133239746,0.6564415693283081,0.6577096581459045,0.658748984336853,0.656947910785675,0.6520412564277649,0.6531788110733032,0.6599323153495789,0.6656090617179871,0.6522454619407654,0.6580854058265686,0.6535745859146118,0.6591017842292786,0.6645485758781433,0.6609919667243958,0.6569110751152039,0.6584216952323914,0.6622450947761536,0.6582280993461609,0.662670910358429,0.6644695997238159,0.6647602319717407,0.6705818772315979,0.6667466163635254,0.6721134185791016,0.6710401177406311,0.6729134917259216,0.6744665503501892,0.6681393384933472,0.6661930084228516,0.6772653460502625,0.6667745113372803,0.6622556447982788,0.6603192090988159,0.6714670658111572,0.6720370650291443,0.6633121967315674,0.6630804538726807,0.6708438992500305,0.6709474325180054,0.6749446392059326,0.6722941398620605,0.6717827320098877,0.6800651550292969,0.6676793694496155,0.6654870510101318,0.658071756362915,0.654055118560791,0.6588186025619507,0.6604210734367371,0.6595771312713623,0.6606809496879578,0.6706702709197998,0.6740835309028625,0.6701357960700989,0.6701306104660034,0.6727221608161926,0.6739596724510193,0.674014687538147,0.6670147180557251,0.6683834195137024,0.6714038848876953,0.6721394062042236,0.6697844862937927,0.6662142276763916,0.6725527048110962,0.668285608291626,0.6680847406387329,0.6709316968917847,0.6755163073539734,0.6707229018211365,0.6693370342254639,0.6685855984687805,0.6696996092796326,0.670492947101593,0.6675918102264404,0.6689587235450745,0.6706271171569824,0.673912525177002,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 2)                 10738     
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 10740     
=================================================================
Total params: 21,478
Trainable params: 21,478
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 64)                320       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 128)               8320      
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 16)                2064      
_________________________________________________________________
bottleneck (Dense)           (None, 2)                 34        
=================================================================
Total params: 10,738
Trainable params: 10,738
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 2)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 16)                48        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 128)               2176      
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 64)                8256      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 260       
=================================================================
Total params: 10,740
Trainable params: 10,740
Non-trainable params: 0
_________________________________________________________________
