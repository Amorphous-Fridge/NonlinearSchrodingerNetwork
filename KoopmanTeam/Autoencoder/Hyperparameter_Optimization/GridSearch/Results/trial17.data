2021-06-25
loss,9.26595687866211,3.619729995727539,2.552046775817871,3.917701005935669,3.381004571914673,2.514939308166504,3.112851142883301,2.7338247299194336,2.9903664588928223,2.3472933769226074,2.3210818767547607,2.956406354904175,2.796060562133789,1.970301628112793,2.1349637508392334,1.6868118047714233,1.3471722602844238,1.1042457818984985,0.7915334105491638,0.8901317715644836,0.8073024153709412,0.8213258385658264,2.4791154861450195,2.032646894454956,1.3759047985076904,1.1158156394958496,0.9760696291923523,0.8748109340667725,0.5992831587791443,0.7653829455375671,0.7572144865989685,0.7731227278709412,0.7719482183456421,0.7494605779647827,0.8776004314422607,1.432387113571167,1.1649470329284668,1.840074062347412,0.8418999314308167,0.6939191222190857,0.6792474389076233,0.9770837426185608,0.7762179374694824,0.8636120557785034,0.8536728620529175,0.9174745082855225,0.5812665820121765,0.5864953398704529,0.6899958252906799,0.6541987061500549,0.7052282094955444,0.5906269550323486,0.6322228312492371,0.633634626865387,0.5786840915679932,0.8242465257644653,1.1546082496643066,2.3469655513763428,1.278852105140686,0.6669788360595703,0.6430463194847107,0.7003310322761536,0.6182924509048462,0.6102778911590576,0.5651956796646118,0.5638008117675781,0.5552290678024292,0.5707447528839111,0.6389756202697754,0.8615263104438782,0.5175885558128357,0.5631476044654846,0.5850515961647034,0.7633447051048279,1.4779380559921265,0.9008184671401978,0.7270188331604004,0.7193251848220825,0.8191089034080505,0.5494279265403748,0.6859394907951355,0.5723369717597961,0.7519800662994385,0.6663165092468262,0.773917555809021,0.8579126596450806,0.8269525766372681,0.8469164967536926,0.7079540491104126,0.794848620891571,0.958415687084198,0.8700035810470581,0.7934128046035767,0.834952175617218,0.791156530380249,0.8986185193061829,0.7955600023269653,0.5470196008682251,0.4613713324069977,1.0167171955108643,
mse,0.6354595422744751,0.7033376097679138,0.6950252056121826,0.7031222581863403,0.7131822109222412,0.7356184124946594,0.7232894897460938,0.7242088913917542,0.7230703830718994,0.7230436205863953,0.7178217172622681,0.7328627705574036,0.7244679927825928,0.7535938024520874,0.7481127977371216,0.7384328246116638,0.7521781921386719,0.7444195747375488,0.7405133247375488,0.7387059926986694,0.7398602962493896,0.7345125675201416,0.7304272651672363,0.7429637908935547,0.7418675422668457,0.7391449213027954,0.738997220993042,0.7400051951408386,0.7405510544776917,0.7343462109565735,0.7282555103302002,0.7313233017921448,0.7340942621231079,0.7282966375350952,0.7271472811698914,0.7299246191978455,0.7266353368759155,0.7191066741943359,0.719748854637146,0.7242218852043152,0.7267957925796509,0.7347922325134277,0.739967942237854,0.7361481189727783,0.7396246194839478,0.7369526028633118,0.7352510690689087,0.7269468903541565,0.7289842963218689,0.7323933243751526,0.7294085025787354,0.7226216197013855,0.7283404469490051,0.7224063277244568,0.7192208170890808,0.7202292084693909,0.7230501770973206,0.7298625707626343,0.7286553382873535,0.7351883053779602,0.7349197268486023,0.7309539318084717,0.7296669483184814,0.7305131554603577,0.7266148328781128,0.7247058153152466,0.7296205163002014,0.7298861145973206,0.7237602472305298,0.7257112860679626,0.7233595252037048,0.7250211834907532,0.7228030562400818,0.7178900837898254,0.7277885675430298,0.7366145849227905,0.7389706373214722,0.7321807146072388,0.7129764556884766,0.7012737393379211,0.7025938630104065,0.7031635046005249,0.7128996849060059,0.7203571796417236,0.7247849106788635,0.7299869060516357,0.7213695645332336,0.7135401368141174,0.7145337462425232,0.7032470703125,0.7303682565689087,0.7163862586021423,0.7249617576599121,0.722419261932373,0.7173696160316467,0.7146552801132202,0.7166628241539001,0.7129478454589844,0.7132021188735962,0.7062315940856934,
mae,0.6605273485183716,0.6432220935821533,0.6407433152198792,0.6350522041320801,0.6470503211021423,0.6578837037086487,0.6507060527801514,0.6571242809295654,0.663602352142334,0.6552571654319763,0.6558130383491516,0.6603763103485107,0.6610422730445862,0.6732031106948853,0.6798281669616699,0.6669965982437134,0.682848334312439,0.6805950999259949,0.6727836728096008,0.6718131899833679,0.674959659576416,0.6701611876487732,0.6688366532325745,0.6767561435699463,0.6813997030258179,0.6787819862365723,0.679526150226593,0.6816816926002502,0.6846360564231873,0.6784349679946899,0.6725480556488037,0.6752787828445435,0.6801449060440063,0.6744746565818787,0.673356294631958,0.7003770470619202,0.7003772258758545,0.6835796236991882,0.6742802262306213,0.6738853454589844,0.6770030856132507,0.6825911998748779,0.6865179538726807,0.6844190359115601,0.6838545799255371,0.6859965324401855,0.6855643391609192,0.6791634559631348,0.6775111556053162,0.68239426612854,0.6754177212715149,0.6722338795661926,0.6766314506530762,0.6747890710830688,0.6710186004638672,0.6756519079208374,0.6798417568206787,0.6886458396911621,0.6813427209854126,0.6877918839454651,0.6839975714683533,0.6801807284355164,0.6823564171791077,0.6820992231369019,0.674274742603302,0.6758214831352234,0.6789479851722717,0.6798751354217529,0.6778321862220764,0.677338182926178,0.6747807860374451,0.6755759716033936,0.678635835647583,0.6741718053817749,0.6849436163902283,0.6915233731269836,0.6910669803619385,0.6854923963546753,0.6786089539527893,0.6766860485076904,0.6677156686782837,0.6624376177787781,0.6720591187477112,0.6778205633163452,0.6811318397521973,0.687178373336792,0.6829318404197693,0.6751146912574768,0.6738409399986267,0.6726673245429993,0.6880152821540833,0.674193799495697,0.679919958114624,0.6790937185287476,0.6765149831771851,0.6755569577217102,0.6778115034103394,0.6746641397476196,0.6723191738128662,0.6660723686218262,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 2)                 12834     
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 12836     
=================================================================
Total params: 25,670
Trainable params: 25,670
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 64)                320       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 128)               8320      
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 32)                4128      
_________________________________________________________________
bottleneck (Dense)           (None, 2)                 66        
=================================================================
Total params: 12,834
Trainable params: 12,834
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 2)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 32)                96        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 128)               4224      
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 64)                8256      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 260       
=================================================================
Total params: 12,836
Trainable params: 12,836
Non-trainable params: 0
_________________________________________________________________
