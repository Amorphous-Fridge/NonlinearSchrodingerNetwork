2021-06-25
loss,5.515982627868652,2.2005198001861572,2.212423324584961,3.0744435787200928,2.306563377380371,2.5220768451690674,2.754373550415039,2.690000057220459,2.9692111015319824,2.5147032737731934,2.9836745262145996,2.141556739807129,2.1596057415008545,1.7281500101089478,2.0673177242279053,1.8385967016220093,2.0946853160858154,2.0351145267486572,1.8212507963180542,2.120283603668213,1.9556492567062378,1.4594475030899048,1.395918846130371,1.5013072490692139,2.0563712120056152,2.278292655944824,1.4470493793487549,1.3324977159500122,2.155332088470459,1.5099624395370483,1.237762689590454,1.4453740119934082,1.2293378114700317,1.24702787399292,1.2535264492034912,0.8054439425468445,0.5448979735374451,0.564685046672821,0.7586250901222229,0.8230554461479187,0.7868009209632874,0.7098538279533386,0.5103033185005188,0.49750617146492004,0.5889240503311157,0.536227285861969,0.5007335543632507,0.5408872961997986,1.082018256187439,0.6476303935050964,0.8414397239685059,1.1257777214050293,0.5954223871231079,0.5132534503936768,0.6359403133392334,0.6710777878761292,0.4586898684501648,0.438544362783432,0.6446502804756165,0.6408393979072571,0.6834446787834167,0.5299788117408752,0.6002486348152161,0.6508289575576782,0.7183785438537598,0.6272077560424805,0.5564213991165161,0.6968874931335449,0.5240720510482788,0.5828851461410522,0.6719552874565125,0.9623873829841614,1.4913502931594849,0.9965577125549316,0.5782513618469238,0.5744821429252625,0.6228621006011963,0.4813355505466461,0.5317703485488892,0.5616336464881897,0.8359337449073792,0.7484767436981201,0.5016322731971741,0.5556016564369202,0.6322464942932129,0.7097563743591309,0.5546110272407532,0.4862358570098877,0.7025585174560547,1.0716984272003174,0.8351311683654785,0.6412661671638489,0.5278006196022034,0.49446338415145874,0.45210325717926025,0.4264131784439087,0.5893679857254028,0.5461328029632568,0.38786983489990234,0.3974536061286926,
mse,0.49509280920028687,0.45841914415359497,0.4774302542209625,0.4874333143234253,0.5029792189598083,0.5060908198356628,0.5184540748596191,0.5546123385429382,0.5449185967445374,0.5675933361053467,0.5657758712768555,0.5734097361564636,0.5783874988555908,0.569186270236969,0.5587353706359863,0.562804102897644,0.5855407118797302,0.5866739153862,0.5874887108802795,0.5990996956825256,0.5622144937515259,0.5878386497497559,0.5617035031318665,0.5735988020896912,0.57148677110672,0.593661367893219,0.6426273584365845,0.597919225692749,0.5568601489067078,0.5114028453826904,0.5199038982391357,0.5236384868621826,0.5309593081474304,0.521196186542511,0.5275099873542786,0.5265645980834961,0.515622079372406,0.5201376080513,0.49860095977783203,0.5047213435173035,0.49991461634635925,0.5115322470664978,0.5158873200416565,0.5174519419670105,0.5133144855499268,0.5211486220359802,0.5245778560638428,0.5175585150718689,0.49504634737968445,0.4841400384902954,0.4875059127807617,0.513156533241272,0.5331253409385681,0.5386151075363159,0.5380410552024841,0.5469592809677124,0.5443073511123657,0.5443475246429443,0.5433956384658813,0.5406731367111206,0.5405693054199219,0.5471823811531067,0.5465439558029175,0.5448494553565979,0.5516170263290405,0.5484745502471924,0.5525364875793457,0.5511876344680786,0.5582980513572693,0.5582898855209351,0.5584899187088013,0.5610814094543457,0.5629298686981201,0.5769525766372681,0.5826575756072998,0.5758250951766968,0.5721672177314758,0.5650221109390259,0.5634517669677734,0.5636324286460876,0.566480815410614,0.5551051497459412,0.5577726364135742,0.5599749684333801,0.5648242831230164,0.5665637850761414,0.5489549040794373,0.5489121079444885,0.5533307790756226,0.5666508674621582,0.5558590888977051,0.5468752980232239,0.5481846332550049,0.5473395586013794,0.553173840045929,0.5564641356468201,0.558472752571106,0.5535812973976135,0.5470487475395203,0.5438148379325867,
mae,0.5358741283416748,0.5067670345306396,0.5236071944236755,0.5321086645126343,0.5436127185821533,0.5448841452598572,0.5469316244125366,0.5709478855133057,0.5579884052276611,0.5743447542190552,0.570278525352478,0.5786510705947876,0.5766672492027283,0.5633946061134338,0.5553693175315857,0.5566752552986145,0.5728421211242676,0.5718109607696533,0.56461501121521,0.5734025835990906,0.5482099652290344,0.5732511878013611,0.5409427881240845,0.5539413094520569,0.5497034192085266,0.571602463722229,0.6128270030021667,0.576255202293396,0.572115421295166,0.513164758682251,0.5125858783721924,0.5124530792236328,0.5201961398124695,0.5107105374336243,0.5157615542411804,0.514980673789978,0.5050578117370605,0.5079030394554138,0.4889245331287384,0.4945038855075836,0.49091747403144836,0.49927300214767456,0.5028044581413269,0.5059707760810852,0.5018424391746521,0.5081324577331543,0.5113275647163391,0.5054917335510254,0.5004589557647705,0.48844918608665466,0.4858666658401489,0.5061244964599609,0.5200686454772949,0.523412823677063,0.5219502449035645,0.5328178405761719,0.5283967852592468,0.5294620394706726,0.528444230556488,0.5257202386856079,0.5248644351959229,0.5310905575752258,0.5319122076034546,0.5296919345855713,0.536091685295105,0.5322377681732178,0.5357130765914917,0.5361732244491577,0.5430227518081665,0.5412520170211792,0.5428721904754639,0.5452896952629089,0.5457620024681091,0.5558558106422424,0.5618703961372375,0.5563454627990723,0.5527032017707825,0.5468555688858032,0.5461480021476746,0.5473977327346802,0.5494288206100464,0.539013147354126,0.5416223406791687,0.5434986352920532,0.5475206971168518,0.5503418445587158,0.5324378609657288,0.5328534245491028,0.5369969606399536,0.5539213418960571,0.5441725850105286,0.5334839820861816,0.5324063301086426,0.53116774559021,0.5376707315444946,0.5392618179321289,0.5418200492858887,0.5376253128051758,0.532709538936615,0.5289832949638367,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 2)                 3314      
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 3316      
=================================================================
Total params: 6,630
Trainable params: 6,630
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 16)                80        
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 64)                1088      
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 32)                2080      
_________________________________________________________________
bottleneck (Dense)           (None, 2)                 66        
=================================================================
Total params: 3,314
Trainable params: 3,314
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 2)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 32)                96        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 64)                2112      
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 16)                1040      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 68        
=================================================================
Total params: 3,316
Trainable params: 3,316
Non-trainable params: 0
_________________________________________________________________
