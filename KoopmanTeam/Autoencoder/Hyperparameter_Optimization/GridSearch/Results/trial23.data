2021-06-25
loss,7.910027503967285,3.4320147037506104,3.231720209121704,2.283498764038086,1.2550987005233765,1.691973090171814,1.2705676555633545,0.9384300112724304,1.4124974012374878,1.0299441814422607,0.9821351766586304,0.7394224405288696,1.128473162651062,0.6816595196723938,1.091953158378601,1.7028117179870605,1.7980962991714478,1.0766308307647705,1.010901927947998,0.911668598651886,1.1842867136001587,0.8062937259674072,0.7881041765213013,0.7320387959480286,0.8192446827888489,1.196657657623291,1.3127285242080688,0.7365951538085938,0.7470216155052185,0.8516322374343872,0.9253820776939392,0.9206260442733765,0.7297436594963074,1.556854248046875,1.215970754623413,0.9593731164932251,1.1387361288070679,1.3850090503692627,0.8007493615150452,0.6842411160469055,0.7102129459381104,0.7437576055526733,0.6470744609832764,0.7078853845596313,0.6684331297874451,0.8165425658226013,0.5022327899932861,0.9054506421089172,0.9302741885185242,1.0107461214065552,0.9075677394866943,0.8074548840522766,0.82010817527771,1.0059491395950317,0.5515981316566467,1.0083214044570923,0.8777439594268799,0.8109047412872314,1.027714490890503,0.8672736287117004,0.9424501657485962,0.6827936768531799,0.8287705779075623,0.8174655437469482,0.8604385852813721,0.8753365874290466,0.7410712242126465,0.9975534677505493,0.6959190964698792,0.7087072134017944,0.7354250550270081,0.9042802453041077,0.7912648916244507,0.8874562978744507,0.7169118523597717,0.5722731947898865,0.8321648240089417,0.5873486995697021,1.578513741493225,1.0496175289154053,1.1270161867141724,0.9694167375564575,0.5885972380638123,0.5491154789924622,0.8513051867485046,1.1520386934280396,0.8740413784980774,0.8214929699897766,0.6257123351097107,0.5096607208251953,0.7935000061988831,1.2695708274841309,0.9293303489685059,0.5732974410057068,0.5875466465950012,0.5438104867935181,1.223750352859497,0.7142502069473267,0.6829010844230652,0.7843903303146362,
mse,0.7073997259140015,0.6958267688751221,0.6830955743789673,0.6995060443878174,0.7157208323478699,0.7122898101806641,0.728243350982666,0.7214618921279907,0.7235212922096252,0.7280471920967102,0.7216304540634155,0.7202328443527222,0.7163338661193848,0.7177380323410034,0.7052144408226013,0.7197687029838562,0.7217996716499329,0.7034763097763062,0.6874094605445862,0.7030593752861023,0.7157052755355835,0.7139813899993896,0.7138070464134216,0.7138574123382568,0.7097195982933044,0.7065279483795166,0.7150585055351257,0.7218391299247742,0.7209003567695618,0.7135492563247681,0.7127824425697327,0.6962563991546631,0.7020024657249451,0.7073068022727966,0.7016953229904175,0.7003280520439148,0.7033064365386963,0.7168118953704834,0.7236288189888,0.7146456837654114,0.7097132802009583,0.714836061000824,0.7147996425628662,0.7171093225479126,0.7050012946128845,0.6984225511550903,0.7002610564231873,0.6962035298347473,0.6991499066352844,0.6816649436950684,0.6870852112770081,0.6881199479103088,0.6873877048492432,0.6912291049957275,0.6872327923774719,0.6794123649597168,0.683495283126831,0.6829695105552673,0.7007735371589661,0.7016781568527222,0.6999424695968628,0.6983399391174316,0.7013400793075562,0.6958625316619873,0.695328950881958,0.6884533166885376,0.6870471239089966,0.6787765622138977,0.6841357946395874,0.6873299479484558,0.6883973479270935,0.6931797862052917,0.681525707244873,0.671954870223999,0.6698516607284546,0.6767483353614807,0.680042564868927,0.6776747703552246,0.6768891215324402,0.6738340854644775,0.6805427670478821,0.6798357367515564,0.6736159324645996,0.6726866364479065,0.6673887372016907,0.6657087206840515,0.6662266254425049,0.6740981936454773,0.6738314628601074,0.666395366191864,0.6646313667297363,0.6746340990066528,0.6787653565406799,0.6695518493652344,0.6753477454185486,0.6774092316627502,0.6755688190460205,0.6725943684577942,0.6744580864906311,0.6724569201469421,
mae,0.674307107925415,0.6543908715248108,0.642204761505127,0.6565305590629578,0.6742475628852844,0.6716217994689941,0.680101752281189,0.6796115040779114,0.6800749897956848,0.6825553178787231,0.6793354749679565,0.6785978078842163,0.6757586598396301,0.6794385313987732,0.6656820178031921,0.6797374486923218,0.6778665781021118,0.6686499714851379,0.6523343920707703,0.6659346222877502,0.6799758076667786,0.676340639591217,0.6742900609970093,0.6727192401885986,0.6744391918182373,0.6688488125801086,0.6788491606712341,0.6795403957366943,0.6799418926239014,0.6733716726303101,0.6780739426612854,0.6595668792724609,0.6613680124282837,0.671205997467041,0.6766270399093628,0.6709833741188049,0.6677281856536865,0.6776645183563232,0.6828754544258118,0.6782171726226807,0.6727858781814575,0.6742527484893799,0.6720454692840576,0.6727392673492432,0.6685593128204346,0.6700108051300049,0.6627005934715271,0.6559292078018188,0.662167489528656,0.6647558808326721,0.6585087776184082,0.6581588983535767,0.6534457206726074,0.6597630381584167,0.6530402302742004,0.6497474908828735,0.6625159978866577,0.6589827537536621,0.6657127141952515,0.6666856408119202,0.6646926999092102,0.6661425828933716,0.6700679063796997,0.6646609306335449,0.6660720705986023,0.657032310962677,0.6527007222175598,0.652087390422821,0.6537958383560181,0.6593932509422302,0.6562243103981018,0.660615861415863,0.6506406664848328,0.6428069472312927,0.6434081792831421,0.64784836769104,0.6455045938491821,0.6492854356765747,0.6462419033050537,0.644904613494873,0.6585909128189087,0.6601808667182922,0.6510932445526123,0.6504630446434021,0.6407850384712219,0.6401501297950745,0.6468002200126648,0.648245632648468,0.6498513221740723,0.6489877104759216,0.6398865580558777,0.6443526744842529,0.6511057019233704,0.6451606750488281,0.6462505459785461,0.6477819681167603,0.6480368971824646,0.6485399603843689,0.6483445763587952,0.6449744701385498,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 2)                 12722     
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 12724     
=================================================================
Total params: 25,446
Trainable params: 25,446
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 16)                80        
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 256)               4352      
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 32)                8224      
_________________________________________________________________
bottleneck (Dense)           (None, 2)                 66        
=================================================================
Total params: 12,722
Trainable params: 12,722
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 2)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 32)                96        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 256)               8448      
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 16)                4112      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 68        
=================================================================
Total params: 12,724
Trainable params: 12,724
Non-trainable params: 0
_________________________________________________________________
