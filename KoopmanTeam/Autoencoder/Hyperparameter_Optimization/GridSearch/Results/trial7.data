2021-06-25
loss,5.794615268707275,4.223146915435791,2.6330981254577637,2.1116013526916504,2.3055410385131836,1.8625197410583496,1.6654386520385742,0.9913027286529541,1.106951117515564,0.9602375030517578,1.221859335899353,1.4830365180969238,0.8293405771255493,1.4946351051330566,1.0590494871139526,1.0407829284667969,0.8857694864273071,1.4571952819824219,0.997040331363678,2.1216204166412354,2.7852587699890137,0.7270916104316711,0.8405554890632629,0.788684606552124,1.0281517505645752,1.3192521333694458,1.1942864656448364,0.7256433963775635,0.7614858150482178,0.950310230255127,1.119676947593689,0.7376837730407715,0.9068794846534729,1.0457340478897095,0.7020057439804077,0.7496962547302246,0.7312294244766235,1.7780574560165405,1.1582419872283936,0.8021557331085205,0.6816455721855164,0.6889280676841736,0.6713474988937378,1.217724084854126,1.2726242542266846,1.1439152956008911,0.7831902503967285,0.722201406955719,0.876573920249939,0.8514983654022217,1.3347363471984863,1.362748384475708,1.227299451828003,0.9696263074874878,0.9084135293960571,0.8106217980384827,1.133571982383728,0.8054350018501282,0.9448584914207458,0.7323355078697205,0.989542543888092,1.0300204753875732,0.7396097779273987,0.8787038326263428,0.7048050761222839,0.7401223182678223,0.7823440432548523,0.9716475009918213,0.8015226125717163,0.6275043487548828,0.8307119607925415,0.8375718593597412,0.8670102953910828,0.718788206577301,0.8528704643249512,0.6916806101799011,0.9434058666229248,1.0027852058410645,1.4040066003799438,0.7728707194328308,0.7816795110702515,0.8856818675994873,0.7569999098777771,0.6871386766433716,0.586759090423584,0.7884745001792908,0.7228519320487976,0.6186792254447937,0.7168705463409424,0.6605921983718872,0.6428229808807373,0.5881859064102173,0.7266706824302673,0.6075754165649414,0.5261589288711548,0.5716258883476257,0.9287739396095276,0.6120140552520752,0.9564615488052368,0.8953703045845032,
mse,0.6792836785316467,0.6998879313468933,0.7334665060043335,0.7674835324287415,0.7962854504585266,0.8057487607002258,0.8181823492050171,0.8242202997207642,0.8252066969871521,0.8283954858779907,0.8219658136367798,0.8307663202285767,0.8354099988937378,0.8367105722427368,0.8297491669654846,0.83203125,0.8362459540367126,0.8369404077529907,0.8308843970298767,0.8262408375740051,0.8252699971199036,0.8432526588439941,0.8415570259094238,0.8427654504776001,0.8428443074226379,0.845719575881958,0.8483307361602783,0.850420355796814,0.8516414165496826,0.8479592204093933,0.8464620113372803,0.8545837998390198,0.8544562458992004,0.8533418774604797,0.8528568744659424,0.8555492162704468,0.8533854484558105,0.8500288128852844,0.8583539724349976,0.8559257984161377,0.8532223701477051,0.8573734760284424,0.8600842356681824,0.8537712097167969,0.8554370403289795,0.8662769198417664,0.8594249486923218,0.8570769429206848,0.8552005887031555,0.8536023497581482,0.8370031714439392,0.8287549614906311,0.8381146788597107,0.8536248803138733,0.8593290448188782,0.8631477952003479,0.8643289804458618,0.8620122671127319,0.8668272495269775,0.8687567114830017,0.8666499257087708,0.8664535284042358,0.8687556385993958,0.8698559403419495,0.8697053790092468,0.8694438338279724,0.8679250478744507,0.8679599165916443,0.8675458431243896,0.8701494336128235,0.8662229776382446,0.8670008182525635,0.8654651045799255,0.8700319528579712,0.8723292350769043,0.8678155541419983,0.8716539740562439,0.8689743876457214,0.8653883934020996,0.8639621734619141,0.8659946918487549,0.8678465485572815,0.8704792857170105,0.8678948283195496,0.8668739199638367,0.8682292103767395,0.8713040351867676,0.8736945986747742,0.8737261295318604,0.8722679018974304,0.8691315650939941,0.8679431676864624,0.8675124645233154,0.8698655962944031,0.863657534122467,0.8674760460853577,0.8703124523162842,0.8671000599861145,0.8624517917633057,0.8688796162605286,
mae,0.7087873816490173,0.7055870890617371,0.7110235691070557,0.7276046872138977,0.751874566078186,0.7557884454727173,0.7564927935600281,0.7676738500595093,0.7645049095153809,0.765383243560791,0.7598402500152588,0.7617607116699219,0.768513023853302,0.7668442726135254,0.7642092704772949,0.7690541744232178,0.7676449418067932,0.7645114064216614,0.764565110206604,0.7708448171615601,0.7516605854034424,0.7716977596282959,0.7720332145690918,0.7707669734954834,0.7742404937744141,0.7709723711013794,0.7681854367256165,0.7768481969833374,0.7737169861793518,0.7733214497566223,0.7718843817710876,0.7813587188720703,0.7729465365409851,0.7732021808624268,0.7753110527992249,0.779915988445282,0.7751941084861755,0.7684661746025085,0.7765834331512451,0.7776851058006287,0.7733604907989502,0.7731884121894836,0.7777374386787415,0.7755287289619446,0.7734105587005615,0.7779523730278015,0.7785254716873169,0.774567186832428,0.7711865305900574,0.7723010182380676,0.7828497886657715,0.7775784134864807,0.7834241986274719,0.7868777513504028,0.7898411750793457,0.7796983122825623,0.7804753184318542,0.7789353132247925,0.7851028442382812,0.7821559309959412,0.7824039459228516,0.7808323502540588,0.7848784923553467,0.7874181866645813,0.7866185903549194,0.7815821766853333,0.787316083908081,0.7807934284210205,0.780979335308075,0.7850342392921448,0.7824077606201172,0.7806358337402344,0.7836976051330566,0.7844017148017883,0.788606584072113,0.7788636088371277,0.7857884764671326,0.7805681824684143,0.7793789505958557,0.7882305979728699,0.7854382395744324,0.7875407934188843,0.7840423583984375,0.7858028411865234,0.7813270688056946,0.7822078466415405,0.7812626361846924,0.7846567630767822,0.7886747121810913,0.7861682772636414,0.7812491655349731,0.778110146522522,0.7813959717750549,0.7801811695098877,0.7843996286392212,0.7849046587944031,0.7776303291320801,0.7805431485176086,0.7749712467193604,0.7891104817390442,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 2)                 10642     
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 10644     
=================================================================
Total params: 21,286
Trainable params: 21,286
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 16)                80        
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 128)               2176      
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 64)                8256      
_________________________________________________________________
bottleneck (Dense)           (None, 2)                 130       
=================================================================
Total params: 10,642
Trainable params: 10,642
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 2)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 64)                192       
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 128)               8320      
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 16)                2064      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 68        
=================================================================
Total params: 10,644
Trainable params: 10,644
Non-trainable params: 0
_________________________________________________________________
