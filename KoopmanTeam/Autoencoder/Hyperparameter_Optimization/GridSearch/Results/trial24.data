2021-06-25
loss,14.093952178955078,7.185683727264404,7.280913829803467,7.718436241149902,6.0099358558654785,7.0655741691589355,4.764524936676025,3.440685987472534,3.622297763824463,1.6957019567489624,2.5857536792755127,1.6167542934417725,2.8427278995513916,3.0896451473236084,2.3759655952453613,1.6608977317810059,1.4302576780319214,2.523632764816284,1.9289699792861938,2.437685012817383,1.1706938743591309,1.2289024591445923,2.219935655593872,1.5868741273880005,1.1198982000350952,1.0390583276748657,1.0875200033187866,1.0389541387557983,1.2620357275009155,1.3078320026397705,1.9989601373672485,2.018080711364746,1.971001148223877,1.4856328964233398,1.7319848537445068,1.0420372486114502,0.8672089576721191,1.5151747465133667,1.1937512159347534,0.9271125197410583,0.8570534586906433,1.678897500038147,2.474581241607666,1.4587812423706055,1.0094399452209473,1.4368023872375488,1.0860079526901245,0.9137069582939148,0.854708731174469,1.2943729162216187,1.2932767868041992,1.4999642372131348,1.0981889963150024,1.0290077924728394,0.8259007930755615,0.7900882959365845,0.9548214673995972,0.7260206341743469,1.1285234689712524,0.8570384979248047,1.080446720123291,1.2615339756011963,1.2581181526184082,1.0253841876983643,0.6026313900947571,0.6131036877632141,0.737602710723877,1.340758204460144,1.1662081480026245,0.8929145932197571,0.8226672410964966,0.768452525138855,0.9051477909088135,1.4154696464538574,0.9054176211357117,0.7753892540931702,1.4983669519424438,1.109075665473938,1.0409163236618042,1.0190491676330566,0.8884158134460449,1.137251377105713,0.7144606113433838,0.9493305683135986,1.2964364290237427,1.274680733680725,1.5549565553665161,1.2414474487304688,0.7118500471115112,0.9426762461662292,0.8347044587135315,0.7444697618484497,0.6176708340644836,0.9500290155410767,1.0553210973739624,0.7275288105010986,0.7196645140647888,0.9449765682220459,0.7961136102676392,0.7377723455429077,
mse,0.5931058526039124,0.6180117130279541,0.6476280093193054,0.5260936617851257,0.13659030199050903,0.12017446756362915,0.15910080075263977,0.14349323511123657,0.1275797337293625,0.08688590675592422,0.11608576029539108,0.08123088628053665,0.07673611491918564,0.0747845470905304,0.06134019047021866,0.052471574395895004,0.052269238978624344,0.047752369195222855,0.04331280291080475,0.04721028730273247,0.0420653373003006,0.03692413866519928,0.0420968234539032,0.03898891806602478,0.036727696657180786,0.037887848913669586,0.037449534982442856,0.03535681590437889,0.03778550401329994,0.037331048399209976,0.04416745528578758,0.04364519193768501,0.04472249001264572,0.03918901085853577,0.04572441428899765,0.040132228285074234,0.035335976630449295,0.03842037171125412,0.0372287854552269,0.03608984127640724,0.03709792718291283,0.04077979922294617,0.05153675004839897,0.04668255150318146,0.04063631594181061,0.042079806327819824,0.03958767279982567,0.04062947630882263,0.03881357982754707,0.03938397392630577,0.03935487940907478,0.04106380045413971,0.03959516063332558,0.03920629620552063,0.04143446311354637,0.03916720300912857,0.03811710700392723,0.03853387013077736,0.041261568665504456,0.04017085209488869,0.039256297051906586,0.04068866744637489,0.04333367198705673,0.04142822325229645,0.038571905344724655,0.03714373707771301,0.03802832216024399,0.04366927593946457,0.043335892260074615,0.04101790115237236,0.04222414642572403,0.04076053574681282,0.041621044278144836,0.04313584417104721,0.043381284922361374,0.04453291371464729,0.04487960785627365,0.04613190144300461,0.04052535071969032,0.041888169944286346,0.043655358254909515,0.044744428247213364,0.041623663157224655,0.0431734099984169,0.04287391155958176,0.05321073532104492,0.051446184515953064,0.0503784716129303,0.043801456689834595,0.0443747416138649,0.04430174455046654,0.045185718685388565,0.04268974810838699,0.043232012540102005,0.04444791376590729,0.04200147092342377,0.04304160922765732,0.042378686368465424,0.04402803257107735,0.047612324357032776,
mae,0.6225219368934631,0.6289530396461487,0.6414636373519897,0.5613036155700684,0.2946511209011078,0.26930516958236694,0.28968551754951477,0.28554767370224,0.2703995108604431,0.22296258807182312,0.24832893908023834,0.20932067930698395,0.21354983747005463,0.2113872766494751,0.19078963994979858,0.17737267911434174,0.1811407506465912,0.1737716645002365,0.16585350036621094,0.17101244628429413,0.16142597794532776,0.1506139636039734,0.16083449125289917,0.15504096448421478,0.1493401974439621,0.15399470925331116,0.15092530846595764,0.14676694571971893,0.15272031724452972,0.15122047066688538,0.16569827497005463,0.1629212498664856,0.16637782752513885,0.15714560449123383,0.17140504717826843,0.15950682759284973,0.14822664856910706,0.1556040197610855,0.1513352394104004,0.14848026633262634,0.15092487633228302,0.15955162048339844,0.18062545359134674,0.17025524377822876,0.15921027958393097,0.16315416991710663,0.15643444657325745,0.1573641449213028,0.1522860825061798,0.15453757345676422,0.15570658445358276,0.1579943746328354,0.15450119972229004,0.15158522129058838,0.15489767491817474,0.15305520594120026,0.1490197628736496,0.14711180329322815,0.15250593423843384,0.15186090767383575,0.14630885422229767,0.15136173367500305,0.15574708580970764,0.15537910163402557,0.14680996537208557,0.14151717722415924,0.14446400105953217,0.15821556746959686,0.15525437891483307,0.14867709577083588,0.15250390768051147,0.1485518515110016,0.15251143276691437,0.15544342994689941,0.15429911017417908,0.15615911781787872,0.16047047078609467,0.16702835261821747,0.15112075209617615,0.14997291564941406,0.1547657996416092,0.16124729812145233,0.1497509479522705,0.15719746053218842,0.15409059822559357,0.18483518064022064,0.18184861540794373,0.1744667887687683,0.1575465053319931,0.15697702765464783,0.15651437640190125,0.16161324083805084,0.15326546132564545,0.1540239453315735,0.15683338046073914,0.1500212550163269,0.1526366025209427,0.15098384022712708,0.15657192468643188,0.16731098294258118,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 2)                 22994     
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 22996     
=================================================================
Total params: 45,990
Trainable params: 45,990
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 32)                160       
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 128)               4224      
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 128)               16512     
_________________________________________________________________
encoding_layer_4 (Dense)     (None, 16)                2064      
_________________________________________________________________
bottleneck (Dense)           (None, 2)                 34        
=================================================================
Total params: 22,994
Trainable params: 22,994
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 2)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 16)                48        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 128)               2176      
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 128)               16512     
_________________________________________________________________
decoding_layer_4 (Dense)     (None, 32)                4128      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 132       
=================================================================
Total params: 22,996
Trainable params: 22,996
Non-trainable params: 0
_________________________________________________________________
