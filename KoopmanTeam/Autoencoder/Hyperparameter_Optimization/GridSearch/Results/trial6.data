2021-06-25
loss,7.001556873321533,3.2493374347686768,3.0471506118774414,2.909400701522827,2.2173001766204834,2.0308964252471924,2.331536054611206,2.0285627841949463,1.6528937816619873,1.6581521034240723,1.7290457487106323,1.0621733665466309,0.964393675327301,0.8501772284507751,1.1448416709899902,1.0220167636871338,1.1748816967010498,1.2744405269622803,1.2231144905090332,1.7051066160202026,1.0518807172775269,0.7223951816558838,0.6577255129814148,0.6906917691230774,1.007214069366455,0.9056353569030762,1.0368132591247559,0.6539737582206726,0.7624930739402771,0.7037512063980103,0.8883291482925415,0.9600895643234253,1.2500865459442139,0.9364566802978516,1.1306904554367065,0.6612800359725952,0.8986074924468994,0.9198994636535645,0.852317214012146,0.8622344732284546,0.7344725131988525,0.7406564354896545,0.719633936882019,0.6762994527816772,0.7499402761459351,1.2175531387329102,0.7824660539627075,0.769502580165863,0.6619512438774109,1.0357840061187744,0.7323549389839172,0.6787520051002502,0.5857419371604919,0.6233951449394226,0.8972442150115967,0.7459811568260193,0.7022029757499695,0.5801069736480713,0.5403763055801392,0.6856446862220764,1.1216987371444702,0.526179850101471,0.7726113796234131,0.8047804832458496,1.731207251548767,0.8407058119773865,0.5955090522766113,0.587863564491272,0.5468387603759766,0.8339558243751526,0.6887648105621338,0.814240574836731,0.7212393283843994,0.8953620195388794,0.7632192969322205,0.5387611985206604,0.6907367706298828,0.6522284150123596,0.9447335600852966,0.82889723777771,0.774126410484314,0.7639296650886536,0.5510314702987671,0.7567138075828552,0.481597900390625,0.5347170233726501,0.517894983291626,1.1498286724090576,0.7855282425880432,0.558098554611206,0.7223119139671326,0.5385845303535461,0.8528865575790405,0.6705516576766968,0.823088526725769,0.9331617951393127,1.2483735084533691,0.8330517411231995,0.5357380509376526,0.7002974152565002,
mse,0.5214956402778625,0.4407361149787903,0.461690217256546,0.5145242810249329,0.5524183511734009,0.5640209317207336,0.5714861154556274,0.5847930312156677,0.5852593183517456,0.6055983304977417,0.6105309128761292,0.6287186741828918,0.6518700122833252,0.6573083400726318,0.6665747761726379,0.6798070669174194,0.6765158176422119,0.6821410655975342,0.6861582398414612,0.6914708614349365,0.7069762945175171,0.714230477809906,0.7181466817855835,0.7162815928459167,0.7184800505638123,0.7214810252189636,0.7207396030426025,0.7138852477073669,0.7141849994659424,0.714245617389679,0.7154488563537598,0.7164906859397888,0.7240604162216187,0.7232805490493774,0.7253385186195374,0.7261930704116821,0.7302906513214111,0.728894829750061,0.7263100147247314,0.7203463912010193,0.7208123207092285,0.7095720171928406,0.6951362490653992,0.7049200534820557,0.7080929279327393,0.710010290145874,0.7122586965560913,0.7088850140571594,0.7166550159454346,0.7101582288742065,0.7045961618423462,0.6984449625015259,0.7023986577987671,0.7103355526924133,0.7148298025131226,0.7251061201095581,0.7270635366439819,0.7140412926673889,0.7088704705238342,0.710349977016449,0.7062599062919617,0.7034726738929749,0.713571310043335,0.7116747498512268,0.7165037393569946,0.7051167488098145,0.7000781297683716,0.7055026888847351,0.7011827826499939,0.7097950577735901,0.6977580189704895,0.7045192718505859,0.6968302726745605,0.6971099376678467,0.7028875946998596,0.7023999094963074,0.6927122473716736,0.6943790912628174,0.6854143738746643,0.6681902408599854,0.6549562215805054,0.653652012348175,0.6659652590751648,0.6727455854415894,0.6821247339248657,0.678820013999939,0.6777216196060181,0.6832142472267151,0.6834267377853394,0.6862428784370422,0.6812634468078613,0.688416063785553,0.6857560873031616,0.6853950023651123,0.6847987174987793,0.6853556632995605,0.683316171169281,0.6756272315979004,0.6790329813957214,0.6783384680747986,
mae,0.5828949213027954,0.5281636118888855,0.5499829649925232,0.5782071948051453,0.6032307744026184,0.6090276837348938,0.6133474707603455,0.6223915219306946,0.6240596771240234,0.6379371285438538,0.6427667140960693,0.6561514139175415,0.670396625995636,0.6724300384521484,0.6732931733131409,0.679701566696167,0.6763535141944885,0.6781575679779053,0.6749003529548645,0.6785624027252197,0.6802054047584534,0.6804810166358948,0.6780818104743958,0.6717584133148193,0.6853337287902832,0.6792455911636353,0.6826032400131226,0.6740551590919495,0.672281801700592,0.6721364855766296,0.6757648587226868,0.674195408821106,0.6837635040283203,0.6949179172515869,0.6901688575744629,0.6848172545433044,0.6896950602531433,0.6905616521835327,0.6836577653884888,0.6800996661186218,0.6822424530982971,0.6793707013130188,0.6770525574684143,0.6693904995918274,0.6692944169044495,0.6757911443710327,0.6749675869941711,0.6751434206962585,0.6778678894042969,0.682411253452301,0.6789073348045349,0.6631737351417542,0.669750452041626,0.6750734448432922,0.6783721446990967,0.6904528141021729,0.7016204595565796,0.68427574634552,0.6784103512763977,0.6775506734848022,0.6822640299797058,0.6763153672218323,0.6777130961418152,0.6780221462249756,0.6858010292053223,0.6781625151634216,0.6706708669662476,0.6728135943412781,0.6704093813896179,0.6805798411369324,0.6694651246070862,0.6702139973640442,0.6677089929580688,0.6650600433349609,0.6663594245910645,0.6704644560813904,0.6646595597267151,0.6649227738380432,0.6638262271881104,0.6607829928398132,0.6446055769920349,0.63252854347229,0.6412489414215088,0.6441064476966858,0.6526524424552917,0.6503806114196777,0.648173451423645,0.6490604281425476,0.6579590439796448,0.6625543236732483,0.6525472402572632,0.6578991413116455,0.6551113128662109,0.6519579887390137,0.6545335650444031,0.6549971103668213,0.6544652581214905,0.6496133208274841,0.6565505862236023,0.6535791158676147,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 2)                 6450      
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 6452      
=================================================================
Total params: 12,902
Trainable params: 12,902
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 16)                80        
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 128)               2176      
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 32)                4128      
_________________________________________________________________
bottleneck (Dense)           (None, 2)                 66        
=================================================================
Total params: 6,450
Trainable params: 6,450
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 2)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 32)                96        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 128)               4224      
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 16)                2064      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 68        
=================================================================
Total params: 6,452
Trainable params: 6,452
Non-trainable params: 0
_________________________________________________________________
