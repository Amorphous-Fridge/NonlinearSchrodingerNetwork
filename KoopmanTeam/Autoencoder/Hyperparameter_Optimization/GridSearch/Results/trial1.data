2021-06-25
loss,5.452024459838867,2.4277901649475098,2.0552456378936768,1.2978230714797974,1.1182483434677124,1.48258638381958,1.1623716354370117,1.7132675647735596,1.6720925569534302,1.9065097570419312,0.9993335008621216,1.5835837125778198,1.5541003942489624,1.5168213844299316,0.7582672238349915,0.8830349445343018,1.260715126991272,1.3263674974441528,0.8585600852966309,1.3228999376296997,1.2004235982894897,1.658210039138794,1.3375625610351562,1.0501799583435059,1.463545799255371,1.0168335437774658,0.929434597492218,1.213605284690857,0.6891607642173767,0.7046399116516113,1.0449423789978027,1.0682662725448608,0.8118438124656677,1.8430581092834473,0.9708912372589111,0.9058606028556824,1.027486801147461,0.9709625840187073,0.9349455833435059,1.0998455286026,1.5075187683105469,1.0786159038543701,0.7188226580619812,0.9222090840339661,0.9526922106742859,0.6024020910263062,1.0465713739395142,1.4403185844421387,0.8810209035873413,0.838066577911377,0.9765305519104004,0.9204098582267761,0.9169706106185913,1.0507826805114746,1.0569486618041992,0.694588840007782,0.9125483632087708,0.8204721212387085,0.8799684643745422,0.7695104479789734,0.7462917566299438,0.8498952984809875,0.7860916256904602,1.0114524364471436,0.7181079983711243,0.698951780796051,0.6796229481697083,0.6125826239585876,0.5702806115150452,0.8243139386177063,0.7211620211601257,0.5741530060768127,1.0018538236618042,0.8835408091545105,0.6379439830780029,0.5915796160697937,0.6752873063087463,0.7230837345123291,0.6414028406143188,0.5170989036560059,0.6421459317207336,0.6142686605453491,0.7342314124107361,1.019734501838684,0.548602819442749,0.501181423664093,0.5187402367591858,0.5765177011489868,0.5231117606163025,0.607174277305603,0.7852404117584229,0.9592748880386353,0.5741106867790222,0.6700287461280823,0.7536079287528992,1.0364618301391602,0.6627651453018188,0.8465273380279541,0.7056921124458313,0.653535783290863,
mse,0.4297758936882019,0.47955694794654846,0.5466412305831909,0.5687428116798401,0.5789671540260315,0.588657557964325,0.5935834050178528,0.5978913307189941,0.6100104451179504,0.5790011882781982,0.6001843810081482,0.5895618200302124,0.5838366150856018,0.6057331562042236,0.5984877347946167,0.5986876487731934,0.5907666683197021,0.6079472303390503,0.5972468852996826,0.6045888662338257,0.5987721085548401,0.6184369921684265,0.6166033148765564,0.6197814345359802,0.6152355670928955,0.6081656813621521,0.608845591545105,0.6143375039100647,0.598547637462616,0.6162964701652527,0.5922771692276001,0.6071980595588684,0.5985714197158813,0.5723484754562378,0.5612210631370544,0.5941370129585266,0.5990813970565796,0.6106550097465515,0.6103626489639282,0.6088318824768066,0.5873374342918396,0.6196597218513489,0.6149455308914185,0.6092181205749512,0.5832435488700867,0.606311559677124,0.5877178311347961,0.6135296821594238,0.6127578616142273,0.6203951835632324,0.6275532245635986,0.5924012660980225,0.6040909290313721,0.5890448093414307,0.5870717167854309,0.5960693955421448,0.5818129181861877,0.5748685002326965,0.5969800353050232,0.6021033525466919,0.6048346161842346,0.5879033207893372,0.5889375805854797,0.6049803495407104,0.6025701761245728,0.6069478392601013,0.575475811958313,0.5869967341423035,0.6019197702407837,0.5881386399269104,0.5978462100028992,0.5936086177825928,0.583599328994751,0.5980994701385498,0.5912181735038757,0.5930810570716858,0.5950547456741333,0.5836580395698547,0.5976376533508301,0.5886729955673218,0.5900664329528809,0.5838825702667236,0.5928864479064941,0.5867359638214111,0.5998678207397461,0.5980244278907776,0.5884281992912292,0.5854886770248413,0.5894108414649963,0.5839104652404785,0.5720155835151672,0.5589936971664429,0.5994186401367188,0.5912550091743469,0.5806794166564941,0.5630049705505371,0.5861414074897766,0.5930333137512207,0.6208682060241699,0.6076576113700867,
mae,0.5217964053153992,0.5049358606338501,0.5378410816192627,0.5541315674781799,0.5616552233695984,0.5686771273612976,0.5723230242729187,0.5752309560775757,0.5877096652984619,0.5628320574760437,0.5772266983985901,0.5695508718490601,0.563805103302002,0.5817939043045044,0.5787253379821777,0.5758906006813049,0.571324348449707,0.5874536633491516,0.5762934684753418,0.5816431641578674,0.578567385673523,0.5938676595687866,0.5927459001541138,0.5961974859237671,0.5946153402328491,0.5837371349334717,0.5870726108551025,0.5912609100341797,0.5762690305709839,0.5916157364845276,0.5724487900733948,0.5829711556434631,0.5741914510726929,0.5588281154632568,0.545968234539032,0.5716257691383362,0.5764391422271729,0.5862828493118286,0.5848202109336853,0.5844404101371765,0.565916895866394,0.5957404375076294,0.5937631726264954,0.586654543876648,0.5618674159049988,0.584181010723114,0.5691394209861755,0.5944535732269287,0.5875706672668457,0.5961283445358276,0.6042003631591797,0.574141263961792,0.5786917209625244,0.575796902179718,0.5688619613647461,0.5751544237136841,0.5632396936416626,0.5626207590103149,0.5746464729309082,0.580507218837738,0.5837706923484802,0.5688533186912537,0.5696613788604736,0.5820281505584717,0.5821759104728699,0.5847458839416504,0.5572589635848999,0.5661917924880981,0.5816587805747986,0.569184422492981,0.5760210156440735,0.5729414820671082,0.5660265684127808,0.5764211416244507,0.571505069732666,0.5748608112335205,0.5760796666145325,0.5638989806175232,0.577883780002594,0.5684126615524292,0.5718189477920532,0.5654286742210388,0.5744223594665527,0.5676729083061218,0.5787150263786316,0.5783154964447021,0.5701016783714294,0.566836953163147,0.5705190896987915,0.5641557574272156,0.5579558610916138,0.5441671013832092,0.5758001208305359,0.5711895227432251,0.5624805688858032,0.5484129786491394,0.5653663873672485,0.5706799030303955,0.5961906313896179,0.5860962867736816,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 2)                 2242      
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 2244      
=================================================================
Total params: 4,486
Trainable params: 4,486
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 16)                80        
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 64)                1088      
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 16)                1040      
_________________________________________________________________
bottleneck (Dense)           (None, 2)                 34        
=================================================================
Total params: 2,242
Trainable params: 2,242
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 2)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 16)                48        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 64)                1088      
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 16)                1040      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 68        
=================================================================
Total params: 2,244
Trainable params: 2,244
Non-trainable params: 0
_________________________________________________________________
