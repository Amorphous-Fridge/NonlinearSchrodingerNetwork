2021-06-25
loss,10.021353721618652,1.7506320476531982,1.6256712675094604,1.883851170539856,1.883710503578186,1.6656533479690552,1.7440489530563354,1.6799250841140747,1.1773699522018433,1.9655983448028564,1.0728659629821777,2.853910207748413,1.729619026184082,1.509018898010254,2.101344585418701,1.5517479181289673,1.4484875202178955,1.179593801498413,1.4588948488235474,0.9433667063713074,1.3408147096633911,1.1961424350738525,1.1142027378082275,1.0962016582489014,1.2504932880401611,0.763052225112915,1.081024408340454,1.1277480125427246,1.109683871269226,0.8112615942955017,0.8503111004829407,1.2877200841903687,1.17122220993042,1.6939904689788818,1.8275827169418335,0.9651954174041748,0.96867835521698,0.96381014585495,1.5131139755249023,1.3713465929031372,1.249343991279602,1.046639323234558,0.8565409183502197,0.9519855976104736,1.1562286615371704,1.0571767091751099,1.2406589984893799,1.0761778354644775,0.6460840106010437,0.9669293165206909,0.9110357761383057,0.5465371608734131,0.9927883148193359,0.7446333169937134,0.8425570130348206,0.7359516024589539,0.6933102607727051,0.8123645782470703,0.7408267259597778,0.7754170298576355,1.1896891593933105,1.5461941957473755,1.0458673238754272,0.9001333713531494,1.0223839282989502,0.6082596778869629,0.8329770565032959,0.8715991973876953,0.6978662014007568,0.6302616000175476,1.1458357572555542,1.0252394676208496,1.2278861999511719,0.6144896149635315,0.8226443529129028,0.8609441518783569,0.7759243845939636,0.9249509572982788,1.1326707601547241,0.6911503672599792,0.7142189145088196,0.772919237613678,0.7453770637512207,0.7751352190971375,0.8452455997467041,0.7121644020080566,1.1900628805160522,0.9631733894348145,0.827893853187561,0.7361246347427368,1.0545551776885986,0.5867331027984619,0.8580438494682312,0.7710250616073608,0.8037474155426025,1.2934539318084717,0.6177991628646851,0.5681123733520508,0.6614652276039124,0.49221503734588623,
mse,0.7114515900611877,0.617231547832489,0.6254144310951233,0.6332303881645203,0.6477811336517334,0.6666088700294495,0.6808308362960815,0.699436366558075,0.6863892078399658,0.6564333438873291,0.559593915939331,0.6257361173629761,0.5743119716644287,0.5839500427246094,0.6867751479148865,0.6718675494194031,0.6890980005264282,0.6896730661392212,0.6747003793716431,0.6732805371284485,0.6513464450836182,0.6734643578529358,0.704691469669342,0.6914364099502563,0.6975743174552917,0.6982772946357727,0.6915966868400574,0.6963291764259338,0.6976268291473389,0.6975632309913635,0.6898980140686035,0.7069582939147949,0.680359959602356,0.6542873978614807,0.5864315032958984,0.5832769870758057,0.6045234203338623,0.599916934967041,0.609100878238678,0.6198753714561462,0.6180076599121094,0.6174025535583496,0.6161754727363586,0.6045777201652527,0.5717949867248535,0.5987551212310791,0.5744947195053101,0.6065546274185181,0.6232262253761292,0.6258010268211365,0.609711766242981,0.617927610874176,0.6213954091072083,0.6249641180038452,0.6225342750549316,0.6080735921859741,0.6110972762107849,0.6107111573219299,0.6114903092384338,0.6146883964538574,0.6010580658912659,0.47390928864479065,0.4868561625480652,0.5206219553947449,0.5499472618103027,0.567987322807312,0.5709208250045776,0.581972062587738,0.590598464012146,0.5886589288711548,0.583669900894165,0.5824176669120789,0.5740928053855896,0.5886629223823547,0.5780869126319885,0.5988477468490601,0.6029771566390991,0.591072142124176,0.6063150763511658,0.6025458574295044,0.6067865490913391,0.6011838912963867,0.596900224685669,0.6015298962593079,0.6007640957832336,0.5996116399765015,0.5649560689926147,0.5852547287940979,0.5939979553222656,0.5891551375389099,0.5532810091972351,0.5893756151199341,0.5928854942321777,0.576690137386322,0.5870940089225769,0.5924997329711914,0.5936311483383179,0.5958560705184937,0.5941159725189209,0.5937613844871521,
mae,0.6887695789337158,0.6174641251564026,0.6146703362464905,0.6198084950447083,0.6231983304023743,0.6392400860786438,0.6535744667053223,0.6736327409744263,0.6566954851150513,0.6278871297836304,0.5529882311820984,0.6174777150154114,0.5587121844291687,0.5698121786117554,0.6764458417892456,0.6582968235015869,0.6706464886665344,0.6655335426330566,0.653614342212677,0.6527412533760071,0.6400710344314575,0.6546943783760071,0.683607816696167,0.6671156883239746,0.6701760292053223,0.674106240272522,0.6637237071990967,0.6695797443389893,0.6724065542221069,0.6716694831848145,0.6652587652206421,0.6817442178726196,0.6605272889137268,0.6267651915550232,0.5682756900787354,0.5588727593421936,0.5808070302009583,0.5754369497299194,0.5848469734191895,0.5942900776863098,0.5923041701316833,0.5912182927131653,0.5919831991195679,0.5798598527908325,0.5539494752883911,0.5740752220153809,0.5639602541923523,0.5832103490829468,0.5968824625015259,0.5998364686965942,0.5835603475570679,0.5932725667953491,0.597235381603241,0.6002947092056274,0.597494900226593,0.5845478177070618,0.5877796411514282,0.5870485305786133,0.588581919670105,0.5913031101226807,0.5803534984588623,0.501946747303009,0.5039985775947571,0.5189476609230042,0.5415416955947876,0.5542221069335938,0.555210530757904,0.5672026872634888,0.5741414427757263,0.5702887773513794,0.5652709007263184,0.5638995170593262,0.5597643256187439,0.5728449821472168,0.5594809055328369,0.5794399380683899,0.5821788907051086,0.5695959329605103,0.583337664604187,0.5799516439437866,0.5845897793769836,0.5786346197128296,0.5744838714599609,0.580608606338501,0.5784204602241516,0.5793902277946472,0.5495359301567078,0.5657933950424194,0.5733382701873779,0.5720201134681702,0.5421246290206909,0.5711670517921448,0.5710604190826416,0.5581304430961609,0.5662081837654114,0.5735057592391968,0.5728457570075989,0.5758936405181885,0.5737840533256531,0.5736951231956482,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 2)                 1186      
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 1188      
=================================================================
Total params: 2,374
Trainable params: 2,374
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 16)                80        
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 32)                544       
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 16)                528       
_________________________________________________________________
bottleneck (Dense)           (None, 2)                 34        
=================================================================
Total params: 1,186
Trainable params: 1,186
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 2)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 16)                48        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 32)                544       
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 16)                528       
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 68        
=================================================================
Total params: 1,188
Trainable params: 1,188
Non-trainable params: 0
_________________________________________________________________
