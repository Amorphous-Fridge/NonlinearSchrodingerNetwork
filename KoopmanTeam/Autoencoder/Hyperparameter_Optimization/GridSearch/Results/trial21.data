2021-06-25
loss,7.041929244995117,2.3893933296203613,3.138063430786133,1.5062369108200073,1.4544389247894287,2.0427393913269043,1.153042197227478,1.390038251876831,1.3787360191345215,1.3611695766448975,0.9954149127006531,1.2150720357894897,1.8507839441299438,0.6241594552993774,1.1382129192352295,0.7777625918388367,0.9805105328559875,1.461176872253418,0.9748615026473999,0.6918160319328308,1.0191617012023926,1.582018256187439,1.1433347463607788,0.8813634514808655,0.5655092000961304,0.8487997651100159,1.5319327116012573,0.8505916595458984,0.8472224473953247,0.6979196667671204,0.6536680459976196,0.67701256275177,0.7090380787849426,0.9030526876449585,0.7041923403739929,0.6337246894836426,0.7289074659347534,0.7674127817153931,0.7597541213035583,0.6557139754295349,0.626575767993927,0.8547402024269104,0.8787550330162048,0.6621778011322021,0.8709957599639893,0.8370543718338013,0.7548801898956299,0.5234517455101013,0.6049032807350159,0.5407168865203857,0.9073496460914612,0.710696280002594,0.5849830508232117,0.6947665214538574,0.7104770541191101,0.5833503603935242,0.6435781121253967,0.5532630681991577,0.5773930549621582,0.634657621383667,0.9605284333229065,0.8550511002540588,0.8374396562576294,0.7366504669189453,0.8070340752601624,0.7301045060157776,1.8827651739120483,0.9152591228485107,0.5440627932548523,0.6634886860847473,0.5855928659439087,0.6602377891540527,0.9740134477615356,0.7429512739181519,0.7679911255836487,0.7290676832199097,0.7301711440086365,0.7798265814781189,0.6681170463562012,0.9587980508804321,0.5070925354957581,0.8093158602714539,0.7044806480407715,0.7180030941963196,0.8423709273338318,0.6673163771629333,0.6154996752738953,0.5228734612464905,0.45251357555389404,0.6710094213485718,0.8156031966209412,0.7854058742523193,0.45588603615760803,0.5610746145248413,0.5504160523414612,0.4731498956680298,0.48668915033340454,0.5143278241157532,0.535174548625946,0.6678189635276794,
mse,0.6751428842544556,0.5389922857284546,0.5693942904472351,0.5976051092147827,0.6119104027748108,0.6359003186225891,0.6572355628013611,0.6579558849334717,0.6688154339790344,0.6607740521430969,0.6655877828598022,0.6577497124671936,0.6646098494529724,0.6610851883888245,0.6723313331604004,0.6626696586608887,0.6644197106361389,0.6394884586334229,0.6539748907089233,0.6507303714752197,0.6541857719421387,0.6484250426292419,0.6036650538444519,0.5926638245582581,0.585737407207489,0.584670901298523,0.575848400592804,0.5785739421844482,0.572501540184021,0.5739445686340332,0.5657947063446045,0.5644198060035706,0.5696143507957458,0.5707054138183594,0.5815366506576538,0.5807138681411743,0.5814256072044373,0.579028308391571,0.573357880115509,0.5754755139350891,0.5675709247589111,0.5671953558921814,0.5779813528060913,0.5792714357376099,0.5812556147575378,0.5894620418548584,0.5765869617462158,0.5645914673805237,0.5674341917037964,0.5711480975151062,0.5734326839447021,0.5696566700935364,0.5646048188209534,0.5675609111785889,0.572601318359375,0.570521354675293,0.5699338912963867,0.5745731592178345,0.575381875038147,0.5770453810691833,0.5848870277404785,0.5877513289451599,0.5881913900375366,0.5802882313728333,0.5811329483985901,0.5776917934417725,0.5821130275726318,0.5712199211120605,0.5768141746520996,0.5756407380104065,0.576730489730835,0.5768296718597412,0.5830773711204529,0.578697919845581,0.5681434273719788,0.5672450065612793,0.5691016316413879,0.578315258026123,0.5808394551277161,0.5816240310668945,0.5740836262702942,0.5762941241264343,0.5814728736877441,0.5862833261489868,0.5830702781677246,0.5828914642333984,0.5824276804924011,0.5820268988609314,0.5780757069587708,0.5809081792831421,0.5765442848205566,0.5770968198776245,0.5785496234893799,0.5761198997497559,0.5763378739356995,0.5787136554718018,0.5851563215255737,0.5843249559402466,0.5844826698303223,0.5755952000617981,
mae,0.6894471645355225,0.5728306174278259,0.593086302280426,0.6147026419639587,0.6190704107284546,0.6278820633888245,0.6438205242156982,0.6421354413032532,0.6481350660324097,0.6410563588142395,0.645537257194519,0.6396267414093018,0.6448178887367249,0.6409258246421814,0.6480435132980347,0.6458013653755188,0.6482200622558594,0.6204823851585388,0.6360015273094177,0.6326409578323364,0.6307129859924316,0.6178544759750366,0.5849835276603699,0.5726384520530701,0.5653185844421387,0.5666708946228027,0.562102198600769,0.5611554980278015,0.5532834529876709,0.5560545921325684,0.5471663475036621,0.5456337332725525,0.5505043864250183,0.5527293682098389,0.5616279244422913,0.5609620809555054,0.5617223978042603,0.5588288307189941,0.5546895861625671,0.5567755103111267,0.5476542711257935,0.5480356812477112,0.5583322644233704,0.5596129894256592,0.5623458623886108,0.5683670043945312,0.5559760928153992,0.5465041995048523,0.5491319894790649,0.55168616771698,0.5551311373710632,0.5501602292060852,0.5473268032073975,0.550365149974823,0.5530543327331543,0.5521487593650818,0.5527479648590088,0.5563530921936035,0.5565087795257568,0.5577837228775024,0.5647416114807129,0.5675960779190063,0.5698407888412476,0.5626341104507446,0.5623776912689209,0.560787558555603,0.5691124200820923,0.5554632544517517,0.5584747791290283,0.5580971240997314,0.5577329397201538,0.5583415031433105,0.5625208020210266,0.5583093166351318,0.547319769859314,0.547945499420166,0.5501601099967957,0.5589781403541565,0.5603646636009216,0.5694668292999268,0.5551289916038513,0.5576972961425781,0.561913013458252,0.5667006969451904,0.5632361769676208,0.5635724067687988,0.5631712079048157,0.5630984306335449,0.5605298280715942,0.5618676543235779,0.5609880089759827,0.5585635900497437,0.5599839687347412,0.5581440329551697,0.5576855540275574,0.5604922771453857,0.5643658638000488,0.5632292032241821,0.5656723380088806,0.5568114519119263,
Loss,autoencoding_loss
Optimizer,Adam
Learning Rate,.001
Steps Per Epoch,50
Batch Size,4096
Epochs,100

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
Phi (Functional)             (None, 2)                 8578      
_________________________________________________________________
Phi_inv (Functional)         (None, 4)                 8580      
=================================================================
Total params: 17,158
Trainable params: 17,158
Non-trainable params: 0
_________________________________________________________________
Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 16)                80        
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 256)               4352      
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 16)                4112      
_________________________________________________________________
bottleneck (Dense)           (None, 2)                 34        
=================================================================
Total params: 8,578
Trainable params: 8,578
Non-trainable params: 0
_________________________________________________________________
Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 2)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 16)                48        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 256)               4352      
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 16)                4112      
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 68        
=================================================================
Total params: 8,580
Trainable params: 8,580
Non-trainable params: 0
_________________________________________________________________
