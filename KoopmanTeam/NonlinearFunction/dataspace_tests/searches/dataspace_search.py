import tensorflow as tf
import numpy as np
import os

import sys

from utils import *

'''
This file handles the training of our models.  

The number of initial conditions is 
controlled by the slurm array ID, which selects the element of the dataset_sizes
tuple that will be used for a particular run.

PARAMETERS TO CONFIGURE:
------------------------

DATADIR: Path to the directory containing the dataset
         NOTE: this is NOT the path to the dataset itself,
         just the directory containing the dataset.  The name of the dataset
         is automatically determined

WRITEDIR: Path to directory to write datafiles to.  Again, nameing is automatic

MODELDIR: Path to direcory to save model to.  Naming automatic

TIMESTEP: Timestep to evolve model with
TIMERANGE: Range of time model is evolved through
MAXEVOLVE: The number of initial conditions in the dataset 
           (NOT the number of initial conditions we are training on,
           that is determined via the slurm ID)

           NOTE: Above three params determine which dataset is used.
                 If a dataset matching these specifications is not found,
                 the program will crash (no nice error handling for it either)

MEMFIT: Whether or not we expect the dataset to fit into memory or not.
        Training is faster if it can fit into memory, but that can be a tall order.
        (CURRENTLY BROKEN)

EPOCHS: The number of epochs to train the model for each 'round' of training.
        The first round will be EPOCHS epochs with a learning rate of .001 and
        the second round is another EPOCHS epochs with a learning rate of .0001.
        
COMPRESSED_TRAINING: If true, we train the model in the compressed space.  If false, 
                     we train the model in the original, uncompressed space.  Which 
                     space the model is trained in determines where the loss function 
                     is evaluated (do we want the points close to each other in the 
                     compressed space, or the uncompressed space?).
                     If true, the dataset must be pre-compressed ahead of time using 
                     the tools provided in the processing directory.



OUTPUTS:
--------

A datafile with a .data extension will be created in WRITEDIR.  Contents will be 
generated by the write_history and append_history functions found in utils.py.
A .h5 model will also be saved to MODELDIR.
NO CHECKPOINTING HAS BEEN IMPLEMENTED.  The model is only saved at the end of the entire run,
and the datafile is only written/updated at the end of each round of training (large/small learning rate).
The reason is that the datafile writing functions weren't written to work with them.  Probably something
to fix later, but this works well enough for now.


Naming of files is handled automatically, following the naming convention
0t{timerange}_dt{timestep}_{number of initial conditions}inits
Just to make it confusing though, the dataset that is read must follow the convention
0t{timerange}_{number of initial conditions}inits_dt{timestep}
(sry, didn't notice that till the datasets were already made)
((wait, couldn't we have just renamed them? Nonsense, such ideas are beyond the reach of mortal men!))

'''




####Theese are the only values that need to be configured####
DATADIR='/fslhome/wilhiteh/datasets/'
WRITEDIR='/fslhome/wilhiteh/datafiles/'
MODELDIR='/fslhome/wilhiteh/models/'
TIMESTEP = 0.025
TIMERANGE = 20
MAXEVOLVE = 50000
BATCH_SIZE = 100000 #100000 points on bloch w/ ~500k params fit in ~2GB VRAM, FSC has ~12GB
MEMFIT = True #MEMFIT=False is currently NOT working
EPOCHS = 250
COMPRESSED_TRAINING = False
#############################################################

#Determine what our dataset is
suffix = ''
if COMPRESSED_TRAINING:
    suffix += '_compressed'
if not MEMFIT:
    suffix += '_TFRecord'
suffix += '/'

timestep = str(TIMESTEP).replace('.','p')

dataset = '0t{}_{}inits_dt{}_memfit{}'.format(TIMERANGE,num_files,timestep,suffix)



#Set dataset sizes to test
dataset_sizes = (2000, 3000, 4000, 5000, 7500, 10000, 12500, 15000, 17500, 20000, 25000, 30000, 35000, 40000, 45000, 50000)

size = int(os.getenv('SLURM_ARRAY_TASK_ID'))

datasize = dataset_sizes[size]

if COMPRESSED_TRAINING:
    NAME = '0t{}_dt{}_{}inits'.format(TIMERANGE,timestep,datasize)
else:
    NAME = '0t{}_dt{}_{}inits_blochspace'.format(TIMERANGE,timestep,datasize)
   
    
    
print('Running with name {}\nMEMFIT is {}, CAPBYEVOLVE is {}, and the max evolution file is {}\nReading from dataset {}'.format(NAME,MEMFIT,CAPBYEVOLVE,MAXEVOLVE,DATADIR+dataset))

#Some GPU configuration
#If we want to use multiple GPUs, this bit will need to be modified

physical_devices = tf.config.experimental.list_physical_devices('GPU')
if len(physical_devices) != 0:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
elif len(physical_devices) == 0:
    print("Warning: No GPU detected.  Running tensorflow on CPU")
    

#Import autoencoder functions, needed for comrpessing the data into a space the nonlinear function can train on
Phi = tf.keras.models.load_model('/fslhome/wilhiteh/Autoencoder/trial25e1000Phi.h5', compile=False, custom_objects={'Functional':tf.keras.models.Model})
Phi_inv = tf.keras.models.load_model('/fslhome/wilhiteh/Autoencoder/trial25e1000Phi_inv.h5', compile=False, custom_objects={'Functional':tf.keras.models.Model})


#Loss function
def L2_loss(y_true, y_pred):
    return tf.norm(y_true-y_pred, ord=2, axis=-1)



#This function leads to much quicker training but
##THE DATASET MUST FIT INTO MEMORY
def get_multiple_evolutions(datadir, num_evolutions, train_evolutions):

    pre_evolution = []
    post_evolution = []

    for i in range(train_evolutions):

        with open(datadir+'evolution{}.csv'.format(i), 'r') as f:
            data = np.genfromtxt(f, delimiter=',', skip_header=1)[:,1:]

            pre_evolution.append(data[0])
            for k in data[1:-1]:
                pre_evolution.append(k)
                post_evolution.append(k)
            post_evolution.append(data[-1])

    train = tf.data.Dataset.from_tensor_slices((pre_evolution, post_evolution))

    pre_evolution = []
    post_evolution = []

    for i in range(train_evolutions, num_evolutions):

        with open(datadir+'evolution{}.csv'.format(i), 'r') as f:
            data = np.genfromtxt(f, delimiter=',', skip_header=1)[:,1:]

            pre_evolution.append(data[0])
            for k in data[1:-1]:
                pre_evolution.append(k)
                post_evolution.append(k)
            post_evolution.append(data[-1])

    test = tf.data.Dataset.from_tensor_slices((pre_evolution, post_evolution))

    return train.shuffle(1000000, reshuffle_each_iteration=True), test.shuffle(1000000, reshuffle_each_iteration=True)


inputs = tf.keras.Input(shape=3)



#Only used if MEMFIT=False 
def _parse(ex):
    desc = {'pre_evolve': tf.io.FixedLenFeature([3,], tf.float32), 'post_evolve': tf.io.FixedLenFeature([3,], tf.float32)}
    sample = tf.io.parse_single_example(ex, desc)
    return (sample['pre_evolve'], sample['post_evolve'])



if MEMFIT:
    train_data, val_data = get_multiple_evolutions(DATADIR+dataset,datasize,int(0.9*datasize))
else:
    print('MEMFIT false, using TFRecord solution')

    training_filenames = [DATADIR+dataset+x for x in os.listdir(DATADIR+dataset) if x.endswith('.tfrecord')]
    training_files = tf.data.Dataset.from_tensor_slices(training_filenames) 

    #Ideally we would use tf.data.experimental.AUTOTUNE for all the num_parallel stuff, but it appears to be 
    ##busted in tf 2.0, which the supercomputer runs
    train_data = tf.data.TFRecordDataset(training_files, num_parallel_reads=12, buffer_size=100).map(_parse, num_parallel_calls=12)
    #For some reason, this is loading in the evolution data as evolutions, rather than individual timesteps
    ##Same code works as expected on my laptop in tf 2.3.1.  Dataset itself seems alright as well, though that
    ##hasn't been investigated thouroughly yet


    val_data = train_data.skip(int(0.9*datasize)).take(int(0.1*datasize))
    train_data = train_data.take(int(0.9*datasize))


    val_data = val_data.shuffle(1000000, reshuffle_each_iteration=True)
    train_data = train_data.shuffle(1000000, reshuffle_each_iteration=True)
    

train_data = train_data.batch(100000)
val_data = val_data.batch(100000)


if not MEMFIT:
    print('Enabling prefetching')

    #Same deal with the AUATOTUNE here on the prefetch
    train_data = train_data.prefetch(4)
    val_data = val_data.prefetch(4)
    



#Re-declare model each time to re-initilize parameters
nonlinear_layer_1 = tf.keras.layers.Dense(64, activation='selu', name='nonlinear_layer_1')(inputs)
nonlinear_layer_2 = tf.keras.layers.Dense(256, activation='selu', name='nonlinear_layer_2')(nonlinear_layer_1)
nonlinear_layer_3 = tf.keras.layers.Dense(512, activation='selu', name='nonlinear_layer_3')(nonlinear_layer_2)
nonlinear_layer_4 = tf.keras.layers.Dense(512, activation='selu', name='nonlinear_layer_4')(nonlinear_layer_3)
nonlinear_layer_5 = tf.keras.layers.Dense(256, activation='selu', name='nonlinear_layer_5')(nonlinear_layer_4)
nonlinear_layer_6 = tf.keras.layers.Dense(64, activation='selu', name='nonlinear_layer_6')(nonlinear_layer_5)
evolved = tf.keras.layers.Dense(3, activation='selu', name='evolved_state_layer')(nonlinear_layer_6)

NonlinearEvolution = tf.keras.Model(inputs=inputs, outputs=evolved, name='Evolution')

if COMPRESSED_TRAINING:

    #First 250 epochs w/ high learning rate
    NonlinearEvolution.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=.001), loss=L2_loss, metrics=['mse', 'mae'])
    history = NonlinearEvolution.fit(train_data, validation_data=val_data, epochs=EPOCHS)
    write_history(history, NonlinearEvolution,loss='L2_loss',batch_size='100000', 
                  other_info={'dataset':dataset+' (First {} evolutions)'.format(datasize),
                  'validation':'{}inits'.format(int(0.1*datasize))}, 
                  savedname=NAME+'.data',
                  datadir=WRITEDIR)


    #Second 250 epochs w/ lower learning rate
    NonlinearEvolution.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=.0001), loss=L2_loss, metrics=['mse', 'mae'])
    history = NonlinearEvolution.fit(train_data, validation_data=val_data, epochs=EPOCHS)
    append_history(history, trial=NAME+'.data', 
                   params={'Learning Rate':.0001}, datadir=WRITEDIR)

    #Save the model and label it w/ amount of data it was trained on
    NonlinearEvolution.save(MODELDIR+NAME+'.h5')
    
else:
    
    Phi.trainable = False
    Phi_inv.trainable = False
    
    AntiKoopman = tf.keras.models.Sequeuntial([Phi, NonlinearEvolution, Phi_inv], name='AntiKoopman')
    
    #First 250 epochs w/ high learning rate
    AntiKoopman.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=.001), loss=L2_loss, metrics=['mse', 'mae'])
    history = AntiKoopman.fit(train_data, validation_data=val_data, epochs=EPOCHS)
    write_history(history, [AntiKoopman, Phi, NonlinearEvolution, Phi_inv],loss='L2_loss',batch_size='100000', 
                  other_info={'dataset':dataset+' (First {} evolutions)'.format(datasize),
                  'validation':'{}inits'.format(int(0.1*datasize))}, 
                  savedname=NAME+'.data',
                  datadir=WRITEDIR)


    #Second 250 epochs w/ lower learning rate
    AntiKoopman.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=.0001), loss=L2_loss, metrics=['mse', 'mae'])
    history = AntiKoopman.fit(train_data, validation_data=val_data, epochs=EPOCHS)
    append_history(history, trial=NAME+'.data', 
                   params={'Learning Rate':.0001}, datadir=WRITEDIR)

    #Save the model and label it w/ amount of data it was trained on
    NonlinearEvolution.save(MODELDIR+NAME+'.h5')
