{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End to end test of our general network structure.  Will test on the known non-linear system $$\\dot{x} = ax,$$ $$\\dot{y} = b(x-x^2).$$  The Koopman operator for this system is $$\\frac{d}{dt}\\begin{bmatrix}z_1\\\\ z_2\\\\ z_3\\end{bmatrix} = \\begin{bmatrix} a & 0 & 0\\\\ b& 0 & -b\\\\ 0 & 0 & 2a\\end{bmatrix}\\begin{bmatrix}z_1\\\\ z_2\\\\ z_3\\end{bmatrix}$$ where $$z_1 = x,$$ $$z_2 = y,$$ $$z_3 = x^2.$$ \n",
    "\n",
    "We will have to select $a$ and $b$ ahead of time and train for specific instances of them (equivilent to switching hamiltionians).\n",
    "\n",
    "The solution to the original system for us to compare results agains is $$x(t) = e^{at}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import redirect_stdout  #Used for writing model architecture to datafiles\n",
    "import matplotlib.pyplot as plt         \n",
    "from datetime import date               #Used for datafiles\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.integrate\n",
    "import os\n",
    "\n",
    "\n",
    "#Some GPU configuration\n",
    "#Always uses the 1st GPU avalible (if avalible) unless 1st line is uncommented, in which case no GPU is used\n",
    "\n",
    "#tf.config.set_visible_devices([], 'GPU') #uncomment to set tensorflow to use CPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) != 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\phi$ and $\\phi^{-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_DIMENSION = 3\n",
    "ANTIKOOPMAN_DIMENSION = 2\n",
    "\n",
    "#Train autoencoder on uniform distribution between -MAX_DIM_VALUE and MAX_DIM_VALUE\n",
    "MAX_DIM_VALUE = 100.\n",
    "\n",
    "#LecunUniform seems to give the most consistent results, \n",
    "#though the model still struggles to get started sometimes\n",
    "INITILIZER = tf.keras.initializers.LecunUniform()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_autoencoder_data(batch_size = 512):\n",
    "    '''Generator for data when training the \n",
    "    autoencoder end-to-end\n",
    "    '''\n",
    "    koopman_dimension = 3\n",
    "    triples = np.empty([batch_size, koopman_dimension])\n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            z1, z2 = np.random.uniform(low=-MAX_DIM_VALUE, high=MAX_DIM_VALUE, size=2)\n",
    "            z3 = z1*z1\n",
    "            triples[i] = np.array([z1,z2,z3])\n",
    "\n",
    "        yield (triples, triples)\n",
    "        \n",
    "def generate_encoder_data(batch_size = 4096):\n",
    "    '''Generator to be used when training just\n",
    "    the encoder (phi) on its own\n",
    "    '''\n",
    "    koopman_dimension = 3\n",
    "    doubles = np.empty([batch_size, ANTIKOOPMAN_DIMENSION])\n",
    "    triples = np.empty([batch_size, koopman_dimension])\n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            z1, z2 = np.random.uniform(low=-MAX_DIM_VALUE, high=MAX_DIM_VALUE, size=2)\n",
    "            z3 = z1*z1\n",
    "            doubles[i] = np.array([z1,z2])\n",
    "            triples[i] = np.array([z1, z2, z3])\n",
    "        yield(triples, doubles)\n",
    "        \n",
    "def generate_decoder_data(batch_size = 4096):\n",
    "    '''Generator to be used when training just the\n",
    "    decoder (phi_inv) on its own\n",
    "    '''\n",
    "    doubles = np.empty([batch_size, ANTIKOOPMAN_DIMENSION])\n",
    "    triples = np.empty([batch_size, STATE_DIMENSION])\n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            z1, z2 = np.random.uniform(low=-MAX_DIM_VALUE, high=MAX_DIM_VALUE, size=2)\n",
    "            z3 = z1*z1\n",
    "            doubles[i] = np.array([z1,z2])\n",
    "            triples[i]=np.array([z1,z2,z3])\n",
    "        yield(doubles, triples)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input layers for the encoder and decoder, respectivley\n",
    "initial_state = tf.keras.Input(shape = STATE_DIMENSION)\n",
    "antikoop_state = tf.keras.Input(shape = ANTIKOOPMAN_DIMENSION)\n",
    "\n",
    "##########################################ENCODER####################################################################\n",
    "encoding_layer_1 = tf.keras.layers.Dense(8, activation=\"selu\", name='encoding_layer_1', kernel_initializer = INITILIZER, bias_initializer = INITILIZER)(initial_state)\n",
    "encoding_layer_2 = tf.keras.layers.Dense(32, activation=\"selu\", name='encoding_layer_2', kernel_initializer = INITILIZER, bias_initializer = INITILIZER)(encoding_layer_1)\n",
    "norm_layer_1 = tf.keras.layers.BatchNormalization(name='norm_layer_1')(encoding_layer_2)\n",
    "encoding_layer_3 = tf.keras.layers.Dense(64, activation=\"selu\", name='encoding_layer_3', kernel_initializer = INITILIZER, bias_initializer = INITILIZER)(norm_layer_1)\n",
    "norm_layer_2 = tf.keras.layers.BatchNormalization(name='norm_layer_2')(encoding_layer_3)\n",
    "encoding_layer_4 = tf.keras.layers.Dense(32, activation=\"selu\", name='encoding_layer_4', kernel_initializer=INITILIZER, bias_initializer=INITILIZER)(norm_layer_2)\n",
    "norm_layer_3 = tf.keras.layers.BatchNormalization(name='norm_layer_3')(encoding_layer_4)\n",
    "encoding_layer_5 = tf.keras.layers.Dense(8, activation=\"selu\", name='encoding_layer_5', kernel_initializer=INITILIZER, bias_initializer=INITILIZER)(norm_layer_3)\n",
    "encoded_state = tf.keras.layers.Dense(ANTIKOOPMAN_DIMENSION, activation=\"selu\", name='bottleneck', kernel_initializer = INITILIZER, bias_initializer = INITILIZER)(encoding_layer_5)\n",
    "#####################################################################################################################\n",
    "\n",
    "#########################################DECODER#####################################################################\n",
    "decoding_layer_1 = tf.keras.layers.Dense(16, activation = \"selu\", name='decoding_layer_1', kernel_initializer = INITILIZER, bias_initializer = INITILIZER)(antikoop_state)\n",
    "decoding_layer_2 = tf.keras.layers.Dense(32, activation = \"selu\", name='decoding_layer_2', kernel_initializer = INITILIZER, bias_initializer = INITILIZER)(decoding_layer_1)\n",
    "norm_layer_4 = tf.keras.layers.BatchNormalization(name='norm_layer_4')(decoding_layer_2)\n",
    "decoding_layer_3 = tf.keras.layers.Dense(64, activation = \"selu\", name='decoding_layer_3', kernel_initializer = INITILIZER, bias_initializer = INITILIZER)(norm_layer_4)\n",
    "norm_layer_5 = tf.keras.layers.BatchNormalization(name='norm_layer_5')(decoding_layer_3)\n",
    "decoding_layer_4 = tf.keras.layers.Dense(32, activation = \"selu\", name='decoding_layer_4', kernel_initializer = INITILIZER, bias_initializer = INITILIZER)(norm_layer_5)\n",
    "norm_layer_6 = tf.keras.layers.BatchNormalization(name='norm_layer_6')(decoding_layer_4)\n",
    "decoding_layer_5 = tf.keras.layers.Dense(16, activation = \"selu\", name='decoding_layer_5', kernel_initializer = INITILIZER, bias_initializer = INITILIZER)(norm_layer_6)\n",
    "decoded_state = tf.keras.layers.Dense(STATE_DIMENSION, activation = \"selu\", name='decoded_layer', kernel_initializer = INITILIZER, bias_initializer = INITILIZER)(decoding_layer_5)\n",
    "#####################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "#Model declarations\n",
    "#Autoencoder = tf.keras.Model(initial_state, decoded_state, name = 'Autoencoder')\n",
    "\n",
    "Phi = tf.keras.Model(inputs=initial_state, outputs = encoded_state, name='Phi')\n",
    "Phi_inv = tf.keras.Model(inputs = antikoop_state, outputs = decoded_state, name='Phi_inv')\n",
    "\n",
    "Autoencoder = tf.keras.models.Sequential([Phi, Phi_inv], name='Autoencoder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoding_loss(y_true, y_pred):\n",
    "    '''The L2 norm of the input vector\n",
    "    and the autoencoded vector\n",
    "    Scale down to make gradients more tractable\n",
    "    with large vectors?'''\n",
    "    \n",
    "    return tf.norm((y_true-y_pred)*tf.constant([1.2,1.2,0.8]), ord=2)\n",
    "    \n",
    "    #return tf.abs(y_true[0]-y_pred[0])/y_true[0] + tf.abs(y_true[1]-y_pred[1])/y_true[1] + tf.abs(y_true[2]-y_pred[2])/y_true[2]\n",
    "    \n",
    "    #return tf.abs(y_pred[0]*y_pred[0] - y_pred[2]) + tf.abs(y_true[0]-y_pred[0]) + tf.abs(y_true[1] - y_pred[1]) + tf.abs(y_true[2]-y_pred[2])\n",
    "\n",
    "def encoder_loss(y_true, y_pred):\n",
    "    return tf.norm(y_true-y_pred, ord=2)\n",
    "\n",
    "def decoder_loss(y_true, y_pred):\n",
    "    return tf.norm(y_true-y_pred, ord=2) + 2*tf.abs(y_true[0]-y_pred[0]) + 2*tf.abs(y_true[1]-y_pred[1])\n",
    "\n",
    "\n",
    "\n",
    "def predict_single_state(state, encoder = Phi, decoder = Phi_inv):\n",
    "    '''Outputs the prediction of a single \n",
    "    state.  Primarily for sanity checks.\n",
    "    '''\n",
    "    encoded = encoder(np.array([state,]))\n",
    "    decoded = decoder(encoder(np.array([state,]))).numpy()[0]\n",
    "    L2_dist = np.linalg.norm(state-decoded, ord=2)\n",
    "\n",
    "\n",
    "    print('Initial State:{}\\nEncoded State:{}\\nDecoded State:{}\\nL2 Distance:{}\\nInput z1 Squared:{}\\nOutput z1 Squared:{}\\nz3-z1^2:{}\\nLoss:{}'.format(\n",
    "            state, encoded.numpy(), decoded, L2_dist, state[0]*state[0],\n",
    "            decoded[0]*decoded[0], decoded[2]-decoded[0]*decoded[0], \n",
    "            L2_dist + np.abs(decoded[2]-decoded[0]*decoded[0])))\n",
    "          \n",
    "    return None\n",
    "\n",
    "\n",
    "###################DATA WRITING FUNCTIONS#####################\n",
    "\n",
    "##############################################################\n",
    "def write_history(history, model, loss = 'autoencoding_loss', \n",
    "                  optimizer='Adam', lr='.001', \n",
    "                  batch_size='1024', datadir='./datafiles/'):\n",
    "    '''Writes training history to a datafile\n",
    "    This will create a new trial datafile.  If the model has \n",
    "    had additional training, append_history should be used instead.\n",
    "    \n",
    "    PARAMS:\n",
    "    -------\n",
    "    history - The history callback returned by the model.fit method in keras\n",
    "    model - The model (or list of models) which we want to write the architecture of to the datafile\n",
    "    string loss - The loss function the model was trained on\n",
    "    string optimizer - The optimizer the model was compiled with\n",
    "    string lr - The learning rate the model was initially compiled with\n",
    "    string spe - The number of steps per epoch\n",
    "    string batch_size - The number of samples trained on per step\n",
    "    string datadir - Directory where the datafiles are stored\n",
    "    '''\n",
    "    \n",
    "    rundatadir = datadir\n",
    "    filename = 'trial'+str(len(os.listdir(rundatadir)))\n",
    "\n",
    "    with open(rundatadir+filename+'.data', 'w') as f:\n",
    "        f.write(str(date.today())+'\\n')\n",
    "        for key in history.history.keys():\n",
    "            f.write(key+',')\n",
    "            for epoch in range(history.params['epochs']):\n",
    "                f.write(str(history.history[key][epoch])+',')\n",
    "            f.write('\\n')\n",
    "        f.write(\"Loss,{}\\nOptimizer,{}\\nLearning Rate,{}\\nSteps Per Epoch,{}\\nBatch Size,{}\\nEpochs,{}\\n\".format(loss,optimizer,lr,history.params['steps'],batch_size, history.params['epochs']))\n",
    "        f.write('\\n')\n",
    "        with redirect_stdout(f):\n",
    "            if type(model) == list:\n",
    "                for i in model:              \n",
    "                    i.summary()\n",
    "            else:\n",
    "                model.summary()\n",
    "    return rundatadir+filename+'.data'\n",
    "\n",
    "#####################################################################\n",
    "#####################################################################\n",
    "\n",
    "def append_history(history, trial, datadir='./datafiles/', params_update = True, \n",
    "                   params = {'Loss':None, 'Optimizer':None, 'Learning Rate':None, 'Batch Size':None}):\n",
    "    '''Appends new training data to trial datafile.\n",
    "    This will only work with datafiles written using write_history (or files of identical form)\n",
    "    \n",
    "    PARAMS:\n",
    "    -------\n",
    "    history - The history callback containing the new run's data\n",
    "    int trial - The trial number to append the data to.  If we want to append data to \n",
    "            trial43.data, this would be 43\n",
    "    str datadir - The directory containing the datafile\n",
    "    bool params_update - Boolean indicating if we should update parameters (loss used, optimizer, etc.)\n",
    "                    in addition to adding the new loss data\n",
    "    dict params - Dictionary containing updated parameter values\n",
    "                  If parameter is not included, the previous value \n",
    "                  written for that parameter will be repeated\n",
    "    '''\n",
    "    \n",
    "    filename = 'trial'+str(trial)+'.data'\n",
    "    \n",
    "    \n",
    "    newlines = []\n",
    "    keys = history.history.keys()\n",
    "    \n",
    "    \n",
    "    for k in ['Loss', 'Optimizer', 'Learning Rate', 'Batch Size']:\n",
    "        if k not in params.keys():\n",
    "            params[k] = None\n",
    "    params['Epochs'] = history.params['epochs']\n",
    "    params['Steps Per Epoch'] = history.params['steps']\n",
    "   \n",
    "      \n",
    "    #Read old data and add in new data as it is read\n",
    "    with open(datadir+filename, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            \n",
    "            tag = line.split(',')[0]\n",
    "            \n",
    "            if tag in keys:\n",
    "                newdata = [str(x) for x in history.history[tag]]\n",
    "                newlines.append(line.split(',')[:-1] + newdata ) #[:-1] to drop the newline\n",
    "            elif (tag in params.keys() and params_update == True):\n",
    "                if params[tag] is None:\n",
    "                    newlines.append(line.strip().split(',')[:] + [line.strip().split(',')[-1]])\n",
    "                else:\n",
    "                    newdata = [str(params[tag])]\n",
    "                    newlines.append(line.strip().split(',')[:] + newdata)\n",
    "            else:\n",
    "                newlines.append(line)\n",
    "    \n",
    "    #Write the old data with the appended data\n",
    "    with open(datadir+filename, 'w') as f:\n",
    "        for el in newlines:\n",
    "            if type(el) == list:\n",
    "                f.write(','.join(el))\n",
    "                f.write('\\n')\n",
    "            else:\n",
    "                f.write(el)\n",
    "    return\n",
    "    \n",
    "    \n",
    "#####################################################################\n",
    "#####################################################################\n",
    "    \n",
    "def loss_plot(trial, datadir='./datafiles/', savefig = True,\n",
    "              figdir = './figures/', logplot=False,\n",
    "              metric = None, mark_runs = False, \n",
    "              skip_epochs=0, mark_lowest = True):\n",
    "    '''Creates a plot of the loss/metric for the given trial number\n",
    "    '''\n",
    "    \n",
    "    if metric == None:\n",
    "        metric = 'loss'\n",
    "\n",
    "    losses = []\n",
    "    runs = []\n",
    "    \n",
    "    #Read in the data\n",
    "    with open(datadir+'trial'+str(trial)+'.data', 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if line.split(',')[0] == metric:\n",
    "                losses = [float(x) for x in line.strip().split(',')[1:]]\n",
    "                if not mark_runs:\n",
    "                    break\n",
    "            elif (line.split(',')[0] == 'Epochs' and mark_runs == True):\n",
    "                runs = ['0.']+line.strip().split(',')[1:]\n",
    "                runs = [float(runs[i-1])+float(runs[i-2]) for i in range(2, len(runs))]\n",
    "                break\n",
    "            \n",
    "    \n",
    "    fig, ax = plt.subplots(1,1, figsize = (8,8))\n",
    "\n",
    "\n",
    "    ax.plot(range(len(losses[skip_epochs:])), losses[skip_epochs:])\n",
    "    if mark_runs:\n",
    "        for i in runs:\n",
    "            ax.plot([i,i], ax.get_ylim(), c='black', ls=':')\n",
    "    if mark_lowest:\n",
    "        lowest = min(losses)\n",
    "        ax.plot(losses.index(lowest), lowest, 'go')\n",
    " #       ax.text(ax.get_xlim()[1]-0.05*ax.get_xlim()[0], ax.get_ylim()[1]-0.05*ax.get_ylim()[0], 'Lowest loss: {}'.format(lowest))\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    if logplot:\n",
    "        ax.set_yscale('log')\n",
    "    ax.set_title('Trial '+str(trial)+' Loss')\n",
    "\n",
    "    if savefig:\n",
    "        fig.savefig(figdir+'trial{}.png'.format(trial))\n",
    "    \n",
    "    return None\n",
    "\n",
    "#####################################################################\n",
    "#####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = .0001), loss=autoencoding_loss, metrics = ['mse', 'mae'], run_eagerly=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = Autoencoder.fit(generate_autoencoder_data(4096), steps_per_epoch=50,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1, z2 = np.random.uniform(low=-MAX_DIM_VALUE, high=MAX_DIM_VALUE, size=2)\n",
    "z3 = z1*z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State:[-6.090724688422071, -8.191452779735542, 37.09692723015414]\n",
      "Encoded State:[[-1.3374066  -0.22778395]]\n",
      "Decoded State:[-1.5950345 -1.7481171 -1.7580993]\n",
      "L2 Distance:39.64140377549842\n",
      "Input z1 Squared:37.09692723015414\n",
      "Output z1 Squared:2.544135093688965\n",
      "z3-z1^2:-4.302234649658203\n",
      "Loss:43.94363842515662\n"
     ]
    }
   ],
   "source": [
    "predict_single_state([z1,z2,z3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./trials/trial0.data'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_history(history, [Phi, Phi_inv, Autoencoder], datadir='./trials/', batch_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_history(history, 2, datadir='./trials/', params={'Learning Rate':.0001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi.save('./models/Phi.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear evolution function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure and set up koopman operator\n",
    "#These parameters keep z1 and z2 within +/-100 from t=0.0 to t=10.0 provided z1 and z2 are between 0 and 1 at t=0.0\n",
    "system_a = 0.2\n",
    "system_b = 0.1\n",
    "koopman_operator = np.array([[system_a, 0, 0], [system_b, 0, -system_b], [0, 0, 2*system_a]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def koopman_evolve(t, y):\n",
    "    return np.matmul(koopman_operator, y)\n",
    "\n",
    "        \n",
    "def generate_dynamical_system_data(batch_size = 128, t_span = 10.0):\n",
    "    koopman_dimension = 3\n",
    "    systems = np.empty([batch_size, koopman_dimension])\n",
    "    initial_conditions = np.empty([batch_size, koopman_dimension])\n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            y0 = np.random.random(3)\n",
    "            evolved = []\n",
    "            evolution = scipy.integrate.solve_ivp(koopman_evolve, y0, (0, t_span)).y\n",
    "            for k in evolution:\n",
    "                evolved.append(k[-1])\n",
    "            systems[i] = evolved\n",
    "            initial_conditions[i] = y0\n",
    "        \n",
    "        yield (initial_conditions, systems)\n",
    "         \n",
    "    \n",
    "    \n",
    "#Idealized phi and phi_inv, gives an upper bound on accuracy of \n",
    "#nonlinear network\n",
    "def ideal_compression(state):\n",
    "    return np.array([state[0], state[1]])\n",
    "\n",
    "def ideal_decompression(state):\n",
    "    return np.array([state[0], state[1], state[0]*state[0]])\n",
    "\n",
    "#Generator which pre-applies perfect compression and decompression on system\n",
    "def generate_ideal_system_data(batch_size = 128, t_span=10.0):\n",
    "    systems = np.empty([batch_size, ANTIKOOPMAN_DIMENSION])\n",
    "    initial_conditions = np.empty([batch_size, ANTIKOOPMAN_DIMENSION])\n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            y0 = np.empty([STATE_DIMENSION])\n",
    "            y0[0], y0[1] = np.random.random(2)\n",
    "            y0[2] = y0[0]*y0[0]\n",
    "            #y0.reshape(3,1)\n",
    "            evolution = scipy.integrate.solve_ivp(koopman_evolve,(0,t_span), y0).y[:,-1]\n",
    "            initial_conditions[i] = ideal_compression(y0)\n",
    "            systems[i] = ideal_compression(evolution)\n",
    "        yield(initial_conditions, systems)\n",
    "\n",
    "def nonlinear_loss(y_true, y_pred):\n",
    "    return tf.norm(y_true-y_pred, ord=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(ANTIKOOPMAN_DIMENSION)\n",
    "\n",
    "nonlinear_layer_1 = tf.keras.layers.Dense(8, activation='selu', name='nonlinea_layer_1')(inputs)\n",
    "nonlinear_layer_2 = tf.keras.layers.Dense(16, activation='selu', name='nonlinear_layer_2')(nonlinear_layer_1)\n",
    "nonlinear_layer_3 = tf.keras.layers.Dense(32, activation='selu', name='nonlinear_layer_3')(nonlinear_layer_2)\n",
    "nonlinear_layer_4 = tf.keras.layers.Dense(16, activation='selu', name='nonlinear_layer_4')(nonlinear_layer_3)\n",
    "nonlinear_layer_5 = tf.keras.layers.Dense(8, activation='selu', name='nonlinear_layer_5')(nonlinear_layer_4)\n",
    "evolved = tf.keras.layers.Dense(ANTIKOOPMAN_DIMENSION, activation='selu', name='evolved_state_layer')(nonlinear_layer_5)\n",
    "\n",
    "NonlinearF = tf.keras.models.Model(inputs=inputs, outputs=evolved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NonlinearF.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=.001), loss=nonlinear_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = NonlinearF.fit(generate_ideal_system_data(256), epochs=100, steps_per_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation of the Koopman Operator\n",
    "\n",
    "We start with $$\\dot{x} = ax,$$ $$\\dot{y} = b(x-x^2).$$  We start by converting our existing variables, $x$ and $y$, into $z$'s.  This gives\n",
    "$$z_1 = x\\qquad \\dot{z_1} = \\dot{x} = ax = az_1$$\n",
    "and\n",
    "$$z_2 = y\\qquad \\dot{z_2} = \\dot{y} = b(x-x^2) = b(z_1-z_3)$$\n",
    "Here, since $x^2$ is a nonlinear term, we had to introduce a linear term to replace it.  This leads to\n",
    "$$z_3 = x^2\\qquad \\dot{z_3} = 2x\\dot{x} = 2ax^2 = 2az_3$$\n",
    "All of our original variables have been accounted for, and all of our $z$'s have their derivates described using only linear terms.  By simply putting this together into a matrix, we have our Koopman operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
