2021-06-13
loss,16.524250030517578,14.321320533752441,14.153429985046387,14.185588836669922,14.158822059631348,14.18466567993164,14.119444847106934,12.757718086242676,9.19749641418457,8.756950378417969,8.696168899536133,8.748483657836914,8.708343505859375,8.71332836151123,8.67441177368164,8.691205978393555,8.70498275756836,8.683457374572754,8.67697525024414,8.678759574890137,8.68618392944336,8.660511016845703,8.639081001281738,8.686371803283691,8.666586875915527,8.69601058959961,8.65835189819336,8.680132865905762,8.660513877868652,8.661612510681152,8.667411804199219,8.649230003356934,8.6566162109375,8.660433769226074,8.628559112548828,8.61224365234375,8.654264450073242,8.62366771697998,8.615310668945312,8.587613105773926,8.557494163513184,8.508541107177734,8.456151962280273,8.39326000213623,8.233353614807129,8.072375297546387,7.93271541595459,7.853480815887451,7.780752658843994,7.723712921142578,7.6616034507751465,7.614222526550293,7.5877909660339355,7.594403266906738,7.524752140045166,7.5494384765625,7.519878387451172,7.488089084625244,7.467855930328369,7.462506294250488,7.448364734649658,7.460849761962891,7.424337863922119,7.363544940948486,7.360502243041992,7.347541332244873,7.273108959197998,7.24795389175415,7.234860897064209,7.227950572967529,7.212035655975342,7.216917514801025,7.166794300079346,7.137287616729736,7.127394199371338,7.080084800720215,7.09067440032959,7.068765163421631,6.789311408996582,6.082905292510986,5.286403179168701,4.617422103881836,4.284552097320557,4.097602844238281,4.039487361907959,3.9508023262023926,3.8637564182281494,3.832251787185669,3.805466890335083,3.759979009628296,3.8530938625335693,3.7650363445281982,3.7047946453094482,3.721630573272705,3.7025344371795654,3.6174514293670654,3.6390676498413086,3.590848922729492,3.580803871154785,3.5444064140319824,3.544210433959961,3.5489797592163086,3.5040972232818604,3.4764208793640137,3.501103401184082,3.4639840126037598,3.426690101623535,3.3830835819244385,3.4311301708221436,3.409430503845215,3.400851249694824,3.4155871868133545,3.4362082481384277,3.4367260932922363,3.5165255069732666,3.4952969551086426,3.5134661197662354,3.495002508163452,3.484543800354004,3.4153125286102295,3.3756825923919678,3.426558494567871,3.2810981273651123,3.342564344406128,3.2882397174835205,3.2993340492248535,3.2584638595581055,3.264155864715576,3.272444486618042,3.3046374320983887,3.3195688724517822,3.316316604614258,3.281593084335327,3.266669511795044,3.2199819087982178,3.2522428035736084,3.2443718910217285,3.2806360721588135,3.258176326751709,3.2525365352630615,3.2545766830444336,3.2911107540130615,3.2055721282958984,3.1707873344421387,3.1861655712127686,3.2375481128692627,3.193477153778076,3.1465253829956055,3.155632257461548,3.115966796875,3.1386802196502686,3.1903817653656006,3.1030619144439697,3.0682904720306396,3.076528549194336,3.096757173538208,3.0998456478118896,3.100309371948242,3.0180346965789795,3.051372766494751,3.065519094467163,3.064857244491577,3.0309741497039795,3.033719539642334,3.0328803062438965,3.1017394065856934,3.0902223587036133,3.060307264328003,2.9977755546569824,3.0348870754241943,3.082576274871826,3.064678907394409,3.046283483505249,3.0760254859924316,3.0179693698883057,3.0674474239349365,3.1349904537200928,3.1365745067596436,3.117063522338867,3.091437578201294,3.119333505630493,3.1072471141815186,3.0204339027404785,3.006850004196167,3.0710158348083496,3.0345215797424316,3.0254814624786377,2.988102912902832,3.0161473751068115,3.01947021484375,3.011687994003296,2.974015474319458,3.062408447265625,3.016664981842041,3.0160250663757324,2.9771170616149902,2.9802322387695312,2.934030532836914,3.0153493881225586,2.9202988147735596,2.955796718597412,3.0291879177093506,3.0994086265563965,3.117258071899414,3.077817440032959,3.0844342708587646,3.112064838409424,3.1747124195098877,3.256870985031128,3.1651151180267334,3.1383211612701416,3.116530179977417,3.153843641281128,3.0867648124694824,3.229567766189575,3.180537223815918,3.1714587211608887,3.1479101181030273,3.0311498641967773,3.0307576656341553,2.9642868041992188,3.0316247940063477,3.148463487625122,3.1690149307250977,3.2534213066101074,3.2265796661376953,3.2089285850524902,3.2753686904907227,3.321366548538208,3.2779669761657715,3.329305410385132,3.2130794525146484,3.1316885948181152,3.085184097290039,3.042595148086548,2.9978573322296143,2.9857404232025146,2.978985548019409,3.032224178314209,3.005596876144409,3.012986660003662,3.0449535846710205,3.0625481605529785,2.9760515689849854,3.0038747787475586,3.062782049179077,3.0263619422912598,3.023125648498535,2.9944472312927246,2.960169792175293,2.940661907196045,2.960059404373169,2.9657645225524902,2.9442853927612305,2.9034152030944824,2.895359754562378,2.9204137325286865,2.95442533493042,2.962446928024292,2.923172950744629,2.9654550552368164,2.8819174766540527,2.9175336360931396,2.8901748657226562,2.9134154319763184,2.880223035812378,2.8593382835388184,2.948538303375244,2.9073102474212646,2.916830062866211,2.8932039737701416,2.878552198410034,2.8894925117492676,2.9355406761169434,2.894181728363037,2.894623041152954,2.9209837913513184,2.9384701251983643,2.95316219329834,3.0253610610961914,2.9304819107055664,2.967665195465088,2.9799795150756836,2.9237358570098877,2.8466320037841797,2.8475286960601807,2.923783779144287,2.8512401580810547,2.8896024227142334,2.8574249744415283,2.8855996131896973,2.8236560821533203,2.86506986618042,2.8323974609375,2.878720998764038,2.8606326580047607,2.8871688842773438,2.865057945251465,2.9229495525360107,2.9781746864318848,2.9366488456726074,2.9808554649353027,2.9561831951141357,2.8718926906585693,2.893603801727295,2.8828368186950684,2.8220255374908447,2.8348512649536133,2.87898588180542,2.917013645172119,2.8994433879852295,2.84975528717041,2.830808639526367,2.8142995834350586,2.83476185798645,2.867547035217285,2.896860122680664,2.9520270824432373,2.868692398071289,2.8636767864227295,2.825387477874756,2.867495059967041,2.878305673599243,2.855738878250122,2.8814175128936768,2.8352291584014893,2.8599016666412354,2.8046529293060303,2.8120572566986084,2.836644172668457,2.8405792713165283,2.7988293170928955,2.842245101928711,2.8301327228546143,2.824075937271118,2.8205840587615967,2.771784782409668,2.769923686981201,2.7535436153411865,2.7488934993743896,2.788914680480957,2.7817764282226562,2.8122904300689697,2.842275381088257,2.880368232727051,2.862048625946045,2.8785457611083984,2.843597650527954,2.8270418643951416,2.8139328956604004,2.8566014766693115,2.8442389965057373,2.8067917823791504,2.8689699172973633,2.8289883136749268,2.783893346786499,2.79600191116333,2.8346245288848877,2.8114943504333496,2.821842670440674,2.8078839778900146,2.8473060131073,2.8282341957092285,2.811408281326294,2.8236453533172607,2.813863515853882,2.776702642440796,2.761925458908081,2.8107314109802246,2.8032801151275635,2.7895379066467285,2.7800536155700684,2.7375314235687256,2.7754318714141846,2.8863861560821533,2.8174774646759033,2.8491671085357666,2.8821325302124023,2.842000722885132,2.884106159210205,2.858436346054077,2.8439791202545166,2.8262369632720947,2.8392128944396973,2.914498805999756,2.8659613132476807,2.8504998683929443,2.841642379760742,2.903176784515381,2.7663826942443848,2.7936291694641113,2.7994320392608643,2.7879128456115723,2.7701730728149414,2.8436124324798584,2.745298385620117,2.742600440979004,2.7948660850524902,2.8144216537475586,2.789604902267456,2.786990165710449,2.781862735748291,2.8553757667541504,2.9083895683288574,2.8234941959381104,2.925271987915039,2.8107879161834717,2.7815117835998535,2.8406331539154053,2.875297784805298,2.8517868518829346,2.7918756008148193,2.747098684310913,2.721895217895508,2.7464215755462646,2.769610643386841,2.738295555114746,2.797043561935425,2.737647771835327,2.7988662719726562,2.7658121585845947,2.860748529434204,2.841573715209961,2.845700740814209,2.9140496253967285,3.0219037532806396,2.9491119384765625,2.875119686126709,2.8608877658843994,2.929584264755249,2.9244027137756348,2.852708339691162,2.912119150161743,2.875039577484131,2.8107972145080566,2.8576042652130127,2.819645404815674,2.885732650756836,2.8974313735961914,2.9064719676971436,2.902557134628296,2.8942410945892334,2.858306884765625,2.881549119949341,2.899129629135132,2.83817982673645,2.86430025100708,2.877498149871826,2.9073684215545654,2.941732406616211,2.834702730178833,2.8856868743896484,2.934288263320923,2.8549633026123047,2.930511713027954,2.8505325317382812,2.799116611480713,2.761105537414551,2.784470558166504,2.765681505203247,2.7056491374969482,2.7448508739471436,2.7518365383148193,2.7163031101226807,2.7335212230682373,2.8005447387695312,2.649636745452881,2.6986069679260254,2.690382957458496,2.698587417602539,2.726274013519287,2.7421958446502686,2.6992359161376953,2.7122650146484375,2.7366843223571777,2.689401865005493,2.716240882873535,2.715806007385254,2.855105400085449,2.744781494140625,2.707019090652466,2.7243812084198,2.7258412837982178,2.647897958755493,2.7450339794158936,2.74080228805542,2.663001775741577,2.7272706031799316,2.716214179992676,2.826692581176758,2.8246610164642334,2.813055992126465,2.7855756282806396,2.8098533153533936,2.8178699016571045,2.8001534938812256,2.7588231563568115,2.707045793533325,2.6588940620422363,2.6852002143859863,
Loss:autoencoding_loss
Optimizer:Adam
Learning Rate:.001
Steps Per Epoch:50
States Per Step:512

Model: "Phi"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 4)]               0         
_________________________________________________________________
encoding_layer_1 (Dense)     (None, 8)                 40        
_________________________________________________________________
encoding_layer_2 (Dense)     (None, 16)                144       
_________________________________________________________________
encoding_layer_3 (Dense)     (None, 8)                 136       
_________________________________________________________________
bottleneck (Dense)           (None, 2)                 18        
=================================================================
Total params: 338
Trainable params: 338
Non-trainable params: 0
_________________________________________________________________

Model: "Phi_inv"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_2 (InputLayer)         [(None, 2)]               0         
_________________________________________________________________
decoding_layer_1 (Dense)     (None, 8)                 24        
_________________________________________________________________
decoding_layer_2 (Dense)     (None, 16)                144       
_________________________________________________________________
decoding_layer_3 (Dense)     (None, 8)                 136       
_________________________________________________________________
decoded_layer (Dense)        (None, 4)                 36        
=================================================================
Total params: 340
Trainable params: 340
Non-trainable params: 0
_________________________________________________________________

Model: "Autoencoder"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
functional_1 (Functional)    (None, 2)                 338       
_________________________________________________________________
functional_3 (Functional)    (None, 4)                 340       
=================================================================
Total params: 678
Trainable params: 678
Non-trainable params: 0
_________________________________________________________________
